@inproceedings{bevilacquaRecentTrendsWord2021,
  title = {Recent {{Trends}} in {{Word Sense Disambiguation}}: {{A Survey}}},
  shorttitle = {Recent {{Trends}} in {{Word Sense Disambiguation}}},
  author = {Bevilacqua, Michele and Pasini, Tommaso and Raganato, Alessandro and Navigli, Roberto},
  date = {2021-08-09},
  volume = {5},
  pages = {4330--4338},
  issn = {1045-0823},
  doi = {10.24963/ijcai.2021/593},
  url = {https://www.ijcai.org/proceedings/2021/593},
  urldate = {2023-06-22},
  abstract = {Electronic proceedings of IJCAI 2021},
  eventtitle = {Twenty-{{Ninth International Joint Conference}} on {{Artificial Intelligence}}},
  langid = {english},
  file = {/Users/max18768/Zotero/storage/PES3YZW5/Bevilacqua et al. - 2021 - Recent Trends in Word Sense Disambiguation A Surv.pdf}
}

@article{chandrasekaranEvolutionSemanticSimilarity2021,
  title = {Evolution of {{Semantic Similarity}}—{{A Survey}}},
  author = {Chandrasekaran, Dhivya and Mago, Vijay},
  date = {2021-02-18},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {54},
  number = {2},
  pages = {41:1--41:37},
  issn = {0360-0300},
  doi = {10.1145/3440755},
  url = {https://dl.acm.org/doi/10.1145/3440755},
  urldate = {2023-06-17},
  abstract = {Estimating the semantic similarity between text data is one of the challenging and open research problems in the field of Natural Language Processing (NLP). The versatility of natural language makes it difficult to define rule-based methods for determining semantic similarity measures. To address this issue, various semantic similarity methods have been proposed over the years. This survey article traces the evolution of such methods beginning from traditional NLP techniques such as kernel-based methods to the most recent research work on transformer-based models, categorizing them based on their underlying principles as knowledge-based, corpus-based, deep neural network–based methods, and hybrid methods. Discussing the strengths and weaknesses of each method, this survey provides a comprehensive view of existing systems in place for new researchers to experiment and develop innovative ideas to address the issue of semantic similarity.},
  keywords = {corpus-based methods,knowledge-based methods,linguistics,Semantic similarity,supervised and unsupervised methods,word embeddings},
  file = {/Users/max18768/Zotero/storage/K9ZC823Q/chandrasekaran2021.pdf.pdf;/Users/max18768/Zotero/storage/XDG47NLS/Chandrasekaran and Mago - 2021 - Evolution of Semantic Similarity—A Survey.pdf}
}

@online{evansMachineTranslationMining2016,
  type = {SSRN Scholarly Paper},
  title = {Machine {{Translation}}: {{Mining Text}} for {{Social Theory}}},
  shorttitle = {Machine {{Translation}}},
  author = {Evans, James A. and Aceves, Pedro},
  date = {2016-07-01},
  number = {2822747},
  location = {{Rochester, NY}},
  doi = {10.1146/annurev-soc-081715-074206},
  url = {https://papers.ssrn.com/abstract=2822747},
  urldate = {2023-06-15},
  abstract = {More of the social world lives within electronic text than ever before, from collective activity on the web, social media, and instant messaging to online transactions, government intelligence, and digitized libraries. This supply of text has elicited demand for natural language processing and machine learning tools to filter, search, and translate text into valuable data. We survey some of the most exciting computational approaches to text analysis, highlighting both supervised methods that extend old theories to new data and unsupervised techniques that discover hidden regularities worth theorizing. We then review recent research that uses these tools to develop social insight by exploring (a) collective attention and reasoning through the content of communication; (b) social relationships through the process of communication; and (c) social states, roles, and moves identified through heterogeneous signals within communication. We highlight social questions for which these advances could offer powerful new insight.},
  langid = {english},
  pubstate = {preprint},
  keywords = {James A. Evans,Machine Translation: Mining Text for Social Theory,Pedro Aceves,SSRN},
  file = {/Users/max18768/Zotero/storage/DS475XWS/evans2015.pdf.pdf}
}

@online{gaoSimCSESimpleContrastive2022,
  title = {{{SimCSE}}: {{Simple Contrastive Learning}} of {{Sentence Embeddings}}},
  shorttitle = {{{SimCSE}}},
  author = {Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
  date = {2022-05-18},
  eprint = {2104.08821},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2104.08821},
  url = {http://arxiv.org/abs/2104.08821},
  urldate = {2022-12-13},
  abstract = {This paper presents SimCSE, a simple contrastive learning framework that greatly advances state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation, and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework by using "entailment" pairs as positives and "contradiction" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3\% and 81.6\% Spearman's correlation respectively, a 4.2\% and 2.2\% improvement compared to the previous best results. We also show -- both theoretically and empirically -- that the contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/max18768/Zotero/storage/2CEFWD5L/Gao et al. - 2022 - SimCSE Simple Contrastive Learning of Sentence Em.pdf;/Users/max18768/Zotero/storage/ZUAGJRDI/2104.html}
}

@article{grimmerTextDataPromise2013,
  title = {Text as {{Data}}: {{The Promise}} and {{Pitfalls}} of {{Automatic Content Analysis Methods}} for {{Political Texts}}},
  shorttitle = {Text as {{Data}}},
  author = {Grimmer, Justin and Stewart, Brandon M.},
  date = {2013-07},
  journaltitle = {Political Analysis},
  volume = {21},
  number = {3},
  pages = {267--297},
  publisher = {{Cambridge University Press}},
  issn = {1047-1987, 1476-4989},
  doi = {10.1093/pan/mps028},
  url = {https://www.cambridge.org/core/journals/political-analysis/article/text-as-data-the-promise-and-pitfalls-of-automatic-content-analysis-methods-for-political-texts/F7AAC8B2909441603FEB25C156448F20},
  urldate = {2023-06-15},
  abstract = {Politics and political conflict often occur in the written and spoken word. Scholars have long recognized this, but the massive costs of analyzing even moderately sized collections of texts have hindered their use in political science research. Here lies the promise of automated text analysis: it substantially reduces the costs of analyzing large collections of text. We provide a guide to this exciting new area of research and show how, in many instances, the methods have already obtained part of their promise. But there are pitfalls to using automated methods—they are no substitute for careful thought and close reading and require extensive and problem-specific validation. We survey a wide range of new methods, provide guidance on how to validate the output of the models, and clarify misconceptions and errors in the literature. To conclude, we argue that for automated text methods to become a standard tool for political scientists, methodologists must contribute new methods and new methods of validation.},
  langid = {english},
  file = {/Users/max18768/Zotero/storage/7K82FEB6/Grimmer and Stewart - 2013 - Text as Data The Promise and Pitfalls of Automati.pdf;/Users/max18768/Zotero/storage/IKGLLVQA/grimmer2013.pdf.pdf}
}

@book{harispeSemanticSimilarityNatural2015,
  title = {Semantic {{Similarity}} from {{Natural Language}} and {{Ontology Analysis}}},
  author = {Harispe, Sébastien and Ranwez, Sylvie and Janaqi, Stefan and Montmain, Jacky},
  date = {2015},
  eprint = {1704.05295},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.2200/S00639ED1V01Y201504HLT027},
  url = {http://arxiv.org/abs/1704.05295},
  urldate = {2023-06-19},
  abstract = {Artificial Intelligence federates numerous scientific fields in the aim of developing machines able to assist human operators performing complex treatments -- most of which demand high cognitive skills (e.g. learning or decision processes). Central to this quest is to give machines the ability to estimate the likeness or similarity between things in the way human beings estimate the similarity between stimuli. In this context, this book focuses on semantic measures: approaches designed for comparing semantic entities such as units of language, e.g. words, sentences, or concepts and instances defined into knowledge bases. The aim of these measures is to assess the similarity or relatedness of such semantic entities by taking into account their semantics, i.e. their meaning -- intuitively, the words tea and coffee, which both refer to stimulating beverage, will be estimated to be more semantically similar than the words toffee (confection) and coffee, despite that the last pair has a higher syntactic similarity. The two state-of-the-art approaches for estimating and quantifying semantic similarities/relatedness of semantic entities are presented in detail: the first one relies on corpora analysis and is based on Natural Language Processing techniques and semantic models while the second is based on more or less formal, computer-readable and workable forms of knowledge such as semantic networks, thesaurus or ontologies. (...) Beyond a simple inventory and categorization of existing measures, the aim of this monograph is to convey novices as well as researchers of these domains towards a better understanding of semantic similarity estimation and more generally semantic measures.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/max18768/Zotero/storage/6MP3F3YL/Harispe et al. - 2015 - Semantic Similarity from Natural Language and Onto.pdf;/Users/max18768/Zotero/storage/UIFSXE5V/1704.html}
}

@article{hirschbergAdvancesNaturalLanguage2015,
  title = {Advances in Natural Language Processing},
  author = {Hirschberg, Julia and Manning, Christopher D.},
  date = {2015-07-17},
  journaltitle = {Science},
  volume = {349},
  number = {6245},
  pages = {261--266},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.aaa8685},
  url = {https://www.science.org/doi/abs/10.1126/science.aaa8685},
  urldate = {2023-06-15},
  abstract = {Natural language processing employs computational techniques for the purpose of learning, understanding, and producing human language content. Early computational approaches to language research focused on automating the analysis of the linguistic structure of language and developing basic technologies such as machine translation, speech recognition, and speech synthesis. Today’s researchers refine and make use of such tools in real-world applications, creating spoken dialogue systems and speech-to-speech translation engines, mining social media for information about health or finance, and identifying sentiment and emotion toward products and services. We describe successes and challenges in this rapidly advancing area.},
  file = {/Users/max18768/Zotero/storage/J9BT4UJV/hirschberg2015.pdf.pdf}
}

@online{huangWhiteningBERTEasyUnsupervised2021,
  title = {{{WhiteningBERT}}: {{An Easy Unsupervised Sentence Embedding Approach}}},
  shorttitle = {{{WhiteningBERT}}},
  author = {Huang, Junjie and Tang, Duyu and Zhong, Wanjun and Lu, Shuai and Shou, Linjun and Gong, Ming and Jiang, Daxin and Duan, Nan},
  date = {2021-04-08},
  eprint = {2104.01767},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2104.01767},
  url = {http://arxiv.org/abs/2104.01767},
  urldate = {2022-12-13},
  abstract = {Producing the embedding of a sentence in an unsupervised way is valuable to natural language matching and retrieval problems in practice. In this work, we conduct a thorough examination of pretrained model based unsupervised sentence embeddings. We study on four pretrained models and conduct massive experiments on seven datasets regarding sentence semantics. We have there main findings. First, averaging all tokens is better than only using [CLS] vector. Second, combining both top andbottom layers is better than only using top layers. Lastly, an easy whitening-based vector normalization strategy with less than 10 lines of code consistently boosts the performance.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/max18768/Zotero/storage/H87Z7KXH/Huang et al. - 2021 - WhiteningBERT An Easy Unsupervised Sentence Embed.pdf;/Users/max18768/Zotero/storage/QPDXM2JW/2104.html}
}

@article{lazerComputationalSocialScience2009,
  title = {Computational {{Social Science}}},
  author = {Lazer, David and Pentland, Alex and Adamic, Lada and Aral, Sinan and Barabási, Albert-László and Brewer, Devon and Christakis, Nicholas and Contractor, Noshir and Fowler, James and Gutmann, Myron and Jebara, Tony and King, Gary and Macy, Michael and Roy, Deb and Van Alstyne, Marshall},
  date = {2009-02-06},
  journaltitle = {Science},
  volume = {323},
  number = {5915},
  pages = {721--723},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.1167742},
  url = {https://www.science.org/doi/full/10.1126/science.1167742},
  urldate = {2023-06-15},
  file = {/Users/max18768/Zotero/storage/A9USLW4A/Lazer et al. - 2009 - Computational Social Science.pdf;/Users/max18768/Zotero/storage/BXT75E6T/lazer2009.pdf.pdf}
}

@online{menezesSemanticHypergraphs2021,
  title = {Semantic {{Hypergraphs}}},
  author = {Menezes, Telmo and Roth, Camille},
  date = {2021-02-18},
  eprint = {1908.10784},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1908.10784},
  url = {http://arxiv.org/abs/1908.10784},
  urldate = {2022-07-19},
  abstract = {Approaches to Natural language processing (NLP) may be classified along a double dichotomy open/opaque - strict/adaptive. The former axis relates to the possibility of inspecting the underlying processing rules, the latter to the use of fixed or adaptive rules. We argue that many techniques fall into either the open-strict or opaque-adaptive categories. Our contribution takes steps in the open-adaptive direction, which we suggest is likely to provide key instruments for interdisciplinary research. The central idea of our approach is the Semantic Hypergraph (SH), a novel knowledge representation model that is intrinsically recursive and accommodates the natural hierarchical richness of natural language. The SH model is hybrid in two senses. First, it attempts to combine the strengths of ML and symbolic approaches. Second, it is a formal language representation that reduces but tolerates ambiguity and structural variability. We will see that SH enables simple yet powerful methods of pattern detection, and features a good compromise for intelligibility both for humans and machines. It also provides a semantically deep starting point (in terms of explicit meaning) for further algorithms to operate and collaborate on. We show how modern NLP ML-based building blocks can be used in combination with a random forest classifier and a simple search tree to parse NL to SH, and that this parser can achieve high precision in a diversity of text categories. We define a pattern language representable in SH itself, and a process to discover knowledge inference rules. We then illustrate the efficiency of the SH framework in a variety of tasks, including conjunction decomposition, open information extraction, concept taxonomy inference and co-reference resolution, and an applied example of claim and conflict analysis in a news corpus.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/Users/max18768/Zotero/storage/J373TPHB/Menezes_Roth_2021_Semantic Hypergraphs.pdf;/Users/max18768/Zotero/storage/IFHHHXU5/1908.html}
}

@article{mickusHowDissectMuppet2022,
  title = {How to {{Dissect}} a {{Muppet}}: {{The Structure}} of {{Transformer Embedding Spaces}}},
  shorttitle = {How to {{Dissect}} a {{Muppet}}},
  author = {Mickus, Timothee and Paperno, Denis and Constant, Mathieu},
  date = {2022-09-07},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  shortjournal = {Transactions of the Association for Computational Linguistics},
  volume = {10},
  pages = {981--996},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00501},
  url = {https://doi.org/10.1162/tacl_a_00501},
  urldate = {2023-06-21},
  abstract = {Pretrained embeddings based on the Transformer architecture have taken the NLP community by storm. We show that they can mathematically be reframed as a sum of vector factors and showcase how to use this reframing to study the impact of each component. We provide evidence that multi-head attentions and feed-forwards are not equally useful in all downstream applications, as well as a quantitative overview of the effects of finetuning on the overall embedding space. This approach allows us to draw connections to a wide range of previous studies, from vector space anisotropy to attention weights.},
  file = {/Users/max18768/Zotero/storage/E3RI9MDK/Mickus et al. - 2022 - How to Dissect a Muppet The Structure of Transfor.pdf;/Users/max18768/Zotero/storage/9QMPI4JU/How-to-Dissect-a-Muppet-The-Structure-of.html}
}

@online{mikolovDistributedRepresentationsWords2013,
  title = {Distributed {{Representations}} of {{Words}} and {{Phrases}} and Their {{Compositionality}}},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  date = {2013-10-16},
  eprint = {1310.4546},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1310.4546},
  url = {http://arxiv.org/abs/1310.4546},
  urldate = {2023-06-22},
  abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/max18768/Zotero/storage/DCMZTKVP/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf;/Users/max18768/Zotero/storage/KABIGKIP/1310.html}
}

@article{qiuPretrainedModelsNatural2020,
  title = {Pre-Trained Models for Natural Language Processing: {{A}} Survey},
  shorttitle = {Pre-Trained Models for Natural Language Processing},
  author = {Qiu, XiPeng and Sun, TianXiang and Xu, YiGe and Shao, YunFan and Dai, Ning and Huang, XuanJing},
  date = {2020-10-01},
  journaltitle = {Science China Technological Sciences},
  shortjournal = {Sci. China Technol. Sci.},
  volume = {63},
  number = {10},
  pages = {1872--1897},
  issn = {1869-1900},
  doi = {10.1007/s11431-020-1647-3},
  url = {https://doi.org/10.1007/s11431-020-1647-3},
  urldate = {2023-06-15},
  abstract = {Recently, the emergence of pre-trained models (PTMs) has brought natural language processing (NLP) to a new era. In this survey, we provide a comprehensive review of PTMs for NLP. We first briefly introduce language representation learning and its research progress. Then we systematically categorize existing PTMs based on a taxonomy from four different perspectives. Next, we describe how to adapt the knowledge of PTMs to downstream tasks. Finally, we outline some potential directions of PTMs for future research. This survey is purposed to be a hands-on guide for understanding, using, and developing PTMs for various NLP tasks.},
  langid = {english},
  keywords = {deep learning,distributed representation,language modelling,natural language processing,neural network,pre-trained model,self-supervised learning,word embedding},
  file = {/Users/max18768/Zotero/storage/RNHS3FRU/qiu2020.pdf.pdf;/Users/max18768/Zotero/storage/RS6V5LYW/Qiu et al. - 2020 - Pre-trained models for natural language processing.pdf}
}

@online{reimersClassificationClusteringArguments2019,
  title = {Classification and {{Clustering}} of {{Arguments}} with {{Contextualized Word Embeddings}}},
  author = {Reimers, Nils and Schiller, Benjamin and Beck, Tilman and Daxenberger, Johannes and Stab, Christian and Gurevych, Iryna},
  date = {2019-06-24},
  eprint = {1906.09821},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1906.09821},
  url = {http://arxiv.org/abs/1906.09821},
  urldate = {2023-01-10},
  abstract = {We experiment with two recent contextualized word embedding methods (ELMo and BERT) in the context of open-domain argument search. For the first time, we show how to leverage the power of contextualized word embeddings to classify and cluster topic-dependent arguments, achieving impressive results on both tasks and across multiple datasets. For argument classification, we improve the state-of-the-art for the UKP Sentential Argument Mining Corpus by 20.8 percentage points and for the IBM Debater - Evidence Sentences dataset by 7.4 percentage points. For the understudied task of argument clustering, we propose a pre-training step which improves by 7.8 percentage points over strong baselines on a novel dataset, and by 12.3 percentage points for the Argument Facet Similarity (AFS) Corpus.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/max18768/Zotero/storage/GMRBYSTS/Reimers et al. - 2019 - Classification and Clustering of Arguments with Co.pdf;/Users/max18768/Zotero/storage/PY7UJZ3L/1906.html}
}

@article{rodriguezWordEmbeddingsWhat2022,
  title = {Word {{Embeddings}}: {{What Works}}, {{What Doesn}}’t, and {{How}} to {{Tell}} the {{Difference}} for {{Applied Research}}},
  shorttitle = {Word {{Embeddings}}},
  author = {Rodriguez, Pedro L. and Spirling, Arthur},
  date = {2022-01},
  journaltitle = {The Journal of Politics},
  volume = {84},
  number = {1},
  pages = {101--115},
  publisher = {{The University of Chicago Press}},
  issn = {0022-3816},
  doi = {10.1086/715162},
  url = {https://www.journals.uchicago.edu/doi/full/10.1086/715162},
  urldate = {2022-12-13},
  abstract = {Word embeddings are becoming popular for political science research, yet we know little about their properties and performance. To help scholars seeking to use these techniques, we explore the effects of key parameter choices—including context window length, embedding vector dimensions, and pretrained versus locally fit variants—on the efficiency and quality of inferences possible with these models. Reassuringly we show that results are generally robust to such choices for political corpora of various sizes and in various languages. Beyond reporting extensive technical findings, we provide a novel crowdsourced “Turing test”–style method for examining the relative performance of any two models that produce substantive, text-based outputs. Our results are encouraging: popular, easily available pretrained embeddings perform at a level close to—or surpassing—both human coders and more complicated locally fit models. For completeness, we provide best practice advice for cases where local fitting is required.},
  keywords = {crowdsourcing,deep learning,embeddings,text,Turing test},
  file = {/Users/max18768/Zotero/storage/FMUDZ8CT/Rodriguez and Spirling - 2022 - Word Embeddings What Works, What Doesn’t, and How.pdf}
}

@article{rudinStopExplainingBlack2019,
  title = {Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead},
  author = {Rudin, Cynthia},
  date = {2019-05},
  journaltitle = {Nature Machine Intelligence},
  shortjournal = {Nat Mach Intell},
  volume = {1},
  number = {5},
  pages = {206--215},
  publisher = {{Nature Publishing Group}},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0048-x},
  url = {https://www.nature.com/articles/s42256-019-0048-x},
  urldate = {2023-06-17},
  abstract = {Black box machine learning models are currently being used for high-stakes decision making throughout society, causing problems in healthcare, criminal justice and other domains. Some people hope that creating methods for explaining these black box models will alleviate some of the problems, but trying to explain black box models, rather than creating models that are interpretable in the first place, is likely to perpetuate bad practice and can potentially cause great harm to society. The way forward is to design models that are inherently interpretable. This Perspective clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare and computer vision.},
  issue = {5},
  langid = {english},
  keywords = {Computer science,Criminology,Science,Statistics,technology and society},
  file = {/Users/max18768/Zotero/storage/7KJHMGRV/Rudin - 2019 - Stop explaining black box machine learning models .pdf;/Users/max18768/Zotero/storage/W9H5GJ4P/10.1038@s42256-019-0048-x.pdf.pdf}
}

@misc{TwoSidesEnvironmental,
  title = {The Two Sides of the {{Environmental Kuznets Curve}}: A Socio-Semantic Analysis},
  file = {/Users/max18768/Zotero/storage/7KBYS9C8/The two sides of the Environmental Kuznets Curve .pdf}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  date = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  urldate = {2023-06-21},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  file = {/Users/max18768/Zotero/storage/MG968VR5/Vaswani et al. - 2017 - Attention is All you Need.pdf}
}

@article{wangDeepNeuralNetworkbased2022,
  title = {Deep Neural Network-Based Relation Extraction: An Overview},
  shorttitle = {Deep Neural Network-Based Relation Extraction},
  author = {Wang, Hailin and Qin, Ke and Zakari, Rufai Yusuf and Lu, Guoming and Yin, Jin},
  date = {2022-03-01},
  journaltitle = {Neural Computing and Applications},
  shortjournal = {Neural Comput \& Applic},
  volume = {34},
  number = {6},
  pages = {4781--4801},
  issn = {1433-3058},
  doi = {10.1007/s00521-021-06667-3},
  url = {https://doi.org/10.1007/s00521-021-06667-3},
  urldate = {2022-12-12},
  abstract = {Knowledge is a formal way of understanding the world, providing human-level cognition and intelligence for the next-generation artificial intelligence (AI). An effective way to automatically acquire this important knowledge, called Relation Extraction (RE), plays a vital role in Natural Language Processing (NLP). To date, there are amount of studies for RE in previous works, among which these technologies based on deep neural networks (DNNs) have become the mainstream direction of this research. In particular, the supervised and distant supervision methods based on DNNs are the most popular and reliable solutions for RE, whose various evolutions on structure and settings have affected this task. Understanding the model structure and related settings will give the researchers a deep insight into RE. However, little research has been done on them. Hence, this paper starts from these two points and carries out analysis around the mainstream research routes, supervised and distant supervision. Meanwhile, we classify all related works according to the evolution of model structure to facilitate the analysis. Finally, we discuss some challenges of RE and give out our conclusion.},
  langid = {english},
  keywords = {Information extraction,Neural networks,Overview,Relation extraction},
  file = {/Users/max18768/Zotero/storage/P5EMTFQ6/Wang et al. - 2022 - Deep neural network-based relation extraction an .pdf}
}

@online{wangTextEmbeddingsWeaklySupervised2022,
  title = {Text {{Embeddings}} by {{Weakly-Supervised Contrastive Pre-training}}},
  author = {Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},
  date = {2022-12-07},
  eprint = {2212.03533},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2212.03533},
  url = {http://arxiv.org/abs/2212.03533},
  urldate = {2023-06-08},
  abstract = {This paper presents E5, a family of state-of-the-art text embeddings that transfer well to a wide range of tasks. The model is trained in a contrastive manner with weak supervision signals from our curated large-scale text pair dataset (called CCPairs). E5 can be readily used as a general-purpose embedding model for any tasks requiring a single-vector representation of texts such as retrieval, clustering, and classification, achieving strong performance in both zero-shot and fine-tuned settings. We conduct extensive evaluations on 56 datasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the first model that outperforms the strong BM25 baseline on the BEIR retrieval benchmark without using any labeled data. When fine-tuned, E5 obtains the best results on the MTEB benchmark, beating existing embedding models with 40x more parameters.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/Users/max18768/Zotero/storage/ACSNGMWD/Wang et al. - 2022 - Text Embeddings by Weakly-Supervised Contrastive P.pdf;/Users/max18768/Zotero/storage/7V9TV47B/2212.html}
}

@online{wangTSDAEUsingTransformerbased2021,
  title = {{{TSDAE}}: {{Using Transformer-based Sequential Denoising Auto-Encoder}} for {{Unsupervised Sentence Embedding Learning}}},
  shorttitle = {{{TSDAE}}},
  author = {Wang, Kexin and Reimers, Nils and Gurevych, Iryna},
  date = {2021-09-10},
  eprint = {2104.06979},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2104.06979},
  url = {http://arxiv.org/abs/2104.06979},
  urldate = {2022-12-13},
  abstract = {Learning sentence embeddings often requires a large amount of labeled data. However, for most tasks and domains, labeled data is seldom available and creating it is expensive. In this work, we present a new state-of-the-art unsupervised method based on pre-trained Transformers and Sequential Denoising Auto-Encoder (TSDAE) which outperforms previous approaches by up to 6.4 points. It can achieve up to 93.1\% of the performance of in-domain supervised approaches. Further, we show that TSDAE is a strong domain adaptation and pre-training method for sentence embeddings, significantly outperforming other approaches like Masked Language Model. A crucial shortcoming of previous studies is the narrow evaluation: Most work mainly evaluates on the single task of Semantic Textual Similarity (STS), which does not require any domain knowledge. It is unclear if these proposed methods generalize to other domains and tasks. We fill this gap and evaluate TSDAE and other recent approaches on four different datasets from heterogeneous domains.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/max18768/Zotero/storage/JEW32I97/Wang et al. - 2021 - TSDAE Using Transformer-based Sequential Denoisin.pdf;/Users/max18768/Zotero/storage/DVMG5EK5/2104.html}
}

@online{wilkersonLargeScaleComputerizedText2017,
  type = {SSRN Scholarly Paper},
  title = {Large-{{Scale Computerized Text Analysis}} in {{Political Science}}: {{Opportunities}} and {{Challenges}}},
  shorttitle = {Large-{{Scale Computerized Text Analysis}} in {{Political Science}}},
  author = {Wilkerson, John and Casas, Andreu},
  date = {2017-05-01},
  number = {2968080},
  location = {{Rochester, NY}},
  doi = {10.1146/annurev-polisci-052615-025542},
  url = {https://papers.ssrn.com/abstract=2968080},
  urldate = {2023-06-15},
  abstract = {Text has always been an important data source in political science. What has changed in recent years is the feasibility of investigating large amounts of text quantitatively. The internet provides political scientists with more data than their mentors could have imagined, and the research community is providing accessible text analysis software packages, along with training and support. As a result, text-as-data research is becoming mainstream in political science. Scholars are tapping new data sources, they are employing more diverse methods, and they are becoming critical consumers of findings based on those methods. In this article, we first describe the four stages of a typical text-as-data project. We then review recent political science applications and explore one important methodological challenge—topic model instability—in greater detail.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Andreu Casas,John Wilkerson,Large-Scale Computerized Text Analysis in Political Science: Opportunities and Challenges,SSRN},
  file = {/Users/max18768/Zotero/storage/Y2WDBP3Q/wilkerson2017.pdf.pdf}
}

@article{yangSurveyExtractionCausal2022,
  title = {A Survey on Extraction of Causal Relations from Natural Language Text},
  author = {Yang, Jie and Han, Soyeon Caren and Poon, Josiah},
  date = {2022-05-01},
  journaltitle = {Knowledge and Information Systems},
  shortjournal = {Knowl Inf Syst},
  volume = {64},
  number = {5},
  pages = {1161--1186},
  issn = {0219-3116},
  doi = {10.1007/s10115-022-01665-w},
  url = {https://doi.org/10.1007/s10115-022-01665-w},
  urldate = {2022-12-12},
  abstract = {As an essential component of human cognition, cause–effect relations appear frequently in text, and curating cause–effect relations from text helps in building causal networks for predictive tasks. Existing causality extraction techniques include knowledge-based, statistical machine learning (ML)-based, and deep learning-based approaches. Each method has its advantages and weaknesses. For example, knowledge-based methods are understandable but require extensive manual domain knowledge and have poor cross-domain applicability. Statistical machine learning methods are more automated because of natural language processing (NLP) toolkits. However, feature engineering is labor-intensive, and toolkits may lead to error propagation. In the past few years, deep learning techniques attract substantial attention from NLP researchers because of its powerful representation learning ability and the rapid increase in computational resources. Their limitations include high computational costs and a lack of adequate annotated training data. In this paper, we conduct a comprehensive survey of causality extraction. We initially introduce primary forms existing in the causality extraction: explicit intra-sentential causality, implicit causality, and inter-sentential causality. Next, we list benchmark datasets and modeling assessment methods for causal relation extraction. Then, we present a structured overview of the three techniques with their representative systems. Lastly, we highlight existing open challenges with their potential directions.},
  langid = {english},
  keywords = {Causality extraction,Deep learning,Explicit intra-sentential causality,Implicit causality,Inter-sentential causality},
  file = {/Users/max18768/Zotero/storage/J9FTAW9R/Yang et al. - 2022 - A survey on extraction of causal relations from na.pdf}
}

@online{youngRecentTrendsDeep2018,
  title = {Recent {{Trends}} in {{Deep Learning Based Natural Language Processing}}},
  author = {Young, Tom and Hazarika, Devamanyu and Poria, Soujanya and Cambria, Erik},
  date = {2018-11-24},
  eprint = {1708.02709},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1708.02709},
  url = {http://arxiv.org/abs/1708.02709},
  urldate = {2023-06-15},
  abstract = {Deep learning methods employ multiple processing layers to learn hierarchical representations of data and have produced state-of-the-art results in many domains. Recently, a variety of model designs and methods have blossomed in the context of natural language processing (NLP). In this paper, we review significant deep learning related models and methods that have been employed for numerous NLP tasks and provide a walk-through of their evolution. We also summarize, compare and contrast the various models and put forward a detailed understanding of the past, present and future of deep learning in NLP.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/max18768/Zotero/storage/NWL9T54I/Young et al. - 2018 - Recent Trends in Deep Learning Based Natural Langu.pdf;/Users/max18768/Zotero/storage/WXUJTMXJ/1708.html}
}
