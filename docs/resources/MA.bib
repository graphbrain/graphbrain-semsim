@book{aggarwalMiningTextData2012,
  title = {Mining {{Text Data}}},
  editor = {Aggarwal, Charu C. and Zhai, ChengXiang},
  date = {2012},
  publisher = {Springer US},
  location = {Boston, MA},
  doi = {10.1007/978-1-4614-3223-4},
  url = {https://link.springer.com/10.1007/978-1-4614-3223-4},
  urldate = {2023-07-31},
  isbn = {978-1-4614-3222-7 978-1-4614-3223-4},
  langid = {english},
  keywords = {Clustering,Data mining,Databases,Embedded,Heterogeneous,Machine learning and e-commerce,Mining text,Multimedia data,Networking applications,Networks,Social networks,Text mining}
}

@article{aizawaInformationtheoreticPerspectiveTf2003,
  title = {An Information-Theoretic Perspective of Tf–Idf Measures},
  author = {Aizawa, Akiko},
  date = {2003-01-01},
  journaltitle = {Information Processing \& Management},
  shortjournal = {Information Processing \& Management},
  volume = {39},
  number = {1},
  pages = {45--65},
  issn = {0306-4573},
  doi = {10.1016/S0306-4573(02)00021-3},
  url = {https://www.sciencedirect.com/science/article/pii/S0306457302000213},
  urldate = {2024-03-27},
  abstract = {This paper presents a mathematical definition of the “probability-weighted amount of information” (PWI), a measure of specificity of terms in documents that is based on an information-theoretic view of retrieval events. The proposed PWI is expressed as a product of the occurrence probabilities of terms and their amounts of information, and corresponds well with the conventional term frequency–inverse document frequency measures that are commonly used in today’s information retrieval systems. The mathematical definition of the PWI is shown, together with some illustrative examples of the calculation.},
  keywords = {Information theory,Term weighting theories,Text categorization,tf–idf},
  file = {/Users/max18768/Zotero/storage/PE76CZNN/Aizawa - 2003 - An information-theoretic perspective of tf–idf mea.pdf;/Users/max18768/Zotero/storage/2MTCHUTZ/S0306457302000213.html}
}

@inproceedings{akbikContextualStringEmbeddings2018,
  title = {Contextual {{String Embeddings}} for {{Sequence Labeling}}},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Computational Linguistics}}},
  author = {Akbik, Alan and Blythe, Duncan and Vollgraf, Roland},
  editor = {Bender, Emily M. and Derczynski, Leon and Isabelle, Pierre},
  date = {2018-08},
  pages = {1638--1649},
  publisher = {Association for Computational Linguistics},
  location = {Santa Fe, New Mexico, USA},
  url = {https://aclanthology.org/C18-1139},
  urldate = {2024-04-07},
  abstract = {Recent advances in language modeling using recurrent neural networks have made it viable to model language as distributions over characters. By learning to predict the next character on the basis of previous characters, such models have been shown to automatically internalize linguistic concepts such as words, sentences, subclauses and even sentiment. In this paper, we propose to leverage the internal states of a trained character language model to produce a novel type of word embedding which we refer to as contextual string embeddings. Our proposed embeddings have the distinct properties that they (a) are trained without any explicit notion of words and thus fundamentally model words as sequences of characters, and (b) are contextualized by their surrounding text, meaning that the same word will have different embeddings depending on its contextual use. We conduct a comparative evaluation against previous embeddings and find that our embeddings are highly useful for downstream tasks: across four classic sequence labeling tasks we consistently outperform the previous state-of-the-art. In particular, we significantly outperform previous work on English and German named entity recognition (NER), allowing us to report new state-of-the-art F1-scores on the CoNLL03 shared task. We release all code and pre-trained language models in a simple-to-use framework to the research community, to enable reproduction of these experiments and application of our proposed embeddings to other tasks: https://github.com/zalandoresearch/flair},
  eventtitle = {{{COLING}} 2018},
  file = {/Users/max18768/Zotero/storage/BKHB7NJM/Akbik et al. - 2018 - Contextual String Embeddings for Sequence Labeling.pdf}
}

@inproceedings{albawiUnderstandingConvolutionalNeural2017,
  title = {Understanding of a Convolutional Neural Network},
  booktitle = {2017 {{International Conference}} on {{Engineering}} and {{Technology}} ({{ICET}})},
  author = {Albawi, Saad and Mohammed, Tareq Abed and Al-Zawi, Saad},
  date = {2017-08},
  pages = {1--6},
  doi = {10.1109/ICEngTechnol.2017.8308186},
  url = {https://ieeexplore.ieee.org/abstract/document/8308186?casa_token=Zj-p3F-vNNoAAAAA:6_vQzHSZ6kJtpH8DBzRYZApO6e1TlcFsogTyOWpaMhJJKb6IpAN7Xwf8kpWy4eS0f8vXWV3_4Lc},
  urldate = {2024-04-06},
  abstract = {The term Deep Learning or Deep Neural Network refers to Artificial Neural Networks (ANN) with multi layers. Over the last few decades, it has been considered to be one of the most powerful tools, and has become very popular in the literature as it is able to handle a huge amount of data. The interest in having deeper hidden layers has recently begun to surpass classical methods performance in different fields; especially in pattern recognition. One of the most popular deep neural networks is the Convolutional Neural Network (CNN). It take this name from mathematical linear operation between matrixes called convolution. CNN have multiple layers; including convolutional layer, non-linearity layer, pooling layer and fully-connected layer. The convolutional and fully-connected layers have parameters but pooling and non-linearity layers don't have parameters. The CNN has an excellent performance in machine learning problems. Specially the applications that deal with image data, such as largest image classification data set (Image Net), computer vision, and in natural language processing (NLP) and the results achieved were very amazing. In this paper we will explain and define all the elements and important issues related to CNN, and how these elements work. In addition, we will also state the parameters that effect CNN efficiency. This paper assumes that the readers have adequate knowledge about both machine learning and artificial neural network.},
  eventtitle = {2017 {{International Conference}} on {{Engineering}} and {{Technology}} ({{ICET}})},
  keywords = {artificial neural networks,computer vision,Convolution,convolutional neural networks,Convolutional neural networks,deep learning,Feature extraction,Image edge detection,Image recognition,machine learning,Neurons},
  file = {/Users/max18768/Zotero/storage/NGHR4YRB/8308186.html}
}

@online{almeidaWordEmbeddingsSurvey2023,
  title = {Word {{Embeddings}}: {{A Survey}}},
  shorttitle = {Word {{Embeddings}}},
  author = {Almeida, Felipe and Xexéo, Geraldo},
  date = {2023-05-01},
  eprint = {1901.09069},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1901.09069},
  url = {http://arxiv.org/abs/1901.09069},
  urldate = {2023-07-26},
  abstract = {This work lists and describes the main recent strategies for building fixed-length, dense and distributed representations for words, based on the distributional hypothesis. These representations are now commonly called word embeddings and, in addition to encoding surprisingly good syntactic and semantic information, have been proven useful as extra features in many downstream NLP tasks.},
  pubstate = {preprint},
  keywords = {A.1,Computer Science - Computation and Language,Computer Science - Machine Learning,I.2.7,Statistics - Machine Learning},
  file = {/Users/max18768/Zotero/storage/J9FX9QHF/Almeida and Xexéo - 2023 - Word Embeddings A Survey.pdf;/Users/max18768/Zotero/storage/DPI8VERS/1901.html}
}

@inproceedings{auerDBpediaNucleusWeb2007,
  title = {{{DBpedia}}: {{A Nucleus}} for a {{Web}} of {{Open Data}}},
  shorttitle = {{{DBpedia}}},
  booktitle = {The {{Semantic Web}}},
  author = {Auer, Sören and Bizer, Christian and Kobilarov, Georgi and Lehmann, Jens and Cyganiak, Richard and Ives, Zachary},
  editor = {Aberer, Karl and Choi, Key-Sun and Noy, Natasha and Allemang, Dean and Lee, Kyung-Il and Nixon, Lyndon and Golbeck, Jennifer and Mika, Peter and Maynard, Diana and Mizoguchi, Riichiro and Schreiber, Guus and Cudré-Mauroux, Philippe},
  date = {2007},
  pages = {722--735},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-76298-0_52},
  abstract = {DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information is published on the Web for human- and machine-consumption. We describe some emerging applications from the DBpedia community and show how website authors can facilitate DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data.},
  isbn = {978-3-540-76298-0},
  langid = {english},
  keywords = {Open Dataset,Relational Database Table,Sophisticated Query,SPARQL Endpoint,Triple Pattern},
  file = {/Users/max18768/Zotero/storage/WISWXB7A/Auer et al. - 2007 - DBpedia A Nucleus for a Web of Open Data.pdf}
}

@online{bahdanauNeuralMachineTranslation2016,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  date = {2016-05-19},
  eprint = {1409.0473},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1409.0473},
  url = {http://arxiv.org/abs/1409.0473},
  urldate = {2024-04-06},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/max18768/Zotero/storage/GJLDS9EL/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf;/Users/max18768/Zotero/storage/M9V3D2EA/1409.html}
}

@inproceedings{bengioNeuralProbabilisticLanguage2000,
  title = {A {{Neural Probabilistic Language Model}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal},
  date = {2000},
  volume = {13},
  publisher = {MIT Press},
  url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/728f206c2a01bf572b5940d7d9a8fa4c-Abstract.html},
  urldate = {2024-04-04},
  abstract = {A goal  of statistical language modeling is  to  learn  the joint probability  function of sequences of words.  This is intrinsically difficult because of  the curse of dimensionality:  we propose to fight it with its own weapons.  In the proposed approach one learns simultaneously (1) a distributed rep(cid:173) resentation for each word (i.e.  a similarity between words) along with (2)  the probability function for word sequences, expressed with these repre(cid:173) sentations.  Generalization is  obtained because a sequence of words that  has  never been seen before gets  high probability if it is  made of words  that are similar to words forming an already seen sentence.  We report on  experiments using neural networks for the probability function, showing  on  two  text  corpora that  the  proposed approach  very  significantly  im(cid:173) proves on a state-of-the-art trigram model.},
  file = {/Users/max18768/Zotero/storage/HNVIMHRM/Bengio et al. - 2000 - A Neural Probabilistic Language Model.pdf}
}

@inproceedings{bevilacquaRecentTrendsWord2021,
  title = {Recent {{Trends}} in {{Word Sense Disambiguation}}: {{A Survey}}},
  shorttitle = {Recent {{Trends}} in {{Word Sense Disambiguation}}},
  author = {Bevilacqua, Michele and Pasini, Tommaso and Raganato, Alessandro and Navigli, Roberto},
  date = {2021-08-09},
  volume = {5},
  pages = {4330--4338},
  issn = {1045-0823},
  doi = {10.24963/ijcai.2021/593},
  url = {https://www.ijcai.org/proceedings/2021/593},
  urldate = {2023-06-22},
  abstract = {Electronic proceedings of IJCAI 2021},
  eventtitle = {Twenty-{{Ninth International Joint Conference}} on {{Artificial Intelligence}}},
  langid = {english},
  file = {/Users/max18768/Zotero/storage/PES3YZW5/Bevilacqua et al. - 2021 - Recent Trends in Word Sense Disambiguation A Surv.pdf}
}

@online{bhattacharjeeTextTransformationsContrastive2022,
  title = {Text {{Transformations}} in {{Contrastive Self-Supervised Learning}}: {{A Review}}},
  shorttitle = {Text {{Transformations}} in {{Contrastive Self-Supervised Learning}}},
  author = {Bhattacharjee, Amrita and Karami, Mansooreh and Liu, Huan},
  date = {2022-06-06},
  eprint = {2203.12000},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2203.12000},
  url = {http://arxiv.org/abs/2203.12000},
  urldate = {2023-08-01},
  abstract = {Contrastive self-supervised learning has become a prominent technique in representation learning. The main step in these methods is to contrast semantically similar and dissimilar pairs of samples. However, in the domain of Natural Language Processing (NLP), the augmentation methods used in creating similar pairs with regard to contrastive learning (CL) assumptions are challenging. This is because, even simply modifying a word in the input might change the semantic meaning of the sentence, and hence, would violate the distributional hypothesis. In this review paper, we formalize the contrastive learning framework, emphasize the considerations that need to be addressed in the data transformation step, and review the state-of-the-art methods and evaluations for contrastive representation learning in NLP. Finally, we describe some challenges and potential directions for learning better text representations using contrastive methods.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/max18768/Zotero/storage/QAQYMN9Y/Bhattacharjee et al. - 2022 - Text Transformations in Contrastive Self-Supervise.pdf;/Users/max18768/Zotero/storage/I9J2UVBX/2203.html}
}

@article{bojanowskiEnrichingWordVectors2017,
  title = {Enriching {{Word Vectors}} with {{Subword Information}}},
  author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
  date = {2017},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  volume = {5},
  pages = {135--146},
  publisher = {MIT Press},
  location = {Cambridge, MA},
  doi = {10.1162/tacl_a_00051},
  url = {https://aclanthology.org/Q17-1010},
  urldate = {2023-07-31},
  abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
  file = {/Users/max18768/Zotero/storage/8IP8ZNRN/Bojanowski et al. - 2017 - Enriching Word Vectors with Subword Information.pdf}
}

@online{bollegalaSurveyWordMetaEmbedding2022,
  title = {A {{Survey}} on {{Word Meta-Embedding Learning}}},
  author = {Bollegala, Danushka and O'Neill, James},
  date = {2022-04-01},
  doi = {10.48550/arXiv.2204.11660},
  url = {https://ui.adsabs.harvard.edu/abs/2022arXiv220411660B},
  urldate = {2024-04-04},
  abstract = {Meta-embedding (ME) learning is an emerging approach that attempts to learn more accurate word embeddings given existing (source) word embeddings as the sole input. Due to their ability to incorporate semantics from multiple source embeddings in a compact manner with superior performance, ME learning has gained popularity among practitioners in NLP. To the best of our knowledge, there exist no prior systematic survey on ME learning and this paper attempts to fill this need. We classify ME learning methods according to multiple factors such as whether they (a) operate on static or contextualised embeddings, (b) trained in an unsupervised manner or (c) fine-tuned for a particular task/domain. Moreover, we discuss the limitations of existing ME learning methods and highlight potential future research directions.},
  organization = {arXiv e-prints},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {ADS Bibcode: 2022arXiv220411660B},
  file = {/Users/max18768/Zotero/storage/CQW37YPM/Bollegala and O'Neill - 2022 - A Survey on Word Meta-Embedding Learning.pdf}
}

@inproceedings{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020},
  volume = {33},
  pages = {1877--1901},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
  urldate = {2024-04-05},
  abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
  file = {/Users/max18768/Zotero/storage/9R25QQAA/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf}
}

@online{cerUniversalSentenceEncoder2018,
  title = {Universal {{Sentence Encoder}}},
  author = {Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St and Constant, Noah and Guajardo-Cespedes, Mario and Yuan, Steve and Tar, Chris and Sung, Yun-Hsuan and Strope, Brian and Kurzweil, Ray},
  date = {2018-04-12},
  eprint = {1803.11175},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1803.11175},
  url = {http://arxiv.org/abs/1803.11175},
  urldate = {2024-04-05},
  abstract = {We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests (WEAT) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on TF Hub.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/max18768/Zotero/storage/M4E89PAF/Cer et al. - 2018 - Universal Sentence Encoder.pdf;/Users/max18768/Zotero/storage/B3F3ZZU5/1803.html}
}

@article{chandrasekaranEvolutionSemanticSimilarity2021,
  title = {Evolution of {{Semantic Similarity}}—{{A Survey}}},
  author = {Chandrasekaran, Dhivya and Mago, Vijay},
  date = {2021-02-18},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {54},
  number = {2},
  pages = {41:1--41:37},
  issn = {0360-0300},
  doi = {10.1145/3440755},
  url = {https://dl.acm.org/doi/10.1145/3440755},
  urldate = {2023-06-17},
  abstract = {Estimating the semantic similarity between text data is one of the challenging and open research problems in the field of Natural Language Processing (NLP). The versatility of natural language makes it difficult to define rule-based methods for determining semantic similarity measures. To address this issue, various semantic similarity methods have been proposed over the years. This survey article traces the evolution of such methods beginning from traditional NLP techniques such as kernel-based methods to the most recent research work on transformer-based models, categorizing them based on their underlying principles as knowledge-based, corpus-based, deep neural network–based methods, and hybrid methods. Discussing the strengths and weaknesses of each method, this survey provides a comprehensive view of existing systems in place for new researchers to experiment and develop innovative ideas to address the issue of semantic similarity.},
  keywords = {corpus-based methods,knowledge-based methods,linguistics,Prio 1,Semantic similarity,supervised and unsupervised methods,word embeddings},
  file = {/Users/max18768/Zotero/storage/K9ZC823Q/chandrasekaran2021.pdf.pdf}
}

@article{chenEmpiricalStudySmoothing1999,
  title = {An Empirical Study of Smoothing Techniques for Language Modeling},
  author = {Chen, Stanley F. and Goodman, Joshua},
  date = {1999-10-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {13},
  number = {4},
  pages = {359--394},
  issn = {0885-2308},
  doi = {10.1006/csla.1999.0128},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230899901286},
  urldate = {2024-04-04},
  abstract = {We survey the most widely-used algorithms for smoothing models for language n -gram modeling. We then present an extensive empirical comparison of several of these smoothing techniques, including those described by Jelinek and Mercer (1980); Katz (1987); Bell, Cleary and Witten (1990); Ney, Essen and Kneser (1994), and Kneser and Ney (1995). We investigate how factors such as training data size, training corpus (e.g. Brown vs. Wall Street Journal), count cutoffs, and n -gram order (bigram vs. trigram) affect the relative performance of these methods, which is measured through the cross-entropy of test data. We find that these factors can significantly affect the relative performance of models, with the most significant factor being training data size. Since no previous comparisons have examined these factors systematically, this is the first thorough characterization of the relative performance of various algorithms. In addition, we introduce methodologies for analyzing smoothing algorithm efficacy in detail, and using these techniques we motivate a novel variation of Kneser–Ney smoothing that consistently outperforms all other algorithms evaluated. Finally, results showing that improved language model smoothing leads to improved speech recognition performance are presented.},
  file = {/Users/max18768/Zotero/storage/9I2Z97MA/Chen and Goodman - 1999 - An empirical study of smoothing techniques for lan.pdf;/Users/max18768/Zotero/storage/HKQIXFP2/S0885230899901286.html}
}

@article{chiccoAdvantagesMatthewsCorrelation2020,
  title = {The Advantages of the {{Matthews}} Correlation Coefficient ({{MCC}}) over {{F1}} Score and Accuracy in Binary Classification Evaluation},
  author = {Chicco, Davide and Jurman, Giuseppe},
  date = {2020-01-02},
  journaltitle = {BMC Genomics},
  shortjournal = {BMC Genomics},
  volume = {21},
  number = {1},
  pages = {6},
  issn = {1471-2164},
  doi = {10.1186/s12864-019-6413-7},
  url = {https://doi.org/10.1186/s12864-019-6413-7},
  urldate = {2024-04-25},
  abstract = {To evaluate binary classifications and their confusion matrices, scientific researchers can employ several statistical rates, accordingly to the goal of the experiment they are investigating. Despite being a crucial issue in machine learning, no widespread consensus has been reached on a unified elective chosen measure yet. Accuracy and F1 score computed on confusion matrices have been (and still are) among the most popular adopted metrics in binary classification tasks. However, these statistical measures can dangerously show overoptimistic inflated results, especially on imbalanced datasets.},
  langid = {english},
  keywords = {Accuracy,Binary classification,Biostatistics,Confusion matrices,Dataset imbalance,F1 score,Genomics,Machine learning,Matthews correlation coefficient},
  file = {/Users/max18768/Zotero/storage/NI6BC7PY/Chicco and Jurman - 2020 - The advantages of the Matthews correlation coeffic.pdf}
}

@inproceedings{choPropertiesNeuralMachine2014,
  title = {On the {{Properties}} of {{Neural Machine Translation}}: {{Encoder}}–{{Decoder Approaches}}},
  shorttitle = {On the {{Properties}} of {{Neural Machine Translation}}},
  booktitle = {Proceedings of {{SSST-8}}, {{Eighth Workshop}} on {{Syntax}}, {{Semantics}} and {{Structure}} in {{Statistical Translation}}},
  author = {Cho, Kyunghyun and family=Merriënboer, given=Bart, prefix=van, useprefix=true and Bahdanau, Dzmitry and Bengio, Yoshua},
  editor = {Wu, Dekai and Carpuat, Marine and Carreras, Xavier and Vecchi, Eva Maria},
  date = {2014-10},
  pages = {103--111},
  publisher = {Association for Computational Linguistics},
  location = {Doha, Qatar},
  doi = {10.3115/v1/W14-4012},
  url = {https://aclanthology.org/W14-4012},
  urldate = {2024-04-07},
  eventtitle = {{{SSST}} 2014},
  file = {/Users/max18768/Zotero/storage/8KWT2F6I/Cho et al. - 2014 - On the Properties of Neural Machine Translation E.pdf}
}

@incollection{chowdharyNaturalLanguageProcessing2020,
  title = {Natural {{Language Processing}}},
  booktitle = {Fundamentals of {{Artificial Intelligence}}},
  author = {Chowdhary, K. R.},
  editor = {Chowdhary, K.R.},
  date = {2020},
  pages = {603--649},
  publisher = {Springer India},
  location = {New Delhi},
  doi = {10.1007/978-81-322-3972-7_19},
  url = {https://doi.org/10.1007/978-81-322-3972-7_19},
  urldate = {2024-03-05},
  abstract = {The abundant volume of natural language text in the connected world, though having a large content of knowledge, but it is becoming increasingly difficult to disseminate it by a human to discover the knowledge/wisdom in it, specifically within any given time limits. The automated NLP is aimed to do this job effectively and with accuracy, like a human does it (for a limited of amount text). This chapter presents the challenges of NLP, progress so far made in this field, NLP applications, components of NLP, and grammar of English language—the way machine requires it. In addition, covers the specific areas like probabilistic parsing, ambiguities and their resolution, information extraction, discourse analysis, NL question-answering, commonsense interfaces, commonsense thinking and reasoning, causal-diversity, and various tools for NLP. Finally, the chapter summary, and a set of relevant exercises are presented.},
  isbn = {978-81-322-3972-7},
  langid = {english},
  keywords = {Ambiguity resolution,Causal-diversity,Challenges of NLP,Commonsense interfaces,Commonsense reasoning,Commonsense thinking,Discourse analysis,Natural language parsing,Natural language processing,NL ambiguities,NLP tools,Probabilistic parsing,Question-answering},
  file = {/Users/max18768/Zotero/storage/FRQ86J93/Chowdhary - 2020 - Natural Language Processing.pdf}
}

@online{chuangDiffCSEDifferencebasedContrastive2022,
  title = {{{DiffCSE}}: {{Difference-based Contrastive Learning}} for {{Sentence Embeddings}}},
  shorttitle = {{{DiffCSE}}},
  author = {Chuang, Yung-Sung and Dangovski, Rumen and Luo, Hongyin and Zhang, Yang and Chang, Shiyu and Soljačić, Marin and Li, Shang-Wen and Yih, Wen-tau and Kim, Yoon and Glass, James},
  date = {2022-04-21},
  eprint = {2204.10298},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2204.10298},
  url = {http://arxiv.org/abs/2204.10298},
  urldate = {2023-08-01},
  abstract = {We propose DiffCSE, an unsupervised contrastive learning framework for learning sentence embeddings. DiffCSE learns sentence embeddings that are sensitive to the difference between the original sentence and an edited sentence, where the edited sentence is obtained by stochastically masking out the original sentence and then sampling from a masked language model. We show that DiffSCE is an instance of equivariant contrastive learning (Dangovski et al., 2021), which generalizes contrastive learning and learns representations that are insensitive to certain types of augmentations and sensitive to other "harmful" types of augmentations. Our experiments show that DiffCSE achieves state-of-the-art results among unsupervised sentence representation learning methods, outperforming unsupervised SimCSE by 2.3 absolute points on semantic textual similarity tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/max18768/Zotero/storage/GDSZ58ER/Chuang et al. - 2022 - DiffCSE Difference-based Contrastive Learning for.pdf;/Users/max18768/Zotero/storage/VCT5CTN8/2204.html}
}

@inproceedings{churchWordAssociationNorms1989,
  title = {Word {{Association Norms}}, {{Mutual Information}}, and {{Lexicography}}},
  booktitle = {27th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Church, Kenneth Ward and Hanks, Patrick},
  date = {1989-06},
  pages = {76--83},
  publisher = {Association for Computational Linguistics},
  location = {Vancouver, British Columbia, Canada},
  doi = {10.3115/981623.981633},
  url = {https://aclanthology.org/P89-1010},
  urldate = {2024-03-29},
  eventtitle = {{{ACL}} 1989},
  file = {/Users/max18768/Zotero/storage/4RHQH9AW/Church and Hanks - 1989 - Word Association Norms, Mutual Information, and Le.pdf}
}

@article{collobertNaturalLanguageProcessing2011,
  title = {Natural {{Language Processing}} ({{Almost}}) from {{Scratch}}},
  author = {Collobert, Ronan and Weston, Jason and Bottou, Léon and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
  date = {2011-11-01},
  journaltitle = {The Journal of Machine Learning Research},
  shortjournal = {J. Mach. Learn. Res.},
  volume = {12},
  pages = {2493--2537},
  issn = {1532-4435},
  abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.},
  issue = {null},
  file = {/Users/max18768/Zotero/storage/UY8JSKJI/Collobert et al. - 2011 - Natural Language Processing (Almost) from Scratch.pdf}
}

@inproceedings{collobertUnifiedArchitectureNatural2008,
  title = {A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning},
  shorttitle = {A Unified Architecture for Natural Language Processing},
  booktitle = {Proceedings of the 25th International Conference on {{Machine}} Learning},
  author = {Collobert, Ronan and Weston, Jason},
  date = {2008-07-05},
  series = {{{ICML}} '08},
  pages = {160--167},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1390156.1390177},
  url = {https://dl.acm.org/doi/10.1145/1390156.1390177},
  urldate = {2024-04-03},
  abstract = {We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance.},
  isbn = {978-1-60558-205-4},
  file = {/Users/max18768/Zotero/storage/UJRE7RPQ/Collobert and Weston - 2008 - A unified architecture for natural language proces.pdf}
}

@inproceedings{conneauSupervisedLearningUniversal2017,
  title = {Supervised {{Learning}} of {{Universal Sentence Representations}} from {{Natural Language Inference Data}}},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Conneau, Alexis and Kiela, Douwe and Schwenk, Holger and Barrault, Loïc and Bordes, Antoine},
  editor = {Palmer, Martha and Hwa, Rebecca and Riedel, Sebastian},
  date = {2017-09},
  pages = {670--680},
  publisher = {Association for Computational Linguistics},
  location = {Copenhagen, Denmark},
  doi = {10.18653/v1/D17-1070},
  url = {https://aclanthology.org/D17-1070},
  urldate = {2024-04-07},
  abstract = {Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.},
  eventtitle = {{{EMNLP}} 2017},
  file = {/Users/max18768/Zotero/storage/C9CCLRES/Conneau et al. - 2017 - Supervised Learning of Universal Sentence Represen.pdf}
}

@online{daiSemisupervisedSequenceLearning2015,
  title = {Semi-Supervised {{Sequence Learning}}},
  author = {Dai, Andrew M. and Le, Quoc V.},
  date = {2015-11-04},
  eprint = {1511.01432},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1511.01432},
  url = {http://arxiv.org/abs/1511.01432},
  urldate = {2024-04-07},
  abstract = {We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/max18768/Zotero/storage/B8AY2JSE/Dai and Le - 2015 - Semi-supervised Sequence Learning.pdf;/Users/max18768/Zotero/storage/4RNLKZ7S/1511.html}
}

@article{deerwesterIndexingLatentSemantic1990,
  title = {Indexing by Latent Semantic Analysis},
  author = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
  date = {1990},
  journaltitle = {Journal of the American Society for Information Science},
  volume = {41},
  number = {6},
  pages = {391--407},
  issn = {1097-4571},
  doi = {10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9},
  urldate = {2024-03-29},
  abstract = {A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (“semantic structure”) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising. © 1990 John Wiley \& Sons, Inc.},
  langid = {english},
  file = {/Users/max18768/Zotero/storage/KVNPFWD2/Deerwester et al. - 1990 - Indexing by latent semantic analysis.pdf;/Users/max18768/Zotero/storage/9WU2ST6K/(SICI)1097-4571(199009)416391AID-ASI13.0.html}
}

@inproceedings{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  date = {2019-06},
  pages = {4171--4186},
  publisher = {Association for Computational Linguistics},
  location = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1423},
  url = {https://aclanthology.org/N19-1423},
  urldate = {2023-07-31},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  eventtitle = {{{NAACL-HLT}} 2019},
  file = {/Users/max18768/Zotero/storage/XWP3KIKZ/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf}
}

@inproceedings{ethayarajhHowContextualAre2019,
  title = {How {{Contextual}} Are {{Contextualized Word Representations}}? {{Comparing}} the {{Geometry}} of {{BERT}}, {{ELMo}}, and {{GPT-2 Embeddings}}},
  shorttitle = {How {{Contextual}} Are {{Contextualized Word Representations}}?},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Ethayarajh, Kawin},
  date = {2019-11},
  pages = {55--65},
  publisher = {Association for Computational Linguistics},
  location = {Hong Kong, China},
  doi = {10.18653/v1/D19-1006},
  url = {https://aclanthology.org/D19-1006},
  urldate = {2023-07-25},
  abstract = {Replacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks. However, just how contextual are the contextualized representations produced by models such as ELMo and BERT? Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations? For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model. While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations. In all layers of ELMo, BERT, and GPT-2, on average, less than 5\% of the variance in a word's contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations.},
  eventtitle = {{{EMNLP-IJCNLP}} 2019},
  file = {/Users/max18768/Zotero/storage/4QH29R33/Ethayarajh - 2019 - How Contextual are Contextualized Word Representat.pdf}
}

@online{evansMachineTranslationMining2016,
  type = {SSRN Scholarly Paper},
  title = {Machine {{Translation}}: {{Mining Text}} for {{Social Theory}}},
  shorttitle = {Machine {{Translation}}},
  author = {Evans, James A. and Aceves, Pedro},
  date = {2016-07-01},
  number = {2822747},
  location = {Rochester, NY},
  doi = {10.1146/annurev-soc-081715-074206},
  url = {https://papers.ssrn.com/abstract=2822747},
  urldate = {2023-06-15},
  abstract = {More of the social world lives within electronic text than ever before, from collective activity on the web, social media, and instant messaging to online transactions, government intelligence, and digitized libraries. This supply of text has elicited demand for natural language processing and machine learning tools to filter, search, and translate text into valuable data. We survey some of the most exciting computational approaches to text analysis, highlighting both supervised methods that extend old theories to new data and unsupervised techniques that discover hidden regularities worth theorizing. We then review recent research that uses these tools to develop social insight by exploring (a) collective attention and reasoning through the content of communication; (b) social relationships through the process of communication; and (c) social states, roles, and moves identified through heterogeneous signals within communication. We highlight social questions for which these advances could offer powerful new insight.},
  langid = {english},
  pubstate = {preprint},
  keywords = {James A. Evans,Machine Translation: Mining Text for Social Theory,Pedro Aceves,SSRN},
  file = {/Users/max18768/Zotero/storage/DS475XWS/evans2015.pdf.pdf}
}

@inproceedings{gabrilovich2007computing,
  title = {Computing Semantic Relatedness Using {{Wikipedia-based}} Explicit Semantic Analysis.},
  booktitle = {{{IJcAI}}},
  author = {Gabrilovich, Evgeniy and Markovitch, Shaul and others},
  date = {2007},
  volume = {7},
  pages = {1606--1611},
  file = {/Users/max18768/Zotero/storage/QTTYM5SW/Gabrilovich et al. - 2007 - Computing semantic relatedness using Wikipedia-bas.pdf}
}

@article{galassiAttentionNaturalLanguage2021,
  title = {Attention in {{Natural Language Processing}}},
  author = {Galassi, Andrea and Lippi, Marco and Torroni, Paolo},
  date = {2021-10},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {32},
  number = {10},
  pages = {4291--4308},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2020.3019893},
  url = {https://ieeexplore.ieee.org/abstract/document/9194070},
  urldate = {2024-04-06},
  abstract = {Attention is an increasingly popular mechanism used in a wide range of neural architectures. The mechanism itself has been realized in a variety of formats. However, because of the fast-paced advances in this domain, a systematic overview of attention is still missing. In this article, we define a unified model for attention architectures in natural language processing, with a focus on those designed to work with vector representations of the textual data. We propose a taxonomy of attention models according to four dimensions: the representation of the input, the compatibility function, the distribution function, and the multiplicity of the input and/or output. We present the examples of how prior information can be exploited in attention models and discuss ongoing research efforts and open challenges in the area, providing the first extensive categorization of the vast body of literature in this exciting domain.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}} and {{Learning Systems}}},
  keywords = {Computational modeling,Computer architecture,Natural language processing,Natural language processing (NLP),neural attention,neural networks,Neural networks,review,survey,Task analysis,Taxonomy,Visualization},
  file = {/Users/max18768/Zotero/storage/SI79G93N/Galassi et al. - 2021 - Attention in Natural Language Processing.pdf;/Users/max18768/Zotero/storage/LV3DXMI4/9194070.html}
}

@inproceedings{ganitkevitchPPDBParaphraseDatabase2013,
  title = {{{PPDB}}: {{The Paraphrase Database}}},
  shorttitle = {{{PPDB}}},
  booktitle = {Proceedings of the 2013 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Ganitkevitch, Juri and Van Durme, Benjamin and Callison-Burch, Chris},
  editor = {Vanderwende, Lucy and Daumé III, Hal and Kirchhoff, Katrin},
  date = {2013-06},
  pages = {758--764},
  publisher = {Association for Computational Linguistics},
  location = {Atlanta, Georgia},
  url = {https://aclanthology.org/N13-1092},
  urldate = {2024-03-27},
  eventtitle = {{{NAACL-HLT}} 2013},
  file = {/Users/max18768/Zotero/storage/ZZ6DBDAR/Ganitkevitch et al. - 2013 - PPDB The Paraphrase Database.pdf}
}

@inproceedings{gaoSimCSESimpleContrastive2021,
  title = {{{SimCSE}}: {{Simple Contrastive Learning}} of {{Sentence Embeddings}}},
  shorttitle = {{{SimCSE}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
  editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
  date = {2021-11},
  pages = {6894--6910},
  publisher = {Association for Computational Linguistics},
  location = {Online and Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.emnlp-main.552},
  url = {https://aclanthology.org/2021.emnlp-main.552},
  urldate = {2024-04-05},
  abstract = {This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using “entailment” pairs as positives and “contradiction” pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3\% and 81.6\% Spearman's correlation respectively, a 4.2\% and 2.2\% improvement compared to previous best results. We also show—both theoretically and empirically—that contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.},
  eventtitle = {{{EMNLP}} 2021},
  file = {/Users/max18768/Zotero/storage/FXWHHIVY/Gao et al. - 2021 - SimCSE Simple Contrastive Learning of Sentence Em.pdf}
}

@online{gaoSimCSESimpleContrastive2022,
  title = {{{SimCSE}}: {{Simple Contrastive Learning}} of {{Sentence Embeddings}}},
  shorttitle = {{{SimCSE}}},
  author = {Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
  date = {2022-05-18},
  eprint = {2104.08821},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2104.08821},
  url = {http://arxiv.org/abs/2104.08821},
  urldate = {2022-12-13},
  abstract = {This paper presents SimCSE, a simple contrastive learning framework that greatly advances state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation, and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework by using "entailment" pairs as positives and "contradiction" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3\% and 81.6\% Spearman's correlation respectively, a 4.2\% and 2.2\% improvement compared to the previous best results. We also show -- both theoretically and empirically -- that the contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/max18768/Zotero/storage/2CEFWD5L/Gao et al. - 2022 - SimCSE Simple Contrastive Learning of Sentence Em.pdf;/Users/max18768/Zotero/storage/ZUAGJRDI/2104.html}
}

@inproceedings{garcia-ferreroBenchmarkingMetaembeddingsWhat2021,
  title = {Benchmarking {{Meta-embeddings}}: {{What Works}} and {{What Does Not}}},
  shorttitle = {Benchmarking {{Meta-embeddings}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2021},
  author = {García-Ferrero, Iker and Agerri, Rodrigo and Rigau, German},
  date = {2021-11},
  pages = {3957--3972},
  publisher = {Association for Computational Linguistics},
  location = {Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.findings-emnlp.333},
  url = {https://aclanthology.org/2021.findings-emnlp.333},
  urldate = {2023-08-01},
  abstract = {In the last few years, several methods have been proposed to build meta-embeddings. The general aim was to obtain new representations integrating complementary knowledge from different source pre-trained embeddings thereby improving their overall quality. However, previous meta-embeddings have been evaluated using a variety of methods and datasets, which makes it difficult to draw meaningful conclusions regarding the merits of each approach. In this paper we propose a unified common framework, including both intrinsic and extrinsic tasks, for a fair and objective meta-embeddings evaluation. Furthermore, we present a new method to generate meta-embeddings, outperforming previous work on a large number of intrinsic evaluation benchmarks. Our evaluation framework also allows us to conclude that previous extrinsic evaluations of meta-embeddings have been overestimated.},
  eventtitle = {Findings 2021},
  file = {/Users/max18768/Zotero/storage/X57QCF8Y/García-Ferrero et al. - 2021 - Benchmarking Meta-embeddings What Works and What .pdf}
}

@article{gasparettoSurveyTextClassification2022,
  title = {A {{Survey}} on {{Text Classification Algorithms}}: {{From Text}} to {{Predictions}}},
  shorttitle = {A {{Survey}} on {{Text Classification Algorithms}}},
  author = {Gasparetto, Andrea and Marcuzzo, Matteo and Zangari, Alessandro and Albarelli, Andrea},
  date = {2022-02},
  journaltitle = {Information},
  volume = {13},
  number = {2},
  pages = {83},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2078-2489},
  doi = {10.3390/info13020083},
  url = {https://www.mdpi.com/2078-2489/13/2/83},
  urldate = {2024-04-07},
  abstract = {In recent years, the exponential growth of digital documents has been met by rapid progress in text classification techniques. Newly proposed machine learning algorithms leverage the latest advancements in deep learning methods, allowing for the automatic extraction of expressive features. The swift development of these methods has led to a plethora of strategies to encode natural language into machine-interpretable data. The latest language modelling algorithms are used in conjunction with ad hoc preprocessing procedures, of which the description is often omitted in favour of a more detailed explanation of the classification step. This paper offers a concise review of recent text classification models, with emphasis on the flow of data, from raw text to output labels. We highlight the differences between earlier methods and more recent, deep learning-based methods in both their functioning and in how they transform input data. To give a better perspective on the text classification landscape, we provide an overview of datasets for the English language, as well as supplying instructions for the synthesis of two new multilabel datasets, which we found to be particularly scarce in this setting. Finally, we provide an outline of new experimental results and discuss the open research challenges posed by deep learning-based language models.},
  issue = {2},
  langid = {english},
  keywords = {deep learning,multilabel corpora,news classification,shallow learning,text classification,tokenisation,topic labelling,transformer},
  file = {/Users/max18768/Zotero/storage/7TTXDNAW/Gasparetto et al. - 2022 - A Survey on Text Classification Algorithms From T.pdf}
}

@inproceedings{ghannayWordEmbeddingEvaluation2016,
  title = {Word {{Embedding Evaluation}} and {{Combination}}},
  booktitle = {Proceedings of the {{Tenth International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}}'16)},
  author = {Ghannay, Sahar and Favre, Benoit and Estève, Yannick and Camelin, Nathalie},
  date = {2016-05},
  pages = {300--305},
  publisher = {European Language Resources Association (ELRA)},
  location = {Portorož, Slovenia},
  url = {https://aclanthology.org/L16-1046},
  urldate = {2023-07-25},
  abstract = {Word embeddings have been successfully used in several natural language processing tasks (NLP) and speech processing. Different approaches have been introduced to calculate word embeddings through neural networks. In the literature, many studies focused on word embedding evaluation, but for our knowledge, there are still some gaps. This paper presents a study focusing on a rigorous comparison of the performances of different kinds of word embeddings. These performances are evaluated on different NLP and linguistic tasks, while all the word embeddings are estimated on the same training data using the same vocabulary, the same number of dimensions, and other similar characteristics. The evaluation results reported in this paper match those in the literature, since they point out that the improvements achieved by a word embedding in one task are not consistently observed across all tasks. For that reason, this paper investigates and evaluates approaches to combine word embeddings in order to take advantage of their complementarity, and to look for the effective word embeddings that can achieve good performances on all tasks. As a conclusion, this paper provides new perceptions of intrinsic qualities of the famous word embedding families, which can be different from the ones provided by works previously published in the scientific literature.},
  eventtitle = {{{LREC}} 2016},
  file = {/Users/max18768/Zotero/storage/Q3JQYHX7/Ghannay et al. - 2016 - Word Embedding Evaluation and Combination.pdf}
}

@article{goikoetxeaBilingualEmbeddingsRandom2018a,
  title = {Bilingual Embeddings with Random Walks over Multilingual Wordnets},
  author = {Goikoetxea, Josu and Soroa, Aitor and Agirre, Eneko},
  date = {2018-06-15},
  journaltitle = {Knowledge-Based Systems},
  shortjournal = {Knowledge-Based Systems},
  volume = {150},
  pages = {218--230},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2018.03.017},
  url = {https://www.sciencedirect.com/science/article/pii/S0950705118301412},
  urldate = {2024-04-04},
  abstract = {Bilingual word embeddings represent words of two languages in the same space, and allow to transfer knowledge from one language to the other without machine translation. The main approach is to train monolingual embeddings first and then map them using bilingual dictionaries. In this work, we present a novel method to learn bilingual embeddings based on multilingual knowledge bases (KB) such as WordNet. Our method extracts bilingual information from multilingual wordnets via random walks and learns a joint embedding space in one go. We further reinforce cross-lingual equivalence adding bilingual constraints in the loss function of the popular Skip-gram model. Our experiments on twelve cross-lingual word similarity and relatedness datasets in six language pairs covering four languages show that: 1) our method outperforms the state-of-the-art mapping method using dictionaries; 2) multilingual wordnets on their own improve over text-based systems in similarity datasets; 3) the combination of wordnet-generated information and text is key for good results. Our method can be applied to richer KBs like DBpedia or BabelNet, and can be easily extended to multilingual embeddings. All our software and resources are open source.},
  keywords = {Distributional semantics,Embeddings,Multilinguality,Random walks,Wordnet},
  file = {/Users/max18768/Zotero/storage/HXEA5W8L/Goikoetxea et al. - 2018 - Bilingual embeddings with random walks over multil.pdf}
}

@book{goodfellowDeepLearning2016,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  date = {2016-11-10},
  eprint = {omivDQAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {MIT Press},
  abstract = {An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.“Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.”—Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
  isbn = {978-0-262-33737-3},
  langid = {english},
  pagetotal = {801},
  keywords = {Computers / Artificial Intelligence / General,Computers / Computer Science,Computers / Data Science / Machine Learning,Prio 1}
}

@online{gravesSpeechRecognitionDeep2013,
  title = {Speech {{Recognition}} with {{Deep Recurrent Neural Networks}}},
  author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
  date = {2013-03-22},
  eprint = {1303.5778},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1303.5778},
  url = {http://arxiv.org/abs/1303.5778},
  urldate = {2024-04-07},
  abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \textbackslash emph\{deep recurrent neural networks\}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7\% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/max18768/Zotero/storage/PAKRHBIQ/Graves et al. - 2013 - Speech Recognition with Deep Recurrent Neural Netw.pdf;/Users/max18768/Zotero/storage/BISBBT5A/1303.html}
}

@article{grimmerTextDataPromise2013,
  title = {Text as {{Data}}: {{The Promise}} and {{Pitfalls}} of {{Automatic Content Analysis Methods}} for {{Political Texts}}},
  shorttitle = {Text as {{Data}}},
  author = {Grimmer, Justin and Stewart, Brandon M.},
  date = {2013-07},
  journaltitle = {Political Analysis},
  volume = {21},
  number = {3},
  pages = {267--297},
  publisher = {Cambridge University Press},
  issn = {1047-1987, 1476-4989},
  doi = {10.1093/pan/mps028},
  url = {https://www.cambridge.org/core/journals/political-analysis/article/text-as-data-the-promise-and-pitfalls-of-automatic-content-analysis-methods-for-political-texts/F7AAC8B2909441603FEB25C156448F20},
  urldate = {2023-06-15},
  abstract = {Politics and political conflict often occur in the written and spoken word. Scholars have long recognized this, but the massive costs of analyzing even moderately sized collections of texts have hindered their use in political science research. Here lies the promise of automated text analysis: it substantially reduces the costs of analyzing large collections of text. We provide a guide to this exciting new area of research and show how, in many instances, the methods have already obtained part of their promise. But there are pitfalls to using automated methods—they are no substitute for careful thought and close reading and require extensive and problem-specific validation. We survey a wide range of new methods, provide guidance on how to validate the output of the models, and clarify misconceptions and errors in the literature. To conclude, we argue that for automated text methods to become a standard tool for political scientists, methodologists must contribute new methods and new methods of validation.},
  langid = {english},
  file = {/Users/max18768/Zotero/storage/7K82FEB6/Grimmer and Stewart - 2013 - Text as Data The Promise and Pitfalls of Automati.pdf;/Users/max18768/Zotero/storage/IKGLLVQA/grimmer2013.pdf.pdf}
}

@inproceedings{guptaObtainingBetterStatic2021,
  title = {Obtaining {{Better Static Word Embeddings Using Contextual Embedding Models}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Gupta, Prakhar and Jaggi, Martin},
  editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
  date = {2021-08},
  pages = {5241--5253},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2021.acl-long.408},
  url = {https://aclanthology.org/2021.acl-long.408},
  urldate = {2024-04-03},
  abstract = {The advent of contextual word embeddings — representations of words which incorporate semantic and syntactic information from their context—has led to tremendous improvements on a wide variety of NLP tasks. However, recent contextual models have prohibitively high computational cost in many use-cases and are often hard to interpret. In this work, we demonstrate that our proposed distillation method, which is a simple extension of CBOW-based training, allows to significantly improve computational efficiency of NLP applications, while outperforming the quality of existing static embeddings trained from scratch as well as those distilled from previously proposed methods. As a side-effect, our approach also allows a fair comparison of both contextual and static embeddings via standard lexical evaluation tasks.},
  eventtitle = {{{ACL-IJCNLP}} 2021},
  file = {/Users/max18768/Zotero/storage/GAX3FJCN/Gupta and Jaggi - 2021 - Obtaining Better Static Word Embeddings Using Cont.pdf}
}

@article{hanSurveyTechniquesApplications2021,
  title = {A Survey on the Techniques, Applications, and Performance of Short Text Semantic Similarity},
  author = {Han, Mengting and Zhang, Xuan and Yuan, Xin and Jiang, Jiahao and Yun, Wei and Gao, Chen},
  date = {2021},
  journaltitle = {Concurrency and Computation: Practice and Experience},
  volume = {33},
  number = {5},
  pages = {e5971},
  issn = {1532-0634},
  doi = {10.1002/cpe.5971},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.5971},
  urldate = {2023-07-13},
  abstract = {Short text similarity plays an important role in natural language processing (NLP). It has been applied in many fields. Due to the lack of sufficient context in the short text, it is difficult to measure the similarity. The use of semantics similarity to calculate textual similarity has attracted the attention of academia and industry and achieved better results. In this survey, we have conducted a comprehensive and systematic analysis of semantic similarity. We first propose three categories of semantic similarity: corpus-based, knowledge-based, and deep learning (DL)-based. We analyze the pros and cons of representative and novel algorithms in each category. Our analysis also includes the applications of these similarity measurement methods in other areas of NLP. We then evaluate state-of-the-art DL methods on four common datasets, which proved that DL-based can better solve the challenges of the short text similarity, such as sparsity and complexity. Especially, bidirectional encoder representations from transformer model can fully employ scarce information of short texts and semantic information and obtain higher accuracy and F1 value. We finally put forward some future directions.},
  langid = {english},
  keywords = {BERT,deep learning,Prio 2,semantic similarity,short text},
  file = {/Users/max18768/Zotero/storage/CHLP92XM/10.1002@cpe.5971.pdf.pdf;/Users/max18768/Zotero/storage/SSJKLL8Z/cpe.html}
}

@book{harispeSemanticSimilarityNatural2015,
  title = {Semantic {{Similarity}} from {{Natural Language}} and {{Ontology Analysis}}},
  author = {Harispe, Sébastien and Ranwez, Sylvie and Janaqi, Stefan and Montmain, Jacky},
  date = {2015},
  eprint = {1704.05295},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.2200/S00639ED1V01Y201504HLT027},
  url = {http://arxiv.org/abs/1704.05295},
  urldate = {2023-06-19},
  abstract = {Artificial Intelligence federates numerous scientific fields in the aim of developing machines able to assist human operators performing complex treatments -- most of which demand high cognitive skills (e.g. learning or decision processes). Central to this quest is to give machines the ability to estimate the likeness or similarity between things in the way human beings estimate the similarity between stimuli. In this context, this book focuses on semantic measures: approaches designed for comparing semantic entities such as units of language, e.g. words, sentences, or concepts and instances defined into knowledge bases. The aim of these measures is to assess the similarity or relatedness of such semantic entities by taking into account their semantics, i.e. their meaning -- intuitively, the words tea and coffee, which both refer to stimulating beverage, will be estimated to be more semantically similar than the words toffee (confection) and coffee, despite that the last pair has a higher syntactic similarity. The two state-of-the-art approaches for estimating and quantifying semantic similarities/relatedness of semantic entities are presented in detail: the first one relies on corpora analysis and is based on Natural Language Processing techniques and semantic models while the second is based on more or less formal, computer-readable and workable forms of knowledge such as semantic networks, thesaurus or ontologies. (...) Beyond a simple inventory and categorization of existing measures, the aim of this monograph is to convey novices as well as researchers of these domains towards a better understanding of semantic similarity estimation and more generally semantic measures.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Prio 1},
  file = {/Users/max18768/Zotero/storage/6MP3F3YL/Harispe et al. - 2015 - Semantic Similarity from Natural Language and Onto.pdf;/Users/max18768/Zotero/storage/UIFSXE5V/1704.html}
}

@article{harrisDistributionalStructure1954,
  title = {Distributional {{Structure}}},
  author = {Harris, Zellig S.},
  date = {1954-08-01},
  journaltitle = {WORD},
  volume = {10},
  number = {2-3},
  pages = {146--162},
  publisher = {Routledge},
  issn = {0043-7956},
  doi = {10.1080/00437956.1954.11659520},
  url = {https://doi.org/10.1080/00437956.1954.11659520},
  urldate = {2024-03-27},
  file = {/Users/max18768/Zotero/storage/5BXMI3EH/Harris - 1954 - Distributional Structure.pdf}
}

@inproceedings{hePairwiseWordInteraction2016,
  title = {Pairwise {{Word Interaction Modeling}} with {{Deep Neural Networks}} for {{Semantic Similarity Measurement}}},
  booktitle = {Proceedings of the 2016 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {He, Hua and Lin, Jimmy},
  editor = {Knight, Kevin and Nenkova, Ani and Rambow, Owen},
  date = {2016-06},
  pages = {937--948},
  publisher = {Association for Computational Linguistics},
  location = {San Diego, California},
  doi = {10.18653/v1/N16-1108},
  url = {https://aclanthology.org/N16-1108},
  urldate = {2024-04-01},
  eventtitle = {{{NAACL-HLT}} 2016},
  file = {/Users/max18768/Zotero/storage/663VLDEA/He and Lin - 2016 - Pairwise Word Interaction Modeling with Deep Neura.pdf}
}

@inproceedings{hewittDesigningInterpretingProbes2019,
  title = {Designing and {{Interpreting Probes}} with {{Control Tasks}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Hewitt, John and Liang, Percy},
  editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
  date = {2019-11},
  pages = {2733--2743},
  publisher = {Association for Computational Linguistics},
  location = {Hong Kong, China},
  doi = {10.18653/v1/D19-1275},
  url = {https://aclanthology.org/D19-1275},
  urldate = {2024-04-05},
  abstract = {Probes, supervised models trained to predict properties (like parts-of-speech) from representations (like ELMo), have achieved high accuracy on a range of linguistic tasks. But does this mean that the representations encode linguistic structure or just that the probe has learned the linguistic task? In this paper, we propose control tasks, which associate word types with random outputs, to complement linguistic tasks. By construction, these tasks can only be learned by the probe itself. So a good probe, (one that reflects the representation), should be selective, achieving high linguistic task accuracy and low control task accuracy. The selectivity of a probe puts linguistic task accuracy in context with the probe's capacity to memorize from word types. We construct control tasks for English part-of-speech tagging and dependency edge prediction, and show that popular probes on ELMo representations are not selective. We also find that dropout, commonly used to control probe complexity, is ineffective for improving selectivity of MLPs, but that other forms of regularization are effective. Finally, we find that while probes on the first layer of ELMo yield slightly better part-of-speech tagging accuracy than the second, probes on the second layer are substantially more selective, which raises the question of which layer better represents parts-of-speech.},
  eventtitle = {{{EMNLP-IJCNLP}} 2019},
  file = {/Users/max18768/Zotero/storage/C7XYB4SR/Hewitt and Liang - 2019 - Designing and Interpreting Probes with Control Tas.pdf}
}

@inproceedings{hewittStructuralProbeFinding2019,
  title = {A {{Structural Probe}} for {{Finding Syntax}} in {{Word Representations}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Hewitt, John and Manning, Christopher D.},
  editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
  date = {2019-06},
  pages = {4129--4138},
  publisher = {Association for Computational Linguistics},
  location = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1419},
  url = {https://aclanthology.org/N19-1419},
  urldate = {2024-04-05},
  abstract = {Recent work has improved our ability to detect linguistic knowledge in word representations. However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety. In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a linear transformation of a neural network's word representation space. The probe identifies a linear transformation under which squared L2 distance encodes the distance between words in the parse tree, and one in which squared L2 norm encodes depth in the parse tree. Using our probe, we show that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models' vector geometry.},
  eventtitle = {{{NAACL-HLT}} 2019},
  file = {/Users/max18768/Zotero/storage/UPSC78JM/Hewitt and Manning - 2019 - A Structural Probe for Finding Syntax in Word Repr.pdf}
}

@article{hirschbergAdvancesNaturalLanguage2015,
  title = {Advances in Natural Language Processing},
  author = {Hirschberg, Julia and Manning, Christopher D.},
  date = {2015-07-17},
  journaltitle = {Science},
  volume = {349},
  number = {6245},
  pages = {261--266},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aaa8685},
  url = {https://www.science.org/doi/abs/10.1126/science.aaa8685},
  urldate = {2023-06-15},
  abstract = {Natural language processing employs computational techniques for the purpose of learning, understanding, and producing human language content. Early computational approaches to language research focused on automating the analysis of the linguistic structure of language and developing basic technologies such as machine translation, speech recognition, and speech synthesis. Today’s researchers refine and make use of such tools in real-world applications, creating spoken dialogue systems and speech-to-speech translation engines, mining social media for information about health or finance, and identifying sentiment and emotion toward products and services. We describe successes and challenges in this rapidly advancing area.},
  file = {/Users/max18768/Zotero/storage/J9BT4UJV/hirschberg2015.pdf.pdf}
}

@article{hochreiterLongShortTermMemory1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  date = {1997-11},
  journaltitle = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  url = {https://ieeexplore.ieee.org/abstract/document/6795963},
  urldate = {2024-04-06},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  eventtitle = {Neural {{Computation}}},
  file = {/Users/max18768/Zotero/storage/JUL4BUYK/6795963.html}
}

@article{honnibal2020spacy,
  title = {{{spaCy}}: {{Industrial-strength}} Natural Language Processing in Python},
  author = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
  date = {2020},
  doi = {10.5281/zenodo.1212303},
  added-at = {2023-05-22T04:49:27.000+0200},
  interhash = {2d1b3a0bb97e51df1b88d8852cd5ac01},
  intrahash = {616669ca18ac051794c0459373696942},
  keywords = {nlp},
  timestamp = {2023-05-22T04:49:27.000+0200}
}

@inproceedings{howardUniversalLanguageModel2018,
  title = {Universal {{Language Model Fine-tuning}} for {{Text Classification}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Howard, Jeremy and Ruder, Sebastian},
  date = {2018-07},
  pages = {328--339},
  publisher = {Association for Computational Linguistics},
  location = {Melbourne, Australia},
  doi = {10.18653/v1/P18-1031},
  url = {https://aclanthology.org/P18-1031},
  urldate = {2023-08-01},
  abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24\% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.},
  eventtitle = {{{ACL}} 2018},
  file = {/Users/max18768/Zotero/storage/BJ4RHWNT/Howard and Ruder - 2018 - Universal Language Model Fine-tuning for Text Clas.pdf}
}

@online{huangWhiteningBERTEasyUnsupervised2021,
  title = {{{WhiteningBERT}}: {{An Easy Unsupervised Sentence Embedding Approach}}},
  shorttitle = {{{WhiteningBERT}}},
  author = {Huang, Junjie and Tang, Duyu and Zhong, Wanjun and Lu, Shuai and Shou, Linjun and Gong, Ming and Jiang, Daxin and Duan, Nan},
  date = {2021-04-08},
  eprint = {2104.01767},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2104.01767},
  url = {http://arxiv.org/abs/2104.01767},
  urldate = {2022-12-13},
  abstract = {Producing the embedding of a sentence in an unsupervised way is valuable to natural language matching and retrieval problems in practice. In this work, we conduct a thorough examination of pretrained model based unsupervised sentence embeddings. We study on four pretrained models and conduct massive experiments on seven datasets regarding sentence semantics. We have there main findings. First, averaging all tokens is better than only using [CLS] vector. Second, combining both top andbottom layers is better than only using top layers. Lastly, an easy whitening-based vector normalization strategy with less than 10 lines of code consistently boosts the performance.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/max18768/Zotero/storage/H87Z7KXH/Huang et al. - 2021 - WhiteningBERT An Easy Unsupervised Sentence Embed.pdf;/Users/max18768/Zotero/storage/QPDXM2JW/2104.html}
}

@article{islamSemanticTextSimilarity2008,
  title = {Semantic Text Similarity Using Corpus-Based Word Similarity and String Similarity},
  author = {Islam, Aminul and Inkpen, Diana},
  date = {2008-07-24},
  journaltitle = {ACM Transactions on Knowledge Discovery from Data},
  shortjournal = {ACM Trans. Knowl. Discov. Data},
  volume = {2},
  number = {2},
  pages = {10:1--10:25},
  issn = {1556-4681},
  doi = {10.1145/1376815.1376819},
  url = {https://dl.acm.org/doi/10.1145/1376815.1376819},
  urldate = {2024-03-28},
  abstract = {We present a method for measuring the semantic similarity of texts using a corpus-based measure of semantic word similarity and a normalized and modified version of the Longest Common Subsequence (LCS) string matching algorithm. Existing methods for computing text similarity have focused mainly on either large documents or individual words. We focus on computing the similarity between two sentences or two short paragraphs. The proposed method can be exploited in a variety of applications involving textual knowledge representation and knowledge discovery. Evaluation results on two different data sets show that our method outperforms several competing methods.},
  keywords = {corpus-based measures,Semantic similarity of words,similarity of short texts},
  file = {/Users/max18768/Zotero/storage/D7ARC3EX/Islam and Inkpen - 2008 - Semantic text similarity using corpus-based word s.pdf}
}

@inproceedings{kalchbrennerConvolutionalNeuralNetwork2014,
  title = {A {{Convolutional Neural Network}} for {{Modelling Sentences}}},
  booktitle = {Proceedings of the 52nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Kalchbrenner, Nal and Grefenstette, Edward and Blunsom, Phil},
  editor = {Toutanova, Kristina and Wu, Hua},
  date = {2014-06},
  pages = {655--665},
  publisher = {Association for Computational Linguistics},
  location = {Baltimore, Maryland},
  doi = {10.3115/v1/P14-1062},
  url = {https://aclanthology.org/P14-1062},
  urldate = {2024-04-06},
  eventtitle = {{{ACL}} 2014},
  file = {/Users/max18768/Zotero/storage/U7N959JD/Kalchbrenner et al. - 2014 - A Convolutional Neural Network for Modelling Sente.pdf}
}

@inproceedings{kanerva2000random,
  title = {Random Indexing of Text Samples for Latent Semantic Analysis},
  booktitle = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  author = {Kanerva, Pentii and Kristoferson, Jan and Holst, Anders},
  date = {2000},
  volume = {22},
  number = {22},
  file = {/Users/max18768/Zotero/storage/D3Y2CIH4/Kanerva et al. - 2000 - Random indexing of text samples for latent semanti.pdf}
}

@article{khomsahAccuracyComparisonWord2Vec2022,
  title = {The {{Accuracy Comparison Between Word2Vec}} and {{FastText On Sentiment Analysis}} of {{Hotel Reviews}}},
  author = {Khomsah, Siti and Ramadhani, Rima and Wijaya, Sena},
  date = {2022-06-30},
  journaltitle = {Jurnal RESTI (Rekayasa Sistem dan Teknologi Informasi)},
  shortjournal = {Jurnal RESTI (Rekayasa Sistem dan Teknologi Informasi)},
  volume = {6},
  pages = {352--358},
  doi = {10.29207/resti.v6i3.3711},
  abstract = {Word embedding vectorization is more efficient than Bag-of-Word in word vector size. Word embedding also overcomes the loss of information related to sentence context, word order, and semantic relationships between words in sentences. Several kinds of Word Embedding are often considered for sentiment analysis, such as Word2Vec and FastText. Fast Text works on N-Gram, while Word2Vec is based on the word. This research aims to compare the accuracy of the sentiment analysis model using Word2Vec and FastText. Both models are tested in the sentiment analysis of Indonesian hotel reviews using the dataset from TripAdvisor.Word2Vec and FastText use the Skip-gram model. Both methods use the same parameters: number of features, minimum word count, number of parallel threads, and the context window size. Those vectorizers are combined by ensemble learning: Random Forest, Extra Tree, and AdaBoost. The Decision Tree is used as a baseline for measuring the performance of both models. The results showed that both FastText and Word2Vec well-to-do increase accuracy on Random Forest and Extra Tree. FastText reached higher accuracy than Word2Vec when using Extra Tree and Random Forest as classifiers. FastText leverage accuracy 8\% (baseline: Decision Tree 85\%), it is proofed by the accuracy of 93\%, with 100 estimators.},
  file = {/Users/max18768/Zotero/storage/WI9UWLFX/Khomsah et al. - 2022 - The Accuracy Comparison Between Word2Vec and FastT.pdf}
}

@inproceedings{komninosDependencyBasedEmbeddings2016,
  title = {Dependency {{Based Embeddings}} for {{Sentence Classification Tasks}}},
  booktitle = {Proceedings of the 2016 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Komninos, Alexandros and Manandhar, Suresh},
  date = {2016-06},
  pages = {1490--1500},
  publisher = {Association for Computational Linguistics},
  location = {San Diego, California},
  doi = {10.18653/v1/N16-1175},
  url = {https://aclanthology.org/N16-1175},
  urldate = {2023-08-01},
  eventtitle = {{{NAACL-HLT}} 2016},
  file = {/Users/max18768/Zotero/storage/6EPCTFXZ/Komninos and Manandhar - 2016 - Dependency Based Embeddings for Sentence Classific.pdf}
}

@article{kowsariTextClassificationAlgorithms2019,
  title = {Text {{Classification Algorithms}}: {{A Survey}}},
  shorttitle = {Text {{Classification Algorithms}}},
  author = {Kowsari, Kamran and Jafari Meimandi, Kiana and Heidarysafa, Mojtaba and Mendu, Sanjana and Barnes, Laura and Brown, Donald},
  date = {2019-04},
  journaltitle = {Information},
  volume = {10},
  number = {4},
  pages = {150},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2078-2489},
  doi = {10.3390/info10040150},
  url = {https://www.mdpi.com/2078-2489/10/4/150},
  urldate = {2024-03-06},
  abstract = {In recent years, there has been an exponential growth in the number of complex documents and texts that require a deeper understanding of machine learning methods to be able to accurately classify texts in many applications. Many machine learning approaches have achieved surpassing results in natural language processing. The success of these learning algorithms relies on their capacity to understand complex models and non-linear relationships within data. However, finding suitable structures, architectures, and techniques for text classification is a challenge for researchers. In this paper, a brief overview of text classification algorithms is discussed. This overview covers different text feature extractions, dimensionality reduction methods, existing algorithms and techniques, and evaluations methods. Finally, the limitations of each technique and their application in real-world problems are discussed.},
  issue = {4},
  langid = {english},
  keywords = {document classification,text analysis,text categorization,text classification,text mining,text representation},
  file = {/Users/max18768/Zotero/storage/PGLUN9T7/Kowsari et al. - 2019 - Text Classification Algorithms A Survey.pdf}
}

@article{kulmanovSemanticSimilarityMachine2021,
  title = {Semantic Similarity and Machine Learning with Ontologies},
  author = {Kulmanov, Maxat and Smaili, Fatima Zohra and Gao, Xin and Hoehndorf, Robert},
  date = {2021-07-01},
  journaltitle = {Briefings in Bioinformatics},
  shortjournal = {Briefings in Bioinformatics},
  volume = {22},
  number = {4},
  pages = {bbaa199},
  issn = {1477-4054},
  doi = {10.1093/bib/bbaa199},
  url = {https://doi.org/10.1093/bib/bbaa199},
  urldate = {2024-03-26},
  abstract = {Ontologies have long been employed in the life sciences to formally represent and reason over domain knowledge and they are employed in almost every major biological database. Recently, ontologies are increasingly being used to provide background knowledge in similarity-based analysis and machine learning models. The methods employed to combine ontologies and machine learning are still novel and actively being developed. We provide an overview over the methods that use ontologies to compute similarity and incorporate them in machine learning methods; in particular, we outline how semantic similarity measures and ontology embeddings can exploit the background knowledge in ontologies and how ontologies can provide constraints that improve machine learning models. The methods and experiments we describe are available as a set of executable notebooks, and we also provide a set of slides and additional resources at https://github.com/bio-ontology-research-group/machine-learning-with-ontologies.},
  file = {/Users/max18768/Zotero/storage/NB68E3TP/Kulmanov et al. - 2021 - Semantic similarity and machine learning with onto.pdf;/Users/max18768/Zotero/storage/HJ5U7565/5922325.html}
}

@article{landauerIntroductionLatentSemantic1998,
  title = {An Introduction to Latent Semantic Analysis},
  author = {Landauer, Thomas K and Foltz, Peter W. and Laham, Darrell},
  date = {1998-01-01},
  journaltitle = {Discourse Processes},
  volume = {25},
  number = {2-3},
  pages = {259--284},
  publisher = {Routledge},
  issn = {0163-853X},
  doi = {10.1080/01638539809545028},
  url = {https://doi.org/10.1080/01638539809545028},
  urldate = {2024-03-29},
  abstract = {Latent Semantic Analysis (LSA) is a theory and method for extracting and representing the contextual‐usage meaning of words by statistical computations applied to a large corpus of text (Landauer \& Dumais, 1997). The underlying idea is that the aggregate of all the word contexts in which a given word does and does not appear provides a set of mutual constraints that largely determines the similarity of meaning of words and sets of words to each other. The adequacy of LSA's reflection of human knowledge has been established in a variety of ways. For example, its scores overlap those of humans on standard vocabulary and subject matter tests; it mimics human word sorting and category judgments; it simulates word‐word and passage‐word lexical priming data; and, as reported in 3 following articles in this issue, it accurately estimates passage coherence, learnability of passages by individual students, and the quality and quantity of knowledge contained in an essay.}
}

@article{landauerSolutionPlatoProblem1997,
  title = {A Solution to {{Plato}}'s Problem: {{The}} Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge},
  shorttitle = {A Solution to {{Plato}}'s Problem},
  author = {Landauer, Thomas K. and Dumais, Susan T.},
  date = {1997},
  journaltitle = {Psychological Review},
  volume = {104},
  number = {2},
  pages = {211--240},
  publisher = {American Psychological Association},
  location = {US},
  issn = {1939-1471},
  doi = {10.1037/0033-295X.104.2.211},
  abstract = {How do people know as much as they do with as little information as they get? The problem takes many forms; learning vocabulary from text is an especially dramatic and convenient case for research. A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena. By inducing global knowledge indirectly from local co-occurrence data in a large body of representative text, LSA acquired knowledge about the full vocabulary of English at a comparable rate to schoolchildren. LSA uses no prior linguistic or perceptual similarity knowledge; it is based solely on a general mathematical learning method that achieves powerful inductive effects by extracting the right number of dimensions (e.g., 300) to represent objects and contexts. Relations to other theories, phenomena and problems are sketched. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Knowledge Level,Learning,Psycholinguistics,Semantics,Theories},
  file = {/Users/max18768/Zotero/storage/5JBFNSC8/1997-03612-001.html}
}

@online{lauscherSpecializingUnsupervisedPretraining2020,
  title = {Specializing {{Unsupervised Pretraining Models}} for {{Word-Level Semantic Similarity}}},
  author = {Lauscher, Anne and Vulić, Ivan and Ponti, Edoardo Maria and Korhonen, Anna and Glavaš, Goran},
  date = {2020-04-20},
  eprint = {1909.02339},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1909.02339},
  url = {http://arxiv.org/abs/1909.02339},
  urldate = {2023-07-13},
  abstract = {Unsupervised pretraining models have been shown to facilitate a wide range of downstream NLP applications. These models, however, retain some of the limitations of traditional static word embeddings. In particular, they encode only the distributional knowledge available in raw text corpora, incorporated through language modeling objectives. In this work, we complement such distributional knowledge with external lexical knowledge, that is, we integrate the discrete knowledge on word-level semantic similarity into pretraining. To this end, we generalize the standard BERT model to a multi-task learning setting where we couple BERT's masked language modeling and next sentence prediction objectives with an auxiliary task of binary word relation classification. Our experiments suggest that our "Lexically Informed" BERT (LIBERT), specialized for the word-level semantic similarity, yields better performance than the lexically blind "vanilla" BERT on several language understanding tasks. Concretely, LIBERT outperforms BERT in 9 out of 10 tasks of the GLUE benchmark and is on a par with BERT in the remaining one. Moreover, we show consistent gains on 3 benchmarks for lexical simplification, a task where knowledge about word-level semantic similarity is paramount.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Prio 3},
  file = {/Users/max18768/Zotero/storage/MHFBI2IU/Lauscher et al. - 2020 - Specializing Unsupervised Pretraining Models for W.pdf;/Users/max18768/Zotero/storage/RN5U5PS5/1909.html}
}

@article{lazerComputationalSocialScience2009,
  title = {Computational {{Social Science}}},
  author = {Lazer, David and Pentland, Alex and Adamic, Lada and Aral, Sinan and Barabási, Albert-László and Brewer, Devon and Christakis, Nicholas and Contractor, Noshir and Fowler, James and Gutmann, Myron and Jebara, Tony and King, Gary and Macy, Michael and Roy, Deb and Van Alstyne, Marshall},
  date = {2009-02-06},
  journaltitle = {Science},
  volume = {323},
  number = {5915},
  pages = {721--723},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.1167742},
  url = {https://www.science.org/doi/full/10.1126/science.1167742},
  urldate = {2023-06-15},
  file = {/Users/max18768/Zotero/storage/A9USLW4A/Lazer et al. - 2009 - Computational Social Science.pdf;/Users/max18768/Zotero/storage/BXT75E6T/lazer2009.pdf.pdf}
}

@online{leDistributedRepresentationsSentences2014,
  title = {Distributed {{Representations}} of {{Sentences}} and {{Documents}}},
  author = {Le, Quoc V. and Mikolov, Tomas},
  date = {2014-05-22},
  eprint = {1405.4053},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1405.4053},
  url = {http://arxiv.org/abs/1405.4053},
  urldate = {2024-04-07},
  abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/max18768/Zotero/storage/RKDARXM5/Le and Mikolov - 2014 - Distributed Representations of Sentences and Docum.pdf;/Users/max18768/Zotero/storage/UFLURVAK/1405.html}
}

@article{leeLearningPartsObjects1999,
  title = {Learning the Parts of Objects by Non-Negative Matrix Factorization},
  author = {Lee, Daniel D. and Seung, H. Sebastian},
  date = {1999-10},
  journaltitle = {Nature},
  volume = {401},
  number = {6755},
  pages = {788--791},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/44565},
  url = {https://www.nature.com/articles/44565},
  urldate = {2024-03-29},
  abstract = {Is perception of the whole based on perception of its parts? There is psychological1 and physiological2,3 evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations4,5. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science}
}

@article{levyImprovingDistributionalSimilarity2015a,
  title = {Improving {{Distributional Similarity}} with {{Lessons Learned}} from {{Word Embeddings}}},
  author = {Levy, Omer and Goldberg, Yoav and Dagan, Ido},
  editor = {Collins, Michael and Lee, Lillian},
  date = {2015},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  volume = {3},
  pages = {211--225},
  publisher = {MIT Press},
  location = {Cambridge, MA},
  doi = {10.1162/tacl_a_00134},
  url = {https://aclanthology.org/Q15-1016},
  urldate = {2024-04-04},
  abstract = {Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distributional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter optimizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.},
  file = {/Users/max18768/Zotero/storage/A7M3J475/Levy et al. - 2015 - Improving Distributional Similarity with Lessons L.pdf}
}

@inproceedings{levyNeuralWordEmbedding2014,
  title = {Neural {{Word Embedding}} as {{Implicit Matrix Factorization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Levy, Omer and Goldberg, Yoav},
  date = {2014},
  volume = {27},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2014/hash/feab05aa91085b7a8012516bc3533958-Abstract.html},
  urldate = {2024-04-04},
  abstract = {We analyze skip-gram with negative-sampling (SGNS), a word embedding method introduced by Mikolov et al., and show that it is implicitly factorizing a word-context matrix, whose cells are the pointwise mutual information (PMI) of the respective word and context pairs, shifted by a global constant. We find that another embedding method, NCE, is implicitly factorizing a similar matrix, where each cell is the (shifted) log conditional probability of a word given its context. We show that using a sparse Shifted Positive PMI word-context matrix to represent words improves results on two word similarity tasks and one of two analogy tasks. When dense low-dimensional vectors are preferred, exact factorization with SVD can achieve solutions that are at least as good as SGNS's solutions for word similarity tasks. On analogy questions SGNS remains superior to SVD. We conjecture that this stems from the weighted nature of SGNS's factorization.},
  file = {/Users/max18768/Zotero/storage/NUSVSRWS/Levy and Goldberg - 2014 - Neural Word Embedding as Implicit Matrix Factoriza.pdf}
}

@online{liGeneralTextEmbeddings2023,
  title = {Towards {{General Text Embeddings}} with {{Multi-stage Contrastive Learning}}},
  author = {Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},
  date = {2023-08-06},
  eprint = {2308.03281},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.03281},
  url = {http://arxiv.org/abs/2308.03281},
  urldate = {2024-04-05},
  abstract = {We present GTE, a general-purpose text embedding model trained with multi-stage contrastive learning. In line with recent advancements in unifying various NLP tasks into a single format, we train a unified text embedding model by employing contrastive learning over a diverse mixture of datasets from multiple sources. By significantly increasing the number of training data during both unsupervised pre-training and supervised fine-tuning stages, we achieve substantial performance gains over existing embedding models. Notably, even with a relatively modest parameter count of 110M, GTE\$\_\textbackslash text\{base\}\$ outperforms the black-box embedding API provided by OpenAI and even surpasses 10x larger text embedding models on the massive text embedding benchmark. Furthermore, without additional fine-tuning on each programming language individually, our model outperforms previous best code retrievers of similar size by treating code as text. In summary, our model achieves impressive results by effectively harnessing multi-stage contrastive learning, offering a powerful and efficient text embedding model with broad applicability across various NLP and code-related tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/max18768/Zotero/storage/ZEC2SX6P/Li et al. - 2023 - Towards General Text Embeddings with Multi-stage C.pdf;/Users/max18768/Zotero/storage/F5XN5HNY/2308.html}
}

@inproceedings{liSentenceEmbeddingsPretrained2020,
  title = {On the {{Sentence Embeddings}} from {{Pre-trained Language Models}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Li, Bohan and Zhou, Hao and He, Junxian and Wang, Mingxuan and Yang, Yiming and Li, Lei},
  editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
  date = {2020-11},
  pages = {9119--9130},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2020.emnlp-main.733},
  url = {https://aclanthology.org/2020.emnlp-main.733},
  urldate = {2024-04-05},
  abstract = {Pre-trained contextual representations like BERT have achieved great success in natural language processing. However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning of sentences. In this paper, we argue that the semantic information in the BERT embeddings is not fully exploited. We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks. The code is available at https://github.com/bohanli/BERT-flow.},
  eventtitle = {{{EMNLP}} 2020},
  file = {/Users/max18768/Zotero/storage/DPCM2DQX/Li et al. - 2020 - On the Sentence Embeddings from Pre-trained Langua.pdf}
}

@inproceedings{liuLinguisticKnowledgeTransferability2019,
  title = {Linguistic {{Knowledge}} and {{Transferability}} of {{Contextual Representations}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Liu, Nelson F. and Gardner, Matt and Belinkov, Yonatan and Peters, Matthew E. and Smith, Noah A.},
  editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
  date = {2019-06},
  pages = {1073--1094},
  publisher = {Association for Computational Linguistics},
  location = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1112},
  url = {https://aclanthology.org/N19-1112},
  urldate = {2024-04-05},
  abstract = {Contextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features of language. To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of ELMo, the OpenAI transformer language model, and BERT) with a suite of sixteen diverse probing tasks. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between recurrent neural networks (RNNs) and transformers. For instance, higher layers of RNNs are more task-specific, while transformer layers do not exhibit the same monotonic trend. In addition, to better understand what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks. For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language model pretraining on more data gives the best results.},
  eventtitle = {{{NAACL-HLT}} 2019},
  file = {/Users/max18768/Zotero/storage/TXVJZMUA/Liu et al. - 2019 - Linguistic Knowledge and Transferability of Contex.pdf}
}

@online{liuRoBERTaRobustlyOptimized2019,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  date = {2019-07-26},
  eprint = {1907.11692},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1907.11692},
  url = {http://arxiv.org/abs/1907.11692},
  urldate = {2024-04-07},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/max18768/Zotero/storage/82JL7FZK/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf;/Users/max18768/Zotero/storage/3RVJM5V8/1907.html}
}

@online{liuSurveyContextualEmbeddings2020,
  title = {A {{Survey}} on {{Contextual Embeddings}}},
  author = {Liu, Qi and Kusner, Matt J. and Blunsom, Phil},
  date = {2020-04-13},
  eprint = {2003.07278},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2003.07278},
  url = {http://arxiv.org/abs/2003.07278},
  urldate = {2023-07-31},
  abstract = {Contextual embeddings, such as ELMo and BERT, move beyond global word representations like Word2Vec and achieve ground-breaking performance on a wide range of natural language processing tasks. Contextual embeddings assign each word a representation based on its context, thereby capturing uses of words across varied contexts and encoding knowledge that transfers across languages. In this survey, we review existing contextual embedding models, cross-lingual polyglot pre-training, the application of contextual embeddings in downstream tasks, model compression, and model analyses.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/max18768/Zotero/storage/SNHIBIZP/Liu et al. - 2020 - A Survey on Contextual Embeddings.pdf;/Users/max18768/Zotero/storage/2TX5TELA/2003.html}
}

@incollection{liWordEmbeddingUnderstanding2018,
  title = {Word {{Embedding}} for {{Understanding Natural Language}}: {{A Survey}}},
  shorttitle = {Word {{Embedding}} for {{Understanding Natural Language}}},
  booktitle = {Guide to {{Big Data Applications}}},
  author = {Li, Yang and Yang, Tao},
  editor = {Srinivasan, S.},
  date = {2018},
  series = {Studies in {{Big Data}}},
  pages = {83--104},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-53817-4_4},
  url = {https://doi.org/10.1007/978-3-319-53817-4_4},
  urldate = {2023-07-26},
  abstract = {Word embedding, where semantic and syntactic features are captured from unlabeled text data, is a basic procedure in Natural Language Processing (NLP). The extracted features thus could be organized in low dimensional space. Some representative word embedding approaches include Probability Language Model, Neural Networks Language Model, Sparse Coding, etc. The state-of-the-art methods like skip-gram negative samplings, noise-contrastive estimation, matrix factorization and hierarchical structure regularizer are applied correspondingly to resolve those models. Most of these literatures are working on the observed count and co-occurrence statistic to learn the word embedding. The increasing scale of data, the sparsity of data representation, word position, and training speed are the main challenges for designing word embedding algorithms. In this survey, we first introduce the motivation and background of word embedding. Next we will introduce the methods of text representation as preliminaries, as well as some existing word embedding approaches such as Neural Network Language Model and Sparse Coding Approach, along with their evaluation metrics. In the end, we summarize the applications of word embedding and discuss its future directions.},
  isbn = {978-3-319-53817-4},
  langid = {english},
  keywords = {Neural Network Language Model,Probability Language Model,Sparse coding approach,Word embedding,Word representation},
  file = {/Users/max18768/Zotero/storage/EBUM93ZK/Li and Yang - 2018 - Word Embedding for Understanding Natural Language.pdf}
}

@article{lofiMeasuringSemanticSimilarity2015,
  title = {Measuring {{Semantic Similarity}} and {{Relatedness}} with {{Distributional}} and {{Knowledge-based Approaches}}},
  author = {Lofi, Christoph},
  date = {2015},
  journaltitle = {Information and Media Technologies},
  volume = {10},
  number = {3},
  pages = {493--501},
  doi = {10.11185/imt.10.493},
  abstract = {This paper provides a survey of different techniques for measuring semantic similarity and relatedness of word pairs. This covers both knowledge-based approaches exploiting taxonomies like WordNet, and corpus-based approaches which rely on distributional statistics. We introduce these techniques, provide evaluations of their result performance, and discuss their merits and shortcomings. A special focus is on word embeddings, a new technique which recently became popular with the AI community. While word embeddings are not fully understood yet, they show promising results for similarity tasks, and may also be suitable for capturing significantly more complex features like relational similarity.},
  file = {/Users/max18768/Zotero/storage/PGYI78BT/Lofi - 2015 - Measuring Semantic Similarity and Relatedness with.pdf}
}

@article{lopez-gazpioWordNgramAttention2019,
  title = {Word N-Gram Attention Models for Sentence Similarity and Inference},
  author = {Lopez-Gazpio, I. and Maritxalar, M. and Lapata, M. and Agirre, E.},
  date = {2019-10-15},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {132},
  pages = {1--11},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2019.04.054},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417419302842},
  urldate = {2024-04-01},
  abstract = {Semantic Textual Similarity and Natural Language Inference are two popular natural language understanding tasks used to benchmark sentence representation models where two sentences are paired. In such tasks sentences are represented as bag of words, sequences, trees or convolutions, but the attention model is based on word pairs. In this article we introduce the use of word n-grams in the attention model. Our results on five datasets show an error reduction of up to 41\% with respect to the word-based attention model. The improvements are especially relevant with low data regimes and, in the case of natural language inference, on the recently released hard subset of Natural Language Inference datasets.},
  keywords = {Attention models,Deep learning,Natural Language Inference,Natural language understanding,Semantic textual similarity},
  file = {/Users/max18768/Zotero/storage/3MVBKHAK/S0957417419302842.html}
}

@article{lundProducingHighdimensionalSemantic1996,
  title = {Producing High-Dimensional Semantic Spaces from Lexical Co-Occurrence},
  author = {Lund, Kevin and Burgess, Curt},
  date = {1996},
  journaltitle = {Behavior Research Methods, Instruments \& Computers},
  volume = {28},
  number = {2},
  pages = {203--208},
  publisher = {Psychonomic Society},
  location = {US},
  issn = {0743-3808},
  doi = {10.3758/BF03204766},
  abstract = {Studied the procedure by which high dimensional semantic (SM) spaces may be constructed in an automated fashion from text bodies, and how these spaces can model human concept similarity underlying the development of hyperspace analog to language. A "window," representing a span of words, was passed over the text body being analyzed. In Exp 1, the distances between word vectors (VCs) were studied to see if similarity in word meaning corresponded to that in VC element patterns. Exp 2 tested if the relationship found in Exp 1 could be used in categorization and classification tasks. In Exp 3, the effects of window size and distance metric and their performance as related to human performance was studied. The SM field surrounding the concept included aspects of a word's relationship to other words. The VCs could be used to classify instances of superordinate categories. SM distances correlated with human reaction times. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  keywords = {Computer Applications,Semantics,Text Structure},
  file = {/Users/max18768/Zotero/storage/DH5S9WL6/Lund and Burgess - 1996 - Producing high-dimensional semantic spaces from le.pdf;/Users/max18768/Zotero/storage/VSA964SL/1996-04635-011.html}
}

@online{manningIntroductionInformationRetrieval2008,
  title = {Introduction to {{Information Retrieval}}},
  author = {Manning, Christopher D. and Raghavan, Prabhakar and Schütze, Hinrich},
  date = {2008-07-07},
  publisher = {Cambridge University Press},
  doi = {10.1017/CBO9780511809071},
  url = {https://www.cambridge.org/highereducation/books/introduction-to-information-retrieval/669D108D20F556C5C30957D63B5AB65C},
  urldate = {2024-03-06},
  abstract = {Class-tested and coherent, this textbook teaches classical and web information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. It gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Slides and additional exercises (with solutions for lecturers) are also available through the book's supporting website to help course instructors prepare their lectures.},
  isbn = {9780511809071},
  langid = {english},
  organization = {Higher Education from Cambridge University Press},
  keywords = {Prio 1},
  file = {/Users/max18768/Zotero/storage/Z3SB6G94/Manning et al. - 2008 - Introduction to Information Retrieval.pdf}
}

@online{mccannLearnedTranslationContextualized2018,
  title = {Learned in {{Translation}}: {{Contextualized Word Vectors}}},
  shorttitle = {Learned in {{Translation}}},
  author = {McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
  date = {2018-06-20},
  eprint = {1708.00107},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1708.00107},
  url = {http://arxiv.org/abs/1708.00107},
  urldate = {2023-07-31},
  abstract = {Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/max18768/Zotero/storage/5L99XCYA/McCann et al. - 2018 - Learned in Translation Contextualized Word Vector.pdf;/Users/max18768/Zotero/storage/PPKHKTLY/1708.html}
}

@inproceedings{melamudContext2vecLearningGeneric2016,
  title = {Context2vec: {{Learning Generic Context Embedding}} with {{Bidirectional LSTM}}},
  shorttitle = {Context2vec},
  booktitle = {Proceedings of the 20th {{SIGNLL Conference}} on {{Computational Natural Language Learning}}},
  author = {Melamud, Oren and Goldberger, Jacob and Dagan, Ido},
  editor = {Riezler, Stefan and Goldberg, Yoav},
  date = {2016-08},
  pages = {51--61},
  publisher = {Association for Computational Linguistics},
  location = {Berlin, Germany},
  doi = {10.18653/v1/K16-1006},
  url = {https://aclanthology.org/K16-1006},
  urldate = {2024-04-05},
  eventtitle = {{{CoNLL}} 2016},
  file = {/Users/max18768/Zotero/storage/SNYCGSQZ/Melamud et al. - 2016 - context2vec Learning Generic Context Embedding wi.pdf}
}

@online{menezesSemanticHypergraphs2021,
  title = {Semantic {{Hypergraphs}}},
  author = {Menezes, Telmo and Roth, Camille},
  date = {2021-02-18},
  eprint = {1908.10784},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1908.10784},
  url = {http://arxiv.org/abs/1908.10784},
  urldate = {2022-07-19},
  abstract = {Approaches to Natural language processing (NLP) may be classified along a double dichotomy open/opaque - strict/adaptive. The former axis relates to the possibility of inspecting the underlying processing rules, the latter to the use of fixed or adaptive rules. We argue that many techniques fall into either the open-strict or opaque-adaptive categories. Our contribution takes steps in the open-adaptive direction, which we suggest is likely to provide key instruments for interdisciplinary research. The central idea of our approach is the Semantic Hypergraph (SH), a novel knowledge representation model that is intrinsically recursive and accommodates the natural hierarchical richness of natural language. The SH model is hybrid in two senses. First, it attempts to combine the strengths of ML and symbolic approaches. Second, it is a formal language representation that reduces but tolerates ambiguity and structural variability. We will see that SH enables simple yet powerful methods of pattern detection, and features a good compromise for intelligibility both for humans and machines. It also provides a semantically deep starting point (in terms of explicit meaning) for further algorithms to operate and collaborate on. We show how modern NLP ML-based building blocks can be used in combination with a random forest classifier and a simple search tree to parse NL to SH, and that this parser can achieve high precision in a diversity of text categories. We define a pattern language representable in SH itself, and a process to discover knowledge inference rules. We then illustrate the efficiency of the SH framework in a variety of tasks, including conjunction decomposition, open information extraction, concept taxonomy inference and co-reference resolution, and an applied example of claim and conflict analysis in a news corpus.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/Users/max18768/Zotero/storage/J373TPHB/Menezes_Roth_2021_Semantic Hypergraphs.pdf;/Users/max18768/Zotero/storage/IFHHHXU5/1908.html}
}

@article{mickusHowDissectMuppet2022,
  title = {How to {{Dissect}} a {{Muppet}}: {{The Structure}} of {{Transformer Embedding Spaces}}},
  shorttitle = {How to {{Dissect}} a {{Muppet}}},
  author = {Mickus, Timothee and Paperno, Denis and Constant, Mathieu},
  date = {2022-09-07},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  shortjournal = {Transactions of the Association for Computational Linguistics},
  volume = {10},
  pages = {981--996},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00501},
  url = {https://doi.org/10.1162/tacl_a_00501},
  urldate = {2023-06-21},
  abstract = {Pretrained embeddings based on the Transformer architecture have taken the NLP community by storm. We show that they can mathematically be reframed as a sum of vector factors and showcase how to use this reframing to study the impact of each component. We provide evidence that multi-head attentions and feed-forwards are not equally useful in all downstream applications, as well as a quantitative overview of the effects of finetuning on the overall embedding space. This approach allows us to draw connections to a wide range of previous studies, from vector space anisotropy to attention weights.},
  file = {/Users/max18768/Zotero/storage/E3RI9MDK/Mickus et al. - 2022 - How to Dissect a Muppet The Structure of Transfor.pdf;/Users/max18768/Zotero/storage/9QMPI4JU/How-to-Dissect-a-Muppet-The-Structure-of.html}
}

@inproceedings{mihalceaCorpusbasedKnowledgebasedMeasures2006,
  title = {Corpus-Based and Knowledge-Based Measures of Text Semantic Similarity},
  booktitle = {Proceedings of the 21st National Conference on {{Artificial}} Intelligence - {{Volume}} 1},
  author = {Mihalcea, Rada and Corley, Courtney and Strapparava, Carlo},
  date = {2006-07-16},
  series = {{{AAAI}}'06},
  pages = {775--780},
  publisher = {AAAI Press},
  location = {Boston, Massachusetts},
  abstract = {This paper presents a method for measuring the semantic similarity of texts, using corpus-based and knowledge-based measures of similarity. Previous work on this problem has focused mainly on either large documents (e.g. text classification, information retrieval) or individual words (e.g. synonymy tests). Given that a large fraction of the information available today, on the Web and elsewhere, consists of short text snippets (e.g. abstracts of scientific documents, imagine captions, product descriptions), in this paper we focus on measuring the semantic similarity of short texts. Through experiments performed on a paraphrase data set, we show that the semantic similarity method out-performs methods based on simple lexical matching, resulting in up to 13\% error rate reduction with respect to the traditional vector-based similarity metric.},
  isbn = {978-1-57735-281-5},
  file = {/Users/max18768/Zotero/storage/QFWHL5S8/Mihalcea et al. - 2006 - Corpus-based and knowledge-based measures of text .pdf}
}

@online{mikolovDistributedRepresentationsWords2013,
  title = {Distributed {{Representations}} of {{Words}} and {{Phrases}} and Their {{Compositionality}}},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  date = {2013-10-16},
  eprint = {1310.4546},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1310.4546},
  url = {http://arxiv.org/abs/1310.4546},
  urldate = {2023-06-22},
  abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/max18768/Zotero/storage/DCMZTKVP/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf;/Users/max18768/Zotero/storage/KABIGKIP/1310.html}
}

@online{mikolovEfficientEstimationWord2013,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  date = {2013-09-06},
  eprint = {1301.3781},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1301.3781},
  url = {http://arxiv.org/abs/1301.3781},
  urldate = {2023-07-24},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/max18768/Zotero/storage/7F2CFSU3/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf;/Users/max18768/Zotero/storage/BAX8DJHQ/1301.html}
}

@inproceedings{mikolovLinguisticRegularitiesContinuous2013,
  title = {Linguistic {{Regularities}} in {{Continuous Space Word Representations}}},
  booktitle = {Proceedings of the 2013 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
  date = {2013-06},
  pages = {746--751},
  publisher = {Association for Computational Linguistics},
  location = {Atlanta, Georgia},
  url = {https://aclanthology.org/N13-1090},
  urldate = {2023-07-31},
  eventtitle = {{{NAACL-HLT}} 2013},
  file = {/Users/max18768/Zotero/storage/7NEZLG9M/Mikolov et al. - 2013 - Linguistic Regularities in Continuous Space Word R.pdf}
}

@inproceedings{mikolovRecurrentNeuralNetwork2010,
  title = {Recurrent Neural Network Based Language Model},
  author = {Mikolov, Tomáš and Karafiát, Martin and Burget, Lukáš and Černocký, Jan and Khudanpur, Sanjeev},
  date = {2010},
  pages = {1045--1048},
  doi = {10.21437/Interspeech.2010-343},
  url = {https://www.isca-archive.org/interspeech_2010/mikolov10_interspeech.html},
  urldate = {2024-04-05},
  eventtitle = {Proc. {{Interspeech}} 2010},
  file = {/Users/max18768/Zotero/storage/FZPG7LP3/Mikolov et al. - 2010 - Recurrent neural network based language model.pdf}
}

@article{millerWordNetLexicalDatabase1995,
  title = {{{WordNet}}: A Lexical Database for {{English}}},
  shorttitle = {{{WordNet}}},
  author = {Miller, George A.},
  date = {1995-11-01},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {38},
  number = {11},
  pages = {39--41},
  issn = {0001-0782},
  doi = {10.1145/219717.219748},
  url = {https://dl.acm.org/doi/10.1145/219717.219748},
  urldate = {2024-03-27},
  abstract = {Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].},
  file = {/Users/max18768/Zotero/storage/MCV7JVFR/Miller - 1995 - WordNet a lexical database for English.pdf}
}

@article{minaeeDeepLearningBased2021,
  title = {Deep {{Learning--based Text Classification}}: {{A Comprehensive Review}}},
  shorttitle = {Deep {{Learning--based Text Classification}}},
  author = {Minaee, Shervin and Kalchbrenner, Nal and Cambria, Erik and Nikzad, Narjes and Chenaghlu, Meysam and Gao, Jianfeng},
  date = {2021-04-17},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {54},
  number = {3},
  pages = {62:1--62:40},
  issn = {0360-0300},
  doi = {10.1145/3439726},
  url = {https://dl.acm.org/doi/10.1145/3439726},
  urldate = {2023-07-31},
  abstract = {Deep learning--based models have surpassed classical machine learning--based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this article, we provide a comprehensive review of more than 150 deep learning--based models for text classification developed in recent years, and we discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and we discuss future research directions.},
  keywords = {deep learning,natural language inference,news categorization,question answering,sentiment analysis,Text classification,topic classification},
  file = {/Users/max18768/Zotero/storage/NSS5SX6L/Minaee et al. - 2021 - Deep Learning--based Text Classification A Compre.pdf}
}

@article{minRecentAdvancesNatural2023,
  title = {Recent {{Advances}} in {{Natural Language Processing}} via {{Large Pre-trained Language Models}}: {{A Survey}}},
  shorttitle = {Recent {{Advances}} in {{Natural Language Processing}} via {{Large Pre-trained Language Models}}},
  author = {Min, Bonan and Ross, Hayley and Sulem, Elior and Veyseh, Amir Pouran Ben and Nguyen, Thien Huu and Sainz, Oscar and Agirre, Eneko and Heintz, Ilana and Roth, Dan},
  date = {2023-09-14},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {56},
  number = {2},
  pages = {30:1--30:40},
  issn = {0360-0300},
  doi = {10.1145/3605943},
  url = {https://dl.acm.org/doi/10.1145/3605943},
  urldate = {2024-03-05},
  abstract = {Large, pre-trained language models (PLMs) such as BERT and GPT have drastically changed the Natural Language Processing (NLP) field. For numerous NLP tasks, approaches leveraging PLMs have achieved state-of-the-art performance. The key idea is to learn a generic, latent representation of language from a generic task once, then share it across disparate NLP tasks. Language modeling serves as the generic task, one with abundant self-supervised text available for extensive training. This article presents the key fundamental concepts of PLM architectures and a comprehensive view of the shift to PLM-driven NLP techniques. It surveys work applying the pre-training then fine-tuning, prompting, and text generation approaches. In addition, it discusses PLM limitations and suggested directions for future research.},
  keywords = {foundational models,generative AI,Large language models,neural networks},
  file = {/Users/max18768/Zotero/storage/F2KGENCJ/Min et al. - 2023 - Recent Advances in Natural Language Processing via.pdf}
}

@online{mohammadDistributionalMeasuresProxies2012,
  title = {Distributional {{Measures}} as {{Proxies}} for {{Semantic Relatedness}}},
  author = {Mohammad, Saif M. and Hirst, Graeme},
  date = {2012-03-08},
  eprint = {1203.1889},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1203.1889},
  url = {http://arxiv.org/abs/1203.1889},
  urldate = {2024-03-28},
  abstract = {The automatic ranking of word pairs as per their semantic relatedness and ability to mimic human notions of semantic relatedness has widespread applications. Measures that rely on raw data (distributional measures) and those that use knowledge-rich ontologies both exist. Although extensive studies have been performed to compare ontological measures with human judgment, the distributional measures have primarily been evaluated by indirect means. This paper is a detailed study of some of the major distributional measures; it lists their respective merits and limitations. New measures that overcome these drawbacks, that are more in line with the human notions of semantic relatedness, are suggested. The paper concludes with an exhaustive comparison of the distributional and ontology-based measures. Along the way, significant research problems are identified. Work on these problems may lead to a better understanding of how semantic relatedness is to be measured.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/max18768/Zotero/storage/YNQ84IA6/Mohammad and Hirst - 2012 - Distributional Measures as Proxies for Semantic Re.pdf;/Users/max18768/Zotero/storage/LCF5D9JN/1203.html}
}

@online{mohammadDistributionalMeasuresSemantic2012,
  title = {Distributional {{Measures}} of {{Semantic Distance}}: {{A Survey}}},
  shorttitle = {Distributional {{Measures}} of {{Semantic Distance}}},
  author = {Mohammad, Saif M. and Hirst, Graeme},
  date = {2012-03-08},
  eprint = {1203.1858},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1203.1858},
  url = {http://arxiv.org/abs/1203.1858},
  urldate = {2024-03-26},
  abstract = {The ability to mimic human notions of semantic distance has widespread applications. Some measures rely only on raw text (distributional measures) and some rely on knowledge sources such as WordNet. Although extensive studies have been performed to compare WordNet-based measures with human judgment, the use of distributional measures as proxies to estimate semantic distance has received little attention. Even though they have traditionally performed poorly when compared to WordNet-based measures, they lay claim to certain uniquely attractive features, such as their applicability in resource-poor languages and their ability to mimic both semantic similarity and semantic relatedness. Therefore, this paper presents a detailed study of distributional measures. Particular attention is paid to flesh out the strengths and limitations of both WordNet-based and distributional measures, and how distributional measures of distance can be brought more in line with human notions of semantic distance. We conclude with a brief discussion of recent work on hybrid measures.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/max18768/Zotero/storage/QFPNMS6B/Mohammad and Hirst - 2012 - Distributional Measures of Semantic Distance A Su.pdf;/Users/max18768/Zotero/storage/DMYCTVEB/1203.html}
}

@online{mohammadDistributionalMeasuresSemantic2012a,
  title = {Distributional {{Measures}} of {{Semantic Distance}}: {{A Survey}}},
  shorttitle = {Distributional {{Measures}} of {{Semantic Distance}}},
  author = {Mohammad, Saif M. and Hirst, Graeme},
  date = {2012-03-08},
  eprint = {1203.1858},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1203.1858},
  url = {http://arxiv.org/abs/1203.1858},
  urldate = {2024-03-27},
  abstract = {The ability to mimic human notions of semantic distance has widespread applications. Some measures rely only on raw text (distributional measures) and some rely on knowledge sources such as WordNet. Although extensive studies have been performed to compare WordNet-based measures with human judgment, the use of distributional measures as proxies to estimate semantic distance has received little attention. Even though they have traditionally performed poorly when compared to WordNet-based measures, they lay claim to certain uniquely attractive features, such as their applicability in resource-poor languages and their ability to mimic both semantic similarity and semantic relatedness. Therefore, this paper presents a detailed study of distributional measures. Particular attention is paid to flesh out the strengths and limitations of both WordNet-based and distributional measures, and how distributional measures of distance can be brought more in line with human notions of semantic distance. We conclude with a brief discussion of recent work on hybrid measures.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/max18768/Zotero/storage/Z3JAK3I8/Mohammad and Hirst - 2012 - Distributional Measures of Semantic Distance A Su.pdf;/Users/max18768/Zotero/storage/ZKB935BJ/1203.html}
}

@online{MTEBLeaderboardHugging2023,
  title = {{{MTEB Leaderboard}} - a {{Hugging Face Space}} by Mteb},
  date = {2023},
  url = {https://huggingface.co/spaces/mteb/leaderboard},
  urldate = {2024-04-07},
  abstract = {Discover amazing ML apps made by the community},
  file = {/Users/max18768/Zotero/storage/QAAIYUGF/leaderboard.html}
}

@online{muennighoffMTEBMassiveText2023,
  title = {{{MTEB}}: {{Massive Text Embedding Benchmark}}},
  shorttitle = {{{MTEB}}},
  author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Loïc and Reimers, Nils},
  date = {2023-03-19},
  eprint = {2210.07316},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.07316},
  url = {http://arxiv.org/abs/2210.07316},
  urldate = {2023-07-25},
  abstract = {Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/Users/max18768/Zotero/storage/RP3D73ND/Muennighoff et al. - 2023 - MTEB Massive Text Embedding Benchmark.pdf;/Users/max18768/Zotero/storage/MC7ME5FQ/2210.html}
}

@online{neelakantanTextCodeEmbeddings2022,
  title = {Text and {{Code Embeddings}} by {{Contrastive Pre-Training}}},
  author = {Neelakantan, Arvind and Xu, Tao and Puri, Raul and Radford, Alec and Han, Jesse Michael and Tworek, Jerry and Yuan, Qiming and Tezak, Nikolas and Kim, Jong Wook and Hallacy, Chris and Heidecke, Johannes and Shyam, Pranav and Power, Boris and Nekoul, Tyna Eloundou and Sastry, Girish and Krueger, Gretchen and Schnurr, David and Such, Felipe Petroski and Hsu, Kenny and Thompson, Madeleine and Khan, Tabarak and Sherbakov, Toki and Jang, Joanne and Welinder, Peter and Weng, Lilian},
  date = {2022-01-24},
  eprint = {2201.10005},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2201.10005},
  url = {http://arxiv.org/abs/2201.10005},
  urldate = {2023-08-01},
  abstract = {Text embeddings are useful features in many applications such as semantic search and computing text similarity. Previous work typically trains models customized for different use cases, varying in dataset choice, training objective and model architecture. In this work, we show that contrastive pre-training on unsupervised data at scale leads to high quality vector representations of text and code. The same unsupervised text embeddings that achieve new state-of-the-art results in linear-probe classification also display impressive semantic search capabilities and sometimes even perform competitively with fine-tuned models. On linear-probe classification accuracy averaging over 7 tasks, our best unsupervised model achieves a relative improvement of 4\% and 1.8\% over previous best unsupervised and supervised text embedding models respectively. The same text embeddings when evaluated on large-scale semantic search attains a relative improvement of 23.4\%, 14.7\%, and 10.6\% over previous best unsupervised methods on MSMARCO, Natural Questions and TriviaQA benchmarks, respectively. Similarly to text embeddings, we train code embedding models on (text, code) pairs, obtaining a 20.8\% relative improvement over prior best work on code search.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/max18768/Zotero/storage/YT3IFIQT/Neelakantan et al. - 2022 - Text and Code Embeddings by Contrastive Pre-Traini.pdf;/Users/max18768/Zotero/storage/3KMQMC8I/2201.html}
}

@online{neelakantanTextCodeEmbeddings2022a,
  title = {Text and {{Code Embeddings}} by {{Contrastive Pre-Training}}},
  author = {Neelakantan, Arvind and Xu, Tao and Puri, Raul and Radford, Alec and Han, Jesse Michael and Tworek, Jerry and Yuan, Qiming and Tezak, Nikolas and Kim, Jong Wook and Hallacy, Chris and Heidecke, Johannes and Shyam, Pranav and Power, Boris and Nekoul, Tyna Eloundou and Sastry, Girish and Krueger, Gretchen and Schnurr, David and Such, Felipe Petroski and Hsu, Kenny and Thompson, Madeleine and Khan, Tabarak and Sherbakov, Toki and Jang, Joanne and Welinder, Peter and Weng, Lilian},
  date = {2022-01-24},
  eprint = {2201.10005},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2201.10005},
  url = {http://arxiv.org/abs/2201.10005},
  urldate = {2024-04-05},
  abstract = {Text embeddings are useful features in many applications such as semantic search and computing text similarity. Previous work typically trains models customized for different use cases, varying in dataset choice, training objective and model architecture. In this work, we show that contrastive pre-training on unsupervised data at scale leads to high quality vector representations of text and code. The same unsupervised text embeddings that achieve new state-of-the-art results in linear-probe classification also display impressive semantic search capabilities and sometimes even perform competitively with fine-tuned models. On linear-probe classification accuracy averaging over 7 tasks, our best unsupervised model achieves a relative improvement of 4\% and 1.8\% over previous best unsupervised and supervised text embedding models respectively. The same text embeddings when evaluated on large-scale semantic search attains a relative improvement of 23.4\%, 14.7\%, and 10.6\% over previous best unsupervised methods on MSMARCO, Natural Questions and TriviaQA benchmarks, respectively. Similarly to text embeddings, we train code embedding models on (text, code) pairs, obtaining a 20.8\% relative improvement over prior best work on code search.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/max18768/Zotero/storage/CSIIA8WT/Neelakantan et al. - 2022 - Text and Code Embeddings by Contrastive Pre-Traini.pdf;/Users/max18768/Zotero/storage/323MH22K/2201.html}
}

@inproceedings{ngocExtendedBenchmarkSystem2020,
  title = {An {{Extended Benchmark System}} of {{Word Embedding Methods}} for {{Vulnerability Detection}}},
  booktitle = {The 4th {{International Conference}} on {{Future Networks}} and {{Distributed Systems}} ({{ICFNDS}})},
  author = {Ngoc, Hai Nguyen and Viet, Hoang Nguyen and Uehara, Tetsutaro},
  date = {2020-11-26},
  pages = {1--8},
  publisher = {ACM},
  location = {St.Petersburg Russian Federation},
  doi = {10.1145/3440749.3442661},
  url = {https://dl.acm.org/doi/10.1145/3440749.3442661},
  urldate = {2023-07-25},
  eventtitle = {{{ICFNDS}} '20: {{The}} 4th {{International Conference}} on {{Future Networks}} and {{Distributed Systems}}},
  isbn = {978-1-4503-8886-3},
  langid = {english},
  file = {/Users/max18768/Zotero/storage/VA73E69D/ngoc2020.pdf.pdf;/Users/max18768/Zotero/storage/YVF9KPPA/Ngoc et al. - 2020 - An Extended Benchmark System of Word Embedding Met.pdf}
}

@inproceedings{niLargeDualEncoders2022,
  title = {Large {{Dual Encoders Are Generalizable Retrievers}}},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Ni, Jianmo and Qu, Chen and Lu, Jing and Dai, Zhuyun and Hernandez Abrego, Gustavo and Ma, Ji and Zhao, Vincent and Luan, Yi and Hall, Keith and Chang, Ming-Wei and Yang, Yinfei},
  editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
  date = {2022-12},
  pages = {9844--9855},
  publisher = {Association for Computational Linguistics},
  location = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.669},
  url = {https://aclanthology.org/2022.emnlp-main.669},
  urldate = {2024-04-05},
  abstract = {It has been shown that dual encoders trained on one domain often fail to generalize to other domains for retrieval tasks. One widespread belief is that the bottleneck layer of a dual encoder, where the final score is simply a dot-product between a query vector and a passage vector, is too limited compared to models with fine-grained interactions between the query and the passage. In this paper, we challenge this belief by scaling up the size of the dual encoder model while keeping the bottleneck layer as a single dot-product with a fixed size. With multi-stage training, scaling up the model size brings significant improvement on a variety of retrieval tasks, especially for out-of-domain generalization. We further analyze the impact of the bottleneck layer and demonstrate diminishing improvement when scaling up the embedding size. Experimental results show that our dual encoders, Generalizable T5-based dense Retrievers (GTR), outperform previous sparse and dense retrievers on the BEIR dataset significantly. Most surprisingly, our ablation study finds that GTR is very data efficient, as it only needs 10\% of MS Marco supervised data to match the out-of-domain performance of using all supervised data.},
  eventtitle = {{{EMNLP}} 2022},
  file = {/Users/max18768/Zotero/storage/5YBIUFY7/Ni et al. - 2022 - Large Dual Encoders Are Generalizable Retrievers.pdf}
}

@inproceedings{niSentenceT5ScalableSentence2022,
  title = {Sentence-{{T5}}: {{Scalable Sentence Encoders}} from {{Pre-trained Text-to-Text Models}}},
  shorttitle = {Sentence-{{T5}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Ni, Jianmo and Hernandez Abrego, Gustavo and Constant, Noah and Ma, Ji and Hall, Keith and Cer, Daniel and Yang, Yinfei},
  editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
  date = {2022-05},
  pages = {1864--1874},
  publisher = {Association for Computational Linguistics},
  location = {Dublin, Ireland},
  doi = {10.18653/v1/2022.findings-acl.146},
  url = {https://aclanthology.org/2022.findings-acl.146},
  urldate = {2024-04-05},
  abstract = {We provide the first exploration of sentence embeddings from text-to-text transformers (T5) including the effects of scaling up sentence encoders to 11B parameters. Sentence embeddings are broadly useful for language processing tasks. While T5 achieves impressive performance on language tasks, it is unclear how to produce sentence embeddings from encoder-decoder models. We investigate three methods to construct Sentence-T5 (ST5) models: two utilize only the T5 encoder and one using the full T5 encoder-decoder. We establish a new sentence representation transfer benchmark, SentGLUE, which extends the SentEval toolkit to nine tasks from the GLUE benchmark. Our encoder-only models outperform the previous best models on both SentEval and SentGLUE transfer tasks, including semantic textual similarity (STS). Scaling up ST5 from millions to billions of parameters shown to consistently improve performance. Finally, our encoder-decoder method achieves a new state-of-the-art on STS when using sentence embeddings.},
  eventtitle = {Findings 2022},
  file = {/Users/max18768/Zotero/storage/5ACGB9LA/Ni et al. - 2022 - Sentence-T5 Scalable Sentence Encoders from Pre-t.pdf}
}

@inproceedings{niwaCoOccurrenceVectorsCorpora1994,
  title = {Co-{{Occurrence Vectors From Corpora}} vs. {{Distance Vectors From Dictionaries}}},
  booktitle = {{{COLING}} 1994 {{Volume}} 1: {{The}} 15th {{International Conference}} on {{Computational Linguistics}}},
  author = {Niwa, Yoshiki and Nitta, Yoshihiko},
  date = {1994-08},
  location = {Kyoto, Japan},
  url = {https://aclanthology.org/C94-1049},
  urldate = {2024-03-29},
  eventtitle = {{{COLING}} 1994},
  file = {/Users/max18768/Zotero/storage/ML6AVV93/Niwa and Nitta - 1994 - Co-Occurrence Vectors From Corpora vs. Distance Ve.pdf}
}

@inproceedings{p.SurveySemanticSimilarity2019,
  title = {A {{Survey}} on {{Semantic Similarity}}},
  booktitle = {2019 {{International Conference}} on {{Advances}} in {{Computing}}, {{Communication}} and {{Control}} ({{ICAC3}})},
  author = {P., Sunilkumar and Shaji, Athira P.},
  date = {2019-12},
  pages = {1--8},
  doi = {10.1109/ICAC347590.2019.9036843},
  abstract = {This paper provides a survey of semantic similarity of text documents. Semantic Similarity is an important task in Natural Language Processing (NLP). It is widely used for information retrieval, text classification, question answering, and plagiarism detection. This survey will classify different types of semantic similarity approaches such as corpus-based, knowledge-based and string-based. Various papers are reviewed and prepared performance analysis in this survey.},
  eventtitle = {2019 {{International Conference}} on {{Advances}} in {{Computing}}, {{Communication}} and {{Control}} ({{ICAC3}})},
  keywords = {Corpus Based,Cosine Similarity,Jaccard Similarity,Knowledge based systems,Knowledge Graph Based,Performance analysis,Plagiarism,Prio 2,Semantic Similarity,Semantics,String Based,Task analysis,Text categorization,WordNet},
  file = {/Users/max18768/Zotero/storage/CV2SGWY3/p2019.pdf.pdf;/Users/max18768/Zotero/storage/RDQ9STQK/P. and Shaji - 2019 - A Survey on Semantic Similarity.pdf;/Users/max18768/Zotero/storage/LHCLQ7NC/9036843.html}
}

@incollection{parsonsStratifiedSampling2017,
  title = {Stratified {{Sampling}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Parsons, Van L.},
  date = {2017},
  pages = {1--11},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/9781118445112.stat05999.pub2},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118445112.stat05999.pub2},
  urldate = {2024-01-11},
  abstract = {Stratified sampling is a probability sampling method that is implemented in sample surveys. The target population's elements are divided into distinct groups or strata where within each stratum the elements are similar to each other with respect to select characteristics of importance to the survey. Stratification is also used to increase the efficiency of a sample design with respect to survey costs and estimator precision. In this article, the foundations of stratified sampling are discussed in the framework of simple random sampling. Topics include the forming of the strata and optimal sample allocation among the strata. Practical implementation issues for stratified sampling are discussed and include systematic sampling, implicit stratification, and the construction of strata using modern software. The importance of using stratified sampling in practice is demonstrated by its usage in five major large-scale health surveys conducted in the United States and the United Kingdom. For these surveys, details of the stratification and sampling methods are provided. Topics include multistage cluster sampling within strata and the use of systematic and probability proportional to size sampling.},
  isbn = {978-1-118-44511-2},
  langid = {english},
  keywords = {cluster sampling,complex survey,health survey,implicit stratification,multistage sampling,sample allocation,sample design,sample survey,sampling frame,systematic sampling},
  file = {/Users/max18768/Zotero/storage/H3MCLKVW/9781118445112.stat05999.html}
}

@article{patilSurveyTextRepresentation2023,
  title = {A {{Survey}} of {{Text Representation}} and {{Embedding Techniques}} in {{NLP}}},
  author = {Patil, Rajvardhan and Boit, Sorio and Gudivada, Venkat and Nandigam, Jagadeesh},
  date = {2023},
  journaltitle = {IEEE Access},
  volume = {11},
  pages = {36120--36146},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3266377},
  abstract = {Natural Language Processing (NLP) is a research field where a language in consideration is processed to understand its syntactic, semantic, and sentimental aspects. The advancement in the NLP area has helped solve problems in the domains such as Neural Machine Translation, Name Entity Recognition, Sentiment Analysis, and Chatbots, to name a few. The topic of NLP broadly consists of two main parts: the representation of the input text (raw data) into numerical format (vectors or matrix) and the design of models for processing the numerical data. This paper focuses on the former part and surveys how the NLP field has evolved from rule-based, statistical to more context-sensitive learned representations. For each embedding type, we list their representation, issues they addressed, limitations, and applications. This survey covers the history of text representations from the 1970s and onwards, from regular expressions to the latest vector representations used to encode the raw text data. It demonstrates how the NLP field progressed from where it could comprehend just bits and pieces to all the significant aspects of the text over time.},
  eventtitle = {{{IEEE Access}}},
  keywords = {Electronic mail,embeddings,Grammar,Indexes,language models,literature review,Natural language processing,NLP,Semantics,Sparse matrices,survey,Text mining,text representation,Vocabulary,word embeddings,word vectors},
  file = {/Users/max18768/Zotero/storage/Y62QWT6H/Patil et al. - 2023 - A Survey of Text Representation and Embedding Tech.pdf}
}

@inproceedings{penningtonGloVeGlobalVectors2014,
  title = {{{GloVe}}: {{Global Vectors}} for {{Word Representation}}},
  shorttitle = {{{GloVe}}},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  date = {2014-10},
  pages = {1532--1543},
  publisher = {Association for Computational Linguistics},
  location = {Doha, Qatar},
  doi = {10.3115/v1/D14-1162},
  url = {https://aclanthology.org/D14-1162},
  urldate = {2023-07-26},
  eventtitle = {{{EMNLP}} 2014},
  file = {/Users/max18768/Zotero/storage/5WYU8SBW/Pennington et al. - 2014 - GloVe Global Vectors for Word Representation.pdf}
}

@inproceedings{petersDeepContextualizedWord2018,
  title = {Deep {{Contextualized Word Representations}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long Papers}})},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  date = {2018-06},
  pages = {2227--2237},
  publisher = {Association for Computational Linguistics},
  location = {New Orleans, Louisiana},
  doi = {10.18653/v1/N18-1202},
  url = {https://aclanthology.org/N18-1202},
  urldate = {2023-07-31},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  eventtitle = {{{NAACL-HLT}} 2018},
  file = {/Users/max18768/Zotero/storage/84I4PNVW/Peters et al. - 2018 - Deep Contextualized Word Representations.pdf}
}

@article{qiuPretrainedModelsNatural2020,
  title = {Pre-Trained Models for Natural Language Processing: {{A}} Survey},
  shorttitle = {Pre-Trained Models for Natural Language Processing},
  author = {Qiu, XiPeng and Sun, TianXiang and Xu, YiGe and Shao, YunFan and Dai, Ning and Huang, XuanJing},
  date = {2020-10-01},
  journaltitle = {Science China Technological Sciences},
  shortjournal = {Sci. China Technol. Sci.},
  volume = {63},
  number = {10},
  pages = {1872--1897},
  issn = {1869-1900},
  doi = {10.1007/s11431-020-1647-3},
  url = {https://doi.org/10.1007/s11431-020-1647-3},
  urldate = {2023-06-15},
  abstract = {Recently, the emergence of pre-trained models (PTMs) has brought natural language processing (NLP) to a new era. In this survey, we provide a comprehensive review of PTMs for NLP. We first briefly introduce language representation learning and its research progress. Then we systematically categorize existing PTMs based on a taxonomy from four different perspectives. Next, we describe how to adapt the knowledge of PTMs to downstream tasks. Finally, we outline some potential directions of PTMs for future research. This survey is purposed to be a hands-on guide for understanding, using, and developing PTMs for various NLP tasks.},
  langid = {english},
  keywords = {deep learning,distributed representation,language modelling,natural language processing,neural network,pre-trained model,self-supervised learning,word embedding},
  file = {/Users/max18768/Zotero/storage/RNHS3FRU/qiu2020.pdf.pdf;/Users/max18768/Zotero/storage/RS6V5LYW/Qiu et al. - 2020 - Pre-trained models for natural language processing.pdf}
}

@article{raffelExploringLimitsTransfer2020,
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  date = {2020-01-01},
  journaltitle = {The Journal of Machine Learning Research},
  shortjournal = {J. Mach. Learn. Res.},
  volume = {21},
  number = {1},
  pages = {140:5485--140:5551},
  issn = {1532-4435},
  abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pretraining objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
  keywords = {attention based models,deep learning,multi-task learning,natural language processing,transfer learning},
  file = {/Users/max18768/Zotero/storage/MMTDQUAR/Raffel et al. - 2020 - Exploring the limits of transfer learning with a u.pdf}
}

@inproceedings{ramachandranUnsupervisedPretrainingSequence2017,
  title = {Unsupervised {{Pretraining}} for {{Sequence}} to {{Sequence Learning}}},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Ramachandran, Prajit and Liu, Peter and Le, Quoc},
  editor = {Palmer, Martha and Hwa, Rebecca and Riedel, Sebastian},
  date = {2017-09},
  pages = {383--391},
  publisher = {Association for Computational Linguistics},
  location = {Copenhagen, Denmark},
  doi = {10.18653/v1/D17-1039},
  url = {https://aclanthology.org/D17-1039},
  urldate = {2024-04-05},
  abstract = {This work presents a general unsupervised learning method to improve the accuracy of sequence to sequence (seq2seq) models. In our method, the weights of the encoder and decoder of a seq2seq model are initialized with the pretrained weights of two language models and then fine-tuned with labeled data. We apply this method to challenging benchmarks in machine translation and abstractive summarization and find that it significantly improves the subsequent supervised models. Our main result is that pretraining improves the generalization of seq2seq models. We achieve state-of-the-art results on the WMT English→German task, surpassing a range of methods using both phrase-based machine translation and neural machine translation. Our method achieves a significant improvement of 1.3 BLEU from th previous best models on both WMT'14 and WMT'15 English→German. We also conduct human evaluations on abstractive summarization and find that our method outperforms a purely supervised learning baseline in a statistically significant manner.},
  eventtitle = {{{EMNLP}} 2017},
  file = {/Users/max18768/Zotero/storage/8DDJ9ZW6/Ramachandran et al. - 2017 - Unsupervised Pretraining for Sequence to Sequence .pdf}
}

@inproceedings{rehurek_lrec,
  title = {Software Framework for Topic Modelling with Large Corpora},
  booktitle = {Proceedings of the {{LREC}} 2010 Workshop on New Challenges for {{NLP}} Frameworks},
  author = {Řehůřek, Radim and Sojka, Petr},
  date = {2010-05-22},
  pages = {45--50},
  publisher = {ELRA},
  location = {Valletta, Malta},
  langid = {english}
}

@online{reimersClassificationClusteringArguments2019,
  title = {Classification and {{Clustering}} of {{Arguments}} with {{Contextualized Word Embeddings}}},
  author = {Reimers, Nils and Schiller, Benjamin and Beck, Tilman and Daxenberger, Johannes and Stab, Christian and Gurevych, Iryna},
  date = {2019-06-24},
  eprint = {1906.09821},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1906.09821},
  url = {http://arxiv.org/abs/1906.09821},
  urldate = {2023-01-10},
  abstract = {We experiment with two recent contextualized word embedding methods (ELMo and BERT) in the context of open-domain argument search. For the first time, we show how to leverage the power of contextualized word embeddings to classify and cluster topic-dependent arguments, achieving impressive results on both tasks and across multiple datasets. For argument classification, we improve the state-of-the-art for the UKP Sentential Argument Mining Corpus by 20.8 percentage points and for the IBM Debater - Evidence Sentences dataset by 7.4 percentage points. For the understudied task of argument clustering, we propose a pre-training step which improves by 7.8 percentage points over strong baselines on a novel dataset, and by 12.3 percentage points for the Argument Facet Similarity (AFS) Corpus.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/max18768/Zotero/storage/GMRBYSTS/Reimers et al. - 2019 - Classification and Clustering of Arguments with Co.pdf;/Users/max18768/Zotero/storage/PY7UJZ3L/1906.html}
}

@inproceedings{reimersSentenceBERTSentenceEmbeddings2019,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT-Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Reimers, Nils and Gurevych, Iryna},
  editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
  date = {2019-11},
  pages = {3982--3992},
  publisher = {Association for Computational Linguistics},
  location = {Hong Kong, China},
  doi = {10.18653/v1/D19-1410},
  url = {https://aclanthology.org/D19-1410},
  urldate = {2024-04-05},
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (\textbackslash textasciitilde65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  eventtitle = {{{EMNLP-IJCNLP}} 2019},
  file = {/Users/max18768/Zotero/storage/A97TR2D7/Reimers and Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese B.pdf}
}

@article{rodriguezWordEmbeddingsWhat2022,
  title = {Word {{Embeddings}}: {{What Works}}, {{What Doesn}}’t, and {{How}} to {{Tell}} the {{Difference}} for {{Applied Research}}},
  shorttitle = {Word {{Embeddings}}},
  author = {Rodriguez, Pedro L. and Spirling, Arthur},
  date = {2022-01},
  journaltitle = {The Journal of Politics},
  volume = {84},
  number = {1},
  pages = {101--115},
  publisher = {The University of Chicago Press},
  issn = {0022-3816},
  doi = {10.1086/715162},
  url = {https://www.journals.uchicago.edu/doi/full/10.1086/715162},
  urldate = {2022-12-13},
  abstract = {Word embeddings are becoming popular for political science research, yet we know little about their properties and performance. To help scholars seeking to use these techniques, we explore the effects of key parameter choices—including context window length, embedding vector dimensions, and pretrained versus locally fit variants—on the efficiency and quality of inferences possible with these models. Reassuringly we show that results are generally robust to such choices for political corpora of various sizes and in various languages. Beyond reporting extensive technical findings, we provide a novel crowdsourced “Turing test”–style method for examining the relative performance of any two models that produce substantive, text-based outputs. Our results are encouraging: popular, easily available pretrained embeddings perform at a level close to—or surpassing—both human coders and more complicated locally fit models. For completeness, we provide best practice advice for cases where local fitting is required.},
  keywords = {crowdsourcing,deep learning,embeddings,text,Turing test},
  file = {/Users/max18768/Zotero/storage/FMUDZ8CT/Rodriguez and Spirling - 2022 - Word Embeddings What Works, What Doesn’t, and How.pdf}
}

@article{rodriguezWordEmbeddingsWhat2022a,
  title = {Word {{Embeddings}}: {{What Works}}, {{What Doesn}}’t, and {{How}} to {{Tell}} the {{Difference}} for {{Applied Research}}},
  shorttitle = {Word {{Embeddings}}},
  author = {Rodriguez, Pedro L. and Spirling, Arthur},
  date = {2022-01-01},
  journaltitle = {The Journal of Politics},
  publisher = {The University of Chicago PressChicago, IL},
  doi = {10.1086/715162},
  url = {https://www.journals.uchicago.edu/doi/10.1086/715162},
  urldate = {2023-07-25},
  abstract = {Word embeddings are becoming popular for political science research, yet we know little about their properties and performance. To help scholars seeking to use these techniques, we explore the effects of key parameter choices—including context window length, embedding vector dimensions, and pretrained versus locally fit variants—on the efficiency and quality of inferences possible with these models. Reassuringly we show that results are generally robust to such choices for political corpora of various sizes and in various languages. Beyond reporting extensive technical findings, we provide a novel crowdsourced “Turing test”–style method for examining the relative performance of any two models that produce substantive, text-based outputs. Our results are encouraging: popular, easily available pretrained embeddings perform at a level close to—or surpassing—both human coders and more complicated locally fit models. For completeness, we provide best practice advice for cases where local fitting is required.},
  langid = {english},
  file = {/Users/max18768/Zotero/storage/BZE3JPV9/715162.html}
}

@article{rohde2006improved,
  title = {An Improved Model of Semantic Similarity Based on Lexical Co-Occurrence},
  author = {Rohde, Douglas LT and Gonnerman, Laura M and Plaut, David C},
  date = {2006},
  journaltitle = {Communications of the ACM},
  volume = {8},
  number = {627-633},
  pages = {116},
  file = {/Users/max18768/Zotero/storage/3PMRSHYN/Rohde et al. - 2006 - An improved model of semantic similarity based on .pdf}
}

@article{rudinStopExplainingBlack2019,
  title = {Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead},
  author = {Rudin, Cynthia},
  date = {2019-05},
  journaltitle = {Nature Machine Intelligence},
  shortjournal = {Nat Mach Intell},
  volume = {1},
  number = {5},
  pages = {206--215},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0048-x},
  url = {https://www.nature.com/articles/s42256-019-0048-x},
  urldate = {2023-06-17},
  abstract = {Black box machine learning models are currently being used for high-stakes decision making throughout society, causing problems in healthcare, criminal justice and other domains. Some people hope that creating methods for explaining these black box models will alleviate some of the problems, but trying to explain black box models, rather than creating models that are interpretable in the first place, is likely to perpetuate bad practice and can potentially cause great harm to society. The way forward is to design models that are inherently interpretable. This Perspective clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare and computer vision.},
  issue = {5},
  langid = {english},
  keywords = {Computer science,Criminology,Science,Statistics,technology and society},
  file = {/Users/max18768/Zotero/storage/7KJHMGRV/Rudin - 2019 - Stop explaining black box machine learning models .pdf;/Users/max18768/Zotero/storage/W9H5GJ4P/10.1038@s42256-019-0048-x.pdf.pdf}
}

@article{sabbehComparativeAnalysisWord2023,
  title = {A {{Comparative Analysis}} of {{Word Embedding}} and {{Deep Learning}} for {{Arabic Sentiment Classification}}},
  author = {Sabbeh, Sahar F. and Fasihuddin, Heba A.},
  date = {2023-01},
  journaltitle = {Electronics},
  volume = {12},
  number = {6},
  pages = {1425},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2079-9292},
  doi = {10.3390/electronics12061425},
  url = {https://www.mdpi.com/2079-9292/12/6/1425},
  urldate = {2023-07-25},
  abstract = {Sentiment analysis on social media platforms (i.e., Twitter or Facebook) has become an important tool to learn about users’ opinions and preferences. However, the accuracy of sentiment analysis is disrupted by the challenges of natural language processing (NLP). Recently, deep learning models have proved superior performance over statistical- and lexical-based approaches in NLP-related tasks. Word embedding is an important layer of deep learning models to generate input features. Many word embedding models have been presented for text representation of both classic and context-based word embeddings. In this paper, we present a comparative analysis to evaluate both classic and contextualized word embeddings for sentiment analysis. The four most frequently used word embedding techniques were used in their trained and pre-trained versions. The selected embedding represents classical and contextualized techniques. Classical word embedding includes algorithms such as GloVe, Word2vec, and FastText. By contrast, ARBERT is used as a contextualized embedding model. Since word embedding is more typically employed as the input layer in deep networks, we used deep learning architectures BiLSTM and CNN for sentiment classification. To achieve these goals, the experiments were applied to a series of benchmark datasets: HARD, Khooli, AJGT, ArSAS, and ASTD. Finally, a comparative analysis was conducted on the results obtained for the experimented models. Our outcomes indicate that, generally, generated embedding by one technique achieves higher performance than its pretrained version for the same technique by around 0.28 to 1.8\% accuracy, 0.33 to 2.17\% precision, and 0.44 to 2\% recall. Moreover, the contextualized transformer-based embedding model BERT achieved the highest performance in its pretrained and trained versions. Additionally, the results indicate that BiLSTM outperforms CNN by approximately 2\% in 3 datasets, HARD, Khooli, and ArSAS, while CNN achieved around 2\% higher performance in the smaller datasets, AJGT and ASTD.},
  issue = {6},
  langid = {english},
  keywords = {Arabic sentiment analysis,BiLSTM,CNN word embedding,deep learning},
  file = {/Users/max18768/Zotero/storage/U85KZXXU/Sabbeh and Fasihuddin - 2023 - A Comparative Analysis of Word Embedding and Deep .pdf}
}

@online{salleEnhancingLexVecDistributed2016,
  title = {Enhancing the {{LexVec Distributed Word Representation Model Using Positional Contexts}} and {{External Memory}}},
  author = {Salle, Alexandre and Idiart, Marco and Villavicencio, Aline},
  date = {2016-06-03},
  eprint = {1606.01283},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1606.01283},
  url = {http://arxiv.org/abs/1606.01283},
  urldate = {2023-07-31},
  abstract = {In this paper we take a state-of-the-art model for distributed word representation that explicitly factorizes the positive pointwise mutual information (PPMI) matrix using window sampling and negative sampling and address two of its shortcomings. We improve syntactic performance by using positional contexts, and solve the need to store the PPMI matrix in memory by working on aggregate data in external memory. The effectiveness of both modifications is shown using word similarity and analogy tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/max18768/Zotero/storage/8EEE75GA/Salle et al. - 2016 - Enhancing the LexVec Distributed Word Representati.pdf;/Users/max18768/Zotero/storage/GM8MJNIQ/1606.html}
}

@inproceedings{salleIncorporatingSubwordInformation2018,
  title = {Incorporating {{Subword Information}} into {{Matrix Factorization Word Embeddings}}},
  booktitle = {Proceedings of the {{Second Workshop}} on {{Subword}}/{{Character LEvel Models}}},
  author = {Salle, Alexandre and Villavicencio, Aline},
  date = {2018-06},
  pages = {66--71},
  publisher = {Association for Computational Linguistics},
  location = {New Orleans},
  doi = {10.18653/v1/W18-1209},
  url = {https://aclanthology.org/W18-1209},
  urldate = {2023-07-31},
  abstract = {The positive effect of adding subword information to word embeddings has been demonstrated for predictive models. In this paper we investigate whether similar benefits can also be derived from incorporating subwords into counting models. We evaluate the impact of different types of subwords (n-grams and unsupervised morphemes), with results confirming the importance of subword information in learning representations of rare and out-of-vocabulary words.},
  eventtitle = {{{SCLeM}} 2018},
  file = {/Users/max18768/Zotero/storage/6IGZGTN8/Salle and Villavicencio - 2018 - Incorporating Subword Information into Matrix Fact.pdf}
}

@inproceedings{salleMatrixFactorizationUsing2016,
  title = {Matrix {{Factorization}} Using {{Window Sampling}} and {{Negative Sampling}} for {{Improved Word Representations}}},
  booktitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Salle, Alexandre and Villavicencio, Aline and Idiart, Marco},
  date = {2016-08},
  pages = {419--424},
  publisher = {Association for Computational Linguistics},
  location = {Berlin, Germany},
  doi = {10.18653/v1/P16-2068},
  url = {https://aclanthology.org/P16-2068},
  urldate = {2023-07-31},
  eventtitle = {{{ACL}} 2016},
  file = {/Users/max18768/Zotero/storage/G3UURFDR/Salle et al. - 2016 - Matrix Factorization using Window Sampling and Neg.pdf}
}

@online{salleWhyRoleNegative2019,
  title = {Why {{So Down}}? {{The Role}} of {{Negative}} (and {{Positive}}) {{Pointwise Mutual Information}} in {{Distributional Semantics}}},
  shorttitle = {Why {{So Down}}?},
  author = {Salle, Alexandre and Villavicencio, Aline},
  date = {2019-08-19},
  eprint = {1908.06941},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1908.06941},
  url = {http://arxiv.org/abs/1908.06941},
  urldate = {2023-07-31},
  abstract = {In distributional semantics, the pointwise mutual information (\$\textbackslash mathit\{PMI\}\$) weighting of the cooccurrence matrix performs far better than raw counts. There is, however, an issue with unobserved pair cooccurrences as \$\textbackslash mathit\{PMI\}\$ goes to negative infinity. This problem is aggravated by unreliable statistics from finite corpora which lead to a large number of such pairs. A common practice is to clip negative \$\textbackslash mathit\{PMI\}\$ (\$\textbackslash mathit\{\textbackslash texttt\{-\} PMI\}\$) at \$0\$, also known as Positive \$\textbackslash mathit\{PMI\}\$ (\$\textbackslash mathit\{PPMI\}\$). In this paper, we investigate alternative ways of dealing with \$\textbackslash mathit\{\textbackslash texttt\{-\} PMI\}\$ and, more importantly, study the role that negative information plays in the performance of a low-rank, weighted factorization of different \$\textbackslash mathit\{PMI\}\$ matrices. Using various semantic and syntactic tasks as probes into models which use either negative or positive \$\textbackslash mathit\{PMI\}\$ (or both), we find that most of the encoded semantics and syntax come from positive \$\textbackslash mathit\{PMI\}\$, in contrast to \$\textbackslash mathit\{\textbackslash texttt\{-\} PMI\}\$ which contributes almost exclusively syntactic information. Our findings deepen our understanding of distributional semantics, while also introducing novel \$PMI\$ variants and grounding the popular \$PPMI\$ measure.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/max18768/Zotero/storage/AJ4YPGSK/Salle and Villavicencio - 2019 - Why So Down The Role of Negative (and Positive) P.pdf;/Users/max18768/Zotero/storage/4BPKZT9Z/1908.html}
}

@book{saltonIntroductionModernInformation1983,
  title = {Introduction to {{Modern Information Retrieval}}},
  author = {Salton, Gerard and McGill, Michael J.},
  date = {1983},
  eprint = {7f5TAAAAMAAJ},
  eprinttype = {googlebooks},
  publisher = {McGraw-Hill},
  abstract = {Examines Concepts, Functions \& Processes of Information Retrieval Systems},
  isbn = {978-0-07-054484-0},
  langid = {english},
  pagetotal = {470},
  keywords = {Computers / System Administration / Storage & Retrieval}
}

@article{sanchezOntologybasedSemanticSimilarity2012,
  title = {Ontology-Based Semantic Similarity: {{A}} New Feature-Based Approach},
  shorttitle = {Ontology-Based Semantic Similarity},
  author = {Sánchez, David and Batet, Montserrat and Isern, David and Valls, Aida},
  date = {2012-07-01},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {39},
  number = {9},
  pages = {7718--7728},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2012.01.082},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417412000954},
  urldate = {2024-03-27},
  abstract = {Estimation of the semantic likeness between words is of great importance in many applications dealing with textual data such as natural language processing, knowledge acquisition and information retrieval. Semantic similarity measures exploit knowledge sources as the base to perform the estimations. In recent years, ontologies have grown in interest thanks to global initiatives such as the Semantic Web, offering an structured knowledge representation. Thanks to the possibilities that ontologies enable regarding semantic interpretation of terms many ontology-based similarity measures have been developed. According to the principle in which those measures base the similarity assessment and the way in which ontologies are exploited or complemented with other sources several families of measures can be identified. In this paper, we survey and classify most of the ontology-based approaches developed in order to evaluate their advantages and limitations and compare their expected performance both from theoretical and practical points of view. We also present a new ontology-based measure relying on the exploitation of taxonomical features. The evaluation and comparison of our approach’s results against those reported by related works under a common framework suggest that our measure provides a high accuracy without some of the limitations observed in other works.},
  keywords = {Feature-based similarity,Ontologies,Semantic relatedness,Semantic similarity,WordNet}
}

@article{sanchezSemanticSimilarityMethod2013,
  title = {A Semantic Similarity Method Based on Information Content Exploiting Multiple Ontologies},
  author = {Sánchez, David and Batet, Montserrat},
  date = {2013-03-01},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {40},
  number = {4},
  pages = {1393--1399},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2012.08.049},
  url = {https://www.sciencedirect.com/science/article/pii/S095741741201010X},
  urldate = {2024-03-27},
  abstract = {The quantification of the semantic similarity between terms is an important research area that configures a valuable tool for text understanding. Among the different paradigms used by related works to compute semantic similarity, in recent years, information theoretic approaches have shown promising results by computing the information content (IC) of concepts from the knowledge provided by ontologies. These approaches, however, are hampered by the coverage offered by the single input ontology. In this paper, we propose extending IC-based similarity measures by considering multiple ontologies in an integrated way. Several strategies are proposed according to which ontology the evaluated terms belong. Our proposal has been evaluated by means of a widely used benchmark of medical terms and MeSH and SNOMED CT as ontologies. Results show an improvement in the similarity assessment accuracy when multiple ontologies are considered.},
  keywords = {Information content,MeSH,Ontologies,Semantic similarity,SNOMED CT},
  file = {/Users/max18768/Zotero/storage/5ZQTZNER/S095741741201010X.html}
}

@inproceedings{selvabirundaReviewWordEmbedding2021,
  title = {A {{Review}} on {{Word Embedding Techniques}} for {{Text Classification}}},
  booktitle = {Innovative {{Data Communication Technologies}} and {{Application}}},
  author = {Selva Birunda, S. and Kanniga Devi, R.},
  editor = {Raj, Jennifer S. and Iliyasu, Abdullah M. and Bestak, Robert and Baig, Zubair A.},
  date = {2021},
  series = {Lecture {{Notes}} on {{Data Engineering}} and {{Communications Technologies}}},
  pages = {267--281},
  publisher = {Springer},
  location = {Singapore},
  doi = {10.1007/978-981-15-9651-3_23},
  abstract = {Word embeddings are fundamentally a form of word representation that links the human understanding of knowledge meaningfully to the understanding of a machine. The representations can be a set of real numbers (a vector). Word embeddings are scattered depiction of a text in an n-dimensional space, which tries to capture the word meanings. This paper aims to provide an overview of the different types of word embedding techniques. It is found from the review that there exist three dominant word embeddings namely, Traditional word embedding, Static word embedding, and Contextualized word embedding. BERT is a bidirectional transformer-based Contextualized word embedding which is more efficient as it can be pre-trained and fine-tuned. As a future scope, this word embedding along with the neural network models can be used to increase the model accuracy and it excels in sentiment classification, text classification, next sentence prediction, and other Natural Language Processing tasks. Some of the open issues are also discussed and future research scope for the improvement of word representation.},
  isbn = {9789811596513},
  langid = {english},
  keywords = {Bag of words,Contextualized word embeddings,Natural language processing,Text classification,Transformer,Word embeddings}
}

@online{sezererSurveyNeuralWord2021,
  title = {A {{Survey On Neural Word Embeddings}}},
  author = {Sezerer, Erhan and Tekir, Selma},
  date = {2021-10-04},
  eprint = {2110.01804},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2110.01804},
  url = {http://arxiv.org/abs/2110.01804},
  urldate = {2023-07-26},
  abstract = {Understanding human language has been a sub-challenge on the way of intelligent machines. The study of meaning in natural language processing (NLP) relies on the distributional hypothesis where language elements get meaning from the words that co-occur within contexts. The revolutionary idea of distributed representation for a concept is close to the working of a human mind in that the meaning of a word is spread across several neurons, and a loss of activation will only slightly affect the memory retrieval process. Neural word embeddings transformed the whole field of NLP by introducing substantial improvements in all NLP tasks. In this survey, we provide a comprehensive literature review on neural word embeddings. We give theoretical foundations and describe existing work by an interplay between word embeddings and language modelling. We provide broad coverage on neural word embeddings, including early word embeddings, embeddings targeting specific semantic relations, sense embeddings, morpheme embeddings, and finally, contextual representations. Finally, we describe benchmark datasets in word embeddings' performance evaluation and downstream tasks along with the performance results of/due to word embeddings.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/max18768/Zotero/storage/SIIW63LS/Sezerer and Tekir - 2021 - A Survey On Neural Word Embeddings.pdf;/Users/max18768/Zotero/storage/G4BCRP4A/2110.html}
}

@inproceedings{shaoHCTISemEval2017Task2017,
  title = {{{HCTI}} at {{SemEval-2017 Task}} 1: {{Use}} Convolutional Neural Network to Evaluate {{Semantic Textual Similarity}}},
  shorttitle = {{{HCTI}} at {{SemEval-2017 Task}} 1},
  booktitle = {Proceedings of the 11th {{International Workshop}} on {{Semantic Evaluation}} ({{SemEval-2017}})},
  author = {Shao, Yang},
  editor = {Bethard, Steven and Carpuat, Marine and Apidianaki, Marianna and Mohammad, Saif M. and Cer, Daniel and Jurgens, David},
  date = {2017-08},
  pages = {130--133},
  publisher = {Association for Computational Linguistics},
  location = {Vancouver, Canada},
  doi = {10.18653/v1/S17-2016},
  url = {https://aclanthology.org/S17-2016},
  urldate = {2024-04-01},
  abstract = {This paper describes our convolutional neural network (CNN) system for Semantic Textual Similarity (STS) task. We calculated semantic similarity score between two sentences by comparing their semantic vectors. We generated semantic vector of every sentence by max pooling every dimension of their word vectors. There are mainly two trick points in our system. One is that we trained a CNN to transfer GloVe word vectors to a more proper form for STS task before pooling. Another is that we trained a fully-connected neural network (FCNN) to transfer difference of two semantic vectors to probability of every similarity score. We decided all hyper parameters empirically. In spite of the simplicity of our neural network system, we achieved a good accuracy and ranked 3rd in primary track of SemEval 2017.},
  eventtitle = {{{SemEval}} 2017},
  file = {/Users/max18768/Zotero/storage/6GRC775F/Shao - 2017 - HCTI at SemEval-2017 Task 1 Use convolutional neu.pdf}
}

@article{siebersSurveyTextRepresentation2022,
  title = {A {{Survey}} of {{Text Representation Methods}} and {{Their Genealogy}}},
  author = {Siebers, Philipp and Janiesch, Christian and Zschech, Patrick},
  date = {2022},
  journaltitle = {IEEE Access},
  volume = {10},
  pages = {96492--96513},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3205719},
  abstract = {In recent years, with the advent of highly scalable artificial-neural-network-based text representation methods the field of natural language processing has seen unprecedented growth and sophistication. It has become possible to distill complex linguistic information of text into multidimensional dense numeric vectors with the use of the distributional hypothesis. As a consequence, text representation methods have been evolving at such a quick pace that the research community is struggling to retain knowledge of the methods and their interrelations. We contribute threefold to this lack of compilation, composition, and systematization by providing a survey of current approaches, by arranging them in a genealogy, and by conceptualizing a taxonomy of text representation methods to examine and explain the state-of-the-art. Our research is a valuable guide and reference for artificial intelligence researchers and practitioners interested in natural language processing applications such as recommender systems, chatbots, and sentiment analysis.},
  eventtitle = {{{IEEE Access}}},
  keywords = {Artificial neural networks,Encoding,genealogy,natural language processing,Natural language processing,Pragmatics,Semantics,survey,taxonomy,Text recognition,text representation,Vocabulary},
  file = {/Users/max18768/Zotero/storage/WQTNRYXT/Siebers et al. - 2022 - A Survey of Text Representation Methods and Their .pdf;/Users/max18768/Zotero/storage/CN4AKJQZ/9885209.html}
}

@inproceedings{sitikhuComparisonSemanticSimilarity2019,
  title = {A {{Comparison}} of {{Semantic Similarity Methods}} for {{Maximum Human Interpretability}}},
  booktitle = {2019 {{Artificial Intelligence}} for {{Transforming Business}} and {{Society}} ({{AITB}})},
  author = {Sitikhu, Pinky and Pahi, Kritish and Thapa, Pujan and Shakya, Subarna},
  date = {2019-11},
  volume = {1},
  pages = {1--4},
  doi = {10.1109/AITB48515.2019.8947433},
  abstract = {The inclusion of semantic information in any similarity measures improves the efficiency of the similarity measure and provides human interpretable results for further analysis. The similarity calculation method that focuses on features related to the text's words only, will give less accurate results. This paper presents three different methods that not only focus on the text's words but also incorporates semantic information of texts in their feature vector and computes semantic similarities. These methods are based on corpus-based and knowledge-based methods, which are: cosine similarity using tf-idf vectors, cosine similarity using word embedding and soft cosine similarity using word embedding. Among these three, cosine similarity using tf-idf vectors performed best in finding similarities between short news texts. The similar texts given by the method are easy to interpret and can be used directly in other information retrieval applications.},
  eventtitle = {2019 {{Artificial Intelligence}} for {{Transforming Business}} and {{Society}} ({{AITB}})},
  keywords = {Computational modeling,cosine similarity,Indexes,Information retrieval,Knowledge based systems,Measurement,Prio 2,semantic similarity,Semantics,soft cosine similarity,Sparse matrices,word embedding},
  file = {/Users/max18768/Zotero/storage/75P5ZWQE/Sitikhu et al. - 2019 - A Comparison of Semantic Similarity Methods for Ma.pdf;/Users/max18768/Zotero/storage/L8PFXD5Z/sitikhu2019.pdf.pdf;/Users/max18768/Zotero/storage/D6QY45IK/8947433.html}
}

@article{speerConceptNetOpenMultilingual2017,
  title = {{{ConceptNet}} 5.5: {{An Open Multilingual Graph}} of {{General Knowledge}}},
  shorttitle = {{{ConceptNet}} 5.5},
  author = {Speer, Robyn and Chin, Joshua and Havasi, Catherine},
  date = {2017-02-12},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {31},
  number = {1},
  issn = {2374-3468},
  doi = {10.1609/aaai.v31i1.11164},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/11164},
  urldate = {2024-03-27},
  abstract = {Machine learning about language can be improved by supplying it with specific knowledge and sources of external information. We present here a new version of the linked open data resource ConceptNet that is particularly well suited to be used with modern NLP techniques such as word embeddings. ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use. When ConceptNet is combined with word embeddings acquired from distributional semantics (such as word2vec), it provides applications with understanding that they would not acquire from distributional semantics alone, nor from narrower resources such as WordNet or DBPedia. We demonstrate this with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improvements on applications of word vectors, including solving SAT-style analogies.},
  issue = {1},
  langid = {english},
  keywords = {word embeddings},
  file = {/Users/max18768/Zotero/storage/DUA9DKG9/Speer et al. - 2017 - ConceptNet 5.5 An Open Multilingual Graph of Gene.pdf}
}

@online{speerConceptNetOpenMultilingual2018,
  title = {{{ConceptNet}} 5.5: {{An Open Multilingual Graph}} of {{General Knowledge}}},
  shorttitle = {{{ConceptNet}} 5.5},
  author = {Speer, Robyn and Chin, Joshua and Havasi, Catherine},
  date = {2018-12-11},
  eprint = {1612.03975},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1612.03975},
  url = {http://arxiv.org/abs/1612.03975},
  urldate = {2023-07-31},
  abstract = {Machine learning about language can be improved by supplying it with specific knowledge and sources of external information. We present here a new version of the linked open data resource ConceptNet that is particularly well suited to be used with modern NLP techniques such as word embeddings. ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use. When ConceptNet is combined with word embeddings acquired from distributional semantics (such as word2vec), it provides applications with understanding that they would not acquire from distributional semantics alone, nor from narrower resources such as WordNet or DBPedia. We demonstrate this with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improvements on applications of word vectors, including solving SAT-style analogies.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,I.2.7},
  file = {/Users/max18768/Zotero/storage/BPEUQWVB/Speer et al. - 2018 - ConceptNet 5.5 An Open Multilingual Graph of Gene.pdf;/Users/max18768/Zotero/storage/Z45MTNZT/1612.html}
}

@inproceedings{suchanekYagoCoreSemantic2007,
  title = {Yago: A Core of Semantic Knowledge},
  shorttitle = {Yago},
  booktitle = {Proceedings of the 16th International Conference on {{World Wide Web}}},
  author = {Suchanek, Fabian M. and Kasneci, Gjergji and Weikum, Gerhard},
  date = {2007-05-08},
  series = {{{WWW}} '07},
  pages = {697--706},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1242572.1242667},
  url = {https://dl.acm.org/doi/10.1145/1242572.1242667},
  urldate = {2024-03-27},
  abstract = {We present YAGO, a light-weight and extensible ontology with high coverage and quality. YAGO builds on entities and relations and currently contains more than 1 million entities and 5 million facts. This includes the Is-A hierarchy as well as non-taxonomic relations between entities (such as HASONEPRIZE). The facts have been automatically extracted from Wikipedia and unified with WordNet, using a carefully designed combination of rule-based and heuristic methods described in this paper. The resulting knowledge base is a major step beyond WordNet: in quality by adding knowledge about individuals like persons, organizations, products, etc. with their semantic relationships - and in quantity by increasing the number of facts by more than an order of magnitude. Our empirical evaluation of fact correctness shows an accuracy of about 95\%. YAGO is based on a logically clean model, which is decidable, extensible, and compatible with RDFS. Finally, we show how YAGO can be further extended by state-of-the-art information extraction techniques.},
  isbn = {978-1-59593-654-7},
  keywords = {wikipedia,WordNet},
  file = {/Users/max18768/Zotero/storage/ZUQ54U26/Suchanek et al. - 2007 - Yago a core of semantic knowledge.pdf}
}

@online{suOneEmbedderAny2023,
  title = {One {{Embedder}}, {{Any Task}}: {{Instruction-Finetuned Text Embeddings}}},
  shorttitle = {One {{Embedder}}, {{Any Task}}},
  author = {Su, Hongjin and Shi, Weijia and Kasai, Jungo and Wang, Yizhong and Hu, Yushi and Ostendorf, Mari and Yih, Wen-tau and Smith, Noah A. and Zettlemoyer, Luke and Yu, Tao},
  date = {2023-05-30},
  eprint = {2212.09741},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2212.09741},
  url = {http://arxiv.org/abs/2212.09741},
  urldate = {2023-07-31},
  abstract = {We introduce INSTRUCTOR, a new method for computing text embeddings given task instructions: every text input is embedded together with instructions explaining the use case (e.g., task and domain descriptions). Unlike encoders from prior work that are more specialized, INSTRUCTOR is a single embedder that can generate text embeddings tailored to different downstream tasks and domains, without any further training. We first annotate instructions for 330 diverse tasks and train INSTRUCTOR on this multitask mixture with a contrastive loss. We evaluate INSTRUCTOR on 70 embedding evaluation tasks (66 of which are unseen during training), ranging from classification and information retrieval to semantic textual similarity and text generation evaluation. INSTRUCTOR, while having an order of magnitude fewer parameters than the previous best model, achieves state-of-the-art performance, with an average improvement of 3.4\% compared to the previous best results on the 70 diverse datasets. Our analysis suggests that INSTRUCTOR is robust to changes in instructions, and that instruction finetuning mitigates the challenge of training a single model on diverse datasets. Our model, code, and data are available at https://instructor-embedding.github.io.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/max18768/Zotero/storage/GNDY2UQJ/Su et al. - 2023 - One Embedder, Any Task Instruction-Finetuned Text.pdf;/Users/max18768/Zotero/storage/9TKZG8LM/2212.html}
}

@online{taiImprovedSemanticRepresentations2015,
  title = {Improved {{Semantic Representations From Tree-Structured Long Short-Term Memory Networks}}},
  author = {Tai, Kai Sheng and Socher, Richard and Manning, Christopher D.},
  date = {2015-05-30},
  eprint = {1503.00075},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1503.00075},
  url = {http://arxiv.org/abs/1503.00075},
  urldate = {2024-04-01},
  abstract = {Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/max18768/Zotero/storage/4JQYCSG5/Tai et al. - 2015 - Improved Semantic Representations From Tree-Struct.pdf;/Users/max18768/Zotero/storage/SA5FAYCQ/1503.html}
}

@inproceedings{tenneyBERTRediscoversClassical2019,
  title = {{{BERT Rediscovers}} the {{Classical NLP Pipeline}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
  editor = {Korhonen, Anna and Traum, David and Màrquez, Lluís},
  date = {2019-07},
  pages = {4593--4601},
  publisher = {Association for Computational Linguistics},
  location = {Florence, Italy},
  doi = {10.18653/v1/P19-1452},
  url = {https://aclanthology.org/P19-1452},
  urldate = {2024-04-05},
  abstract = {Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.},
  eventtitle = {{{ACL}} 2019},
  file = {/Users/max18768/Zotero/storage/SDL37R9V/Tenney et al. - 2019 - BERT Rediscovers the Classical NLP Pipeline.pdf}
}

@inproceedings{tianECNUSemEval2017Task2017,
  title = {{{ECNU}} at {{SemEval-2017 Task}} 1: {{Leverage Kernel-based Traditional NLP}} Features and {{Neural Networks}} to {{Build}} a {{Universal Model}} for {{Multilingual}} and {{Cross-lingual Semantic Textual Similarity}}},
  shorttitle = {{{ECNU}} at {{SemEval-2017 Task}} 1},
  booktitle = {Proceedings of the 11th {{International Workshop}} on {{Semantic Evaluation}} ({{SemEval-2017}})},
  author = {Tian, Junfeng and Zhou, Zhiheng and Lan, Man and Wu, Yuanbin},
  editor = {Bethard, Steven and Carpuat, Marine and Apidianaki, Marianna and Mohammad, Saif M. and Cer, Daniel and Jurgens, David},
  date = {2017-08},
  pages = {191--197},
  publisher = {Association for Computational Linguistics},
  location = {Vancouver, Canada},
  doi = {10.18653/v1/S17-2028},
  url = {https://aclanthology.org/S17-2028},
  urldate = {2024-04-01},
  abstract = {To address semantic similarity on multilingual and cross-lingual sentences, we firstly translate other foreign languages into English, and then feed our monolingual English system with various interactive features. Our system is further supported by combining with deep learning semantic similarity and our best run achieves the mean Pearson correlation 73.16\% in primary track.},
  eventtitle = {{{SemEval}} 2017},
  file = {/Users/max18768/Zotero/storage/ENZMG9JB/Tian et al. - 2017 - ECNU at SemEval-2017 Task 1 Leverage Kernel-based.pdf}
}

@article{tienSentenceModelingMultiple2019,
  title = {Sentence Modeling via Multiple Word Embeddings and Multi-Level Comparison for Semantic Textual Similarity},
  author = {Tien, Nguyen Huy and Le, Nguyen Minh and Tomohiro, Yamasaki and Tatsuya, Izuha},
  date = {2019-11-01},
  journaltitle = {Information Processing \& Management},
  shortjournal = {Information Processing \& Management},
  volume = {56},
  number = {6},
  pages = {102090},
  issn = {0306-4573},
  doi = {10.1016/j.ipm.2019.102090},
  url = {https://www.sciencedirect.com/science/article/pii/S0306457319301335},
  urldate = {2024-04-01},
  abstract = {Recently, using a pretrained word embedding to represent words achieves success in many natural language processing tasks. According to objective functions, different word embedding models capture different aspects of linguistic properties. However, the Semantic Textual Similarity task, which evaluates similarity/relation between two sentences, requires to take into account of these linguistic aspects. Therefore, this research aims to encode various characteristics from multiple sets of word embeddings into one embedding and then learn similarity/relation between sentences via this novel embedding. Representing each word by multiple word embeddings, the proposed MaxLSTM-CNN encoder generates a novel sentence embedding. We then learn the similarity/relation between our sentence embeddings via Multi-level comparison. Our method M-MaxLSTM-CNN consistently shows strong performances in several tasks (i.e., measure textual similarity, identify paraphrase, recognize textual entailment). Our model does not use hand-crafted features (e.g., alignment features, Ngram overlaps, dependency features) as well as does not require pre-trained word embeddings to have the same dimension.},
  keywords = {Multi-level comparison,Multiple word embeddings,Semantic,Sentence embedding,Similarity},
  file = {/Users/max18768/Zotero/storage/VELZD3TF/Tien et al. - 2019 - Sentence modeling via multiple word embeddings and.pdf;/Users/max18768/Zotero/storage/4A4ADJ7K/S0306457319301335.html}
}

@article{tienSentenceModelingMultiple2019a,
  title = {Sentence Modeling via Multiple Word Embeddings and Multi-Level Comparison for Semantic Textual Similarity},
  author = {Tien, Nguyen Huy and Le, Nguyen Minh and Tomohiro, Yamasaki and Tatsuya, Izuha},
  date = {2019-11-01},
  journaltitle = {Information Processing \& Management},
  shortjournal = {Information Processing \& Management},
  volume = {56},
  number = {6},
  pages = {102090},
  issn = {0306-4573},
  doi = {10.1016/j.ipm.2019.102090},
  url = {https://www.sciencedirect.com/science/article/pii/S0306457319301335},
  urldate = {2024-04-01},
  abstract = {Recently, using a pretrained word embedding to represent words achieves success in many natural language processing tasks. According to objective functions, different word embedding models capture different aspects of linguistic properties. However, the Semantic Textual Similarity task, which evaluates similarity/relation between two sentences, requires to take into account of these linguistic aspects. Therefore, this research aims to encode various characteristics from multiple sets of word embeddings into one embedding and then learn similarity/relation between sentences via this novel embedding. Representing each word by multiple word embeddings, the proposed MaxLSTM-CNN encoder generates a novel sentence embedding. We then learn the similarity/relation between our sentence embeddings via Multi-level comparison. Our method M-MaxLSTM-CNN consistently shows strong performances in several tasks (i.e., measure textual similarity, identify paraphrase, recognize textual entailment). Our model does not use hand-crafted features (e.g., alignment features, Ngram overlaps, dependency features) as well as does not require pre-trained word embeddings to have the same dimension.},
  keywords = {Multi-level comparison,Multiple word embeddings,Semantic,Sentence embedding,Similarity},
  file = {/Users/max18768/Zotero/storage/R8QBEFB6/Tien et al. - 2019 - Sentence modeling via multiple word embeddings and.pdf;/Users/max18768/Zotero/storage/JJKJ99HP/S0306457319301335.html}
}

@inproceedings{toshevskaComparativeAnalysisWord2020,
  title = {Comparative {{Analysis}} of {{Word Embeddings}} for {{Capturing Word Similarities}}},
  booktitle = {6th {{International Conference}} on {{Natural Language Processing}} ({{NATP}} 2020)},
  author = {Toshevska, Martina and Stojanovska, Frosina and Kalajdjieski, Jovan},
  date = {2020-04-25},
  eprint = {2005.03812},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {09--24},
  doi = {10.5121/csit.2020.100402},
  url = {http://arxiv.org/abs/2005.03812},
  urldate = {2023-07-31},
  abstract = {Distributed language representation has become the most widely used technique for language representation in various natural language processing tasks. Most of the natural language processing models that are based on deep learning techniques use already pre-trained distributed word representations, commonly called word embeddings. Determining the most qualitative word embeddings is of crucial importance for such models. However, selecting the appropriate word embeddings is a perplexing task since the projected embedding space is not intuitive to humans. In this paper, we explore different approaches for creating distributed word representations. We perform an intrinsic evaluation of several state-of-the-art word embedding methods. Their performance on capturing word similarities is analysed with existing benchmark datasets for word pairs similarities. The research in this paper conducts a correlation analysis between ground truth word similarities and similarities obtained by different word embedding methods.},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/max18768/Zotero/storage/XQ9IX8TV/Toshevska et al. - 2020 - Comparative Analysis of Word Embeddings for Captur.pdf;/Users/max18768/Zotero/storage/FESIIUZA/2005.html}
}

@article{turneyCorpusbasedLearningAnalogies2005,
  title = {Corpus-Based {{Learning}} of {{Analogies}} and {{Semantic Relations}}},
  author = {Turney, Peter D. and Littman, Michael L.},
  date = {2005-09-01},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {60},
  number = {1},
  pages = {251--278},
  issn = {1573-0565},
  doi = {10.1007/s10994-005-0913-1},
  url = {https://doi.org/10.1007/s10994-005-0913-1},
  urldate = {2024-03-28},
  abstract = {We present an algorithm for learning from unlabeled text, based on the Vector Space Model (VSM) of information retrieval, that can solve verbal analogy questions of the kind found in the SAT college entrance exam. A verbal analogy has the form A:B::C:D, meaning “A is to B as C is to D”; for example, mason:stone::carpenter:wood. SAT analogy questions provide a word pair, A:B, and the problem is to select the most analogous word pair, C:D, from a set of five choices. The VSM algorithm correctly answers 47\% of a collection of 374 college-level analogy questions (random guessing would yield 20\% correct; the average college-bound senior high school student answers about 57\% correctly). We motivate this research by applying it to a difficult problem in natural language processing, determining semantic relations in noun-modifier pairs. The problem is to classify a noun-modifier pair, such as “laser printer”, according to the semantic relation between the noun (printer) and the modifier (laser). We use a supervised nearest-neighbour algorithm that assigns a class to a given noun-modifier pair by finding the most analogous noun-modifier pair in the training data. With 30 classes of semantic relations, on a collection of 600 labeled noun-modifier pairs, the learning algorithm attains an F value of 26.5\% (random guessing: 3.3\%). With 5 classes of semantic relations, the F value is 43.2\% (random: 20\%). The performance is state-of-the-art for both verbal analogies and noun-modifier relations.},
  langid = {english},
  keywords = {analogy,cosine similarity,metaphor,noun-modifier pairs,semantic relations,vector space model},
  file = {/Users/max18768/Zotero/storage/LT4GWJWG/Turney and Littman - 2005 - Corpus-based Learning of Analogies and Semantic Re.pdf}
}

@article{turneyFrequencyMeaningVector2010,
  title = {From {{Frequency}} to {{Meaning}}: {{Vector Space Models}} of {{Semantics}}},
  shorttitle = {From {{Frequency}} to {{Meaning}}},
  author = {Turney, P. D. and Pantel, P.},
  date = {2010-02-27},
  journaltitle = {Journal of Artificial Intelligence Research},
  volume = {37},
  pages = {141--188},
  issn = {1076-9757},
  doi = {10.1613/jair.2934},
  url = {https://www.jair.org/index.php/jair/article/view/10640},
  urldate = {2024-03-28},
  abstract = {Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field.},
  langid = {english},
  file = {/Users/max18768/Zotero/storage/86EM9RFA/Turney and Pantel - 2010 - From Frequency to Meaning Vector Space Models of .pdf}
}

@misc{TwoSidesEnvironmental,
  title = {The Two Sides of the {{Environmental Kuznets Curve}}: A Socio-Semantic Analysis},
  file = {/Users/max18768/Zotero/storage/7KBYS9C8/The two sides of the Environmental Kuznets Curve .pdf}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  date = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  urldate = {2023-06-21},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  file = {/Users/max18768/Zotero/storage/MG968VR5/Vaswani et al. - 2017 - Attention is All you Need.pdf}
}

@inproceedings{vermaSemanticSimilarityShort2020,
  title = {Semantic Similarity between Short Paragraphs Using {{Deep Learning}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Electronics}}, {{Computing}} and {{Communication Technologies}} ({{CONECCT}})},
  author = {Verma, Dhruv and Muralikrishna, S.N.},
  date = {2020-07},
  pages = {1--5},
  doi = {10.1109/CONECCT50063.2020.9198445},
  url = {https://ieeexplore.ieee.org/abstract/document/9198445?casa_token=snw6G_owZKwAAAAA:7fK9HLm5N8byir2_Q9ZkbgCEcf3yeyO3tZeuSScNIjUwqNzZ4dtu82zb2MvPZcngMhmPXr4UlQ},
  urldate = {2024-03-27},
  abstract = {Textual semantic similarity plays an increasingly important role in tasks such as information retrieval, text mining and text-based searches. Multiple approaches have been presented to enhance methods for information retrieval by understanding the underlying meaning of sentences. However, most of these focus on single line sentences. In this paper, we try to evaluate the effectiveness of these approaches to understand the semantic meaning of short paragraphs. We use an existing recurrent neural network architecture and train it using document embedding vectors to try and infer the meaning of small paragraphs consisting of one, two or three sentences. We use three different methods - Manhattan distance, Euclidean distance and cosine distance - to evaluate the performance and effectiveness of measuring the semantic similarity. The conclusion compares the performance of all three methods.},
  eventtitle = {2020 {{IEEE International Conference}} on {{Electronics}}, {{Computing}} and {{Communication Technologies}} ({{CONECCT}})},
  keywords = {Computer architecture,deep learning,Information retrieval,Logic gates,Mathematical model,natural language processing,recurrent neural networks,Semantics,Siamese networks,Task analysis,Training,word embedding},
  file = {/Users/max18768/Zotero/storage/SA57Q2K6/Verma and Muralikrishna - 2020 - Semantic similarity between short paragraphs using.pdf;/Users/max18768/Zotero/storage/EJANJCCD/9198445.html}
}

@article{vrandecicWikidataFreeCollaborative2014,
  title = {Wikidata: A Free Collaborative Knowledgebase},
  shorttitle = {Wikidata},
  author = {Vrandečić, Denny and Krötzsch, Markus},
  date = {2014-09-23},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {57},
  number = {10},
  pages = {78--85},
  issn = {0001-0782},
  doi = {10.1145/2629489},
  url = {https://dl.acm.org/doi/10.1145/2629489},
  urldate = {2024-03-27},
  abstract = {This collaboratively edited knowledgebase provides a common source of data for Wikipedia, and everyone else.},
  file = {/Users/max18768/Zotero/storage/82VQLCXW/Vrandečić and Krötzsch - 2014 - Wikidata a free collaborative knowledgebase.pdf}
}

@article{wangDeepNeuralNetworkbased2022,
  title = {Deep Neural Network-Based Relation Extraction: An Overview},
  shorttitle = {Deep Neural Network-Based Relation Extraction},
  author = {Wang, Hailin and Qin, Ke and Zakari, Rufai Yusuf and Lu, Guoming and Yin, Jin},
  date = {2022-03-01},
  journaltitle = {Neural Computing and Applications},
  shortjournal = {Neural Comput \& Applic},
  volume = {34},
  number = {6},
  pages = {4781--4801},
  issn = {1433-3058},
  doi = {10.1007/s00521-021-06667-3},
  url = {https://doi.org/10.1007/s00521-021-06667-3},
  urldate = {2022-12-12},
  abstract = {Knowledge is a formal way of understanding the world, providing human-level cognition and intelligence for the next-generation artificial intelligence (AI). An effective way to automatically acquire this important knowledge, called Relation Extraction (RE), plays a vital role in Natural Language Processing (NLP). To date, there are amount of studies for RE in previous works, among which these technologies based on deep neural networks (DNNs) have become the mainstream direction of this research. In particular, the supervised and distant supervision methods based on DNNs are the most popular and reliable solutions for RE, whose various evolutions on structure and settings have affected this task. Understanding the model structure and related settings will give the researchers a deep insight into RE. However, little research has been done on them. Hence, this paper starts from these two points and carries out analysis around the mainstream research routes, supervised and distant supervision. Meanwhile, we classify all related works according to the evolution of model structure to facilitate the analysis. Finally, we discuss some challenges of RE and give out our conclusion.},
  langid = {english},
  keywords = {Information extraction,Neural networks,Overview,Relation extraction},
  file = {/Users/max18768/Zotero/storage/P5EMTFQ6/Wang et al. - 2022 - Deep neural network-based relation extraction an .pdf}
}

@online{wangSentenceSimilarityLearning2017,
  title = {Sentence {{Similarity Learning}} by {{Lexical Decomposition}} and {{Composition}}},
  author = {Wang, Zhiguo and Mi, Haitao and Ittycheriah, Abraham},
  date = {2017-07-14},
  eprint = {1602.07019},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1602.07019},
  url = {http://arxiv.org/abs/1602.07019},
  urldate = {2024-04-01},
  abstract = {Most conventional sentence similarity methods only focus on similar parts of two input sentences, and simply ignore the dissimilar parts, which usually give us some clues and semantic meanings about the sentences. In this work, we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences. The model represents each word as a vector, and calculates a semantic matching vector for each word based on all words in the other sentence. Then, each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector. After this, a two-channel CNN model is employed to capture features by composing the similar and dissimilar components. Finally, a similarity score is estimated over the composed feature vectors. Experimental results show that our model gets the state-of-the-art performance on the answer sentence selection task, and achieves a comparable result on the paraphrase identification task.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/max18768/Zotero/storage/FDXG6KMB/Wang et al. - 2017 - Sentence Similarity Learning by Lexical Decomposit.pdf;/Users/max18768/Zotero/storage/LCKPVRYI/1602.html}
}

@article{wangSurveyWordEmbeddings2020,
  title = {A Survey of Word Embeddings Based on Deep Learning},
  author = {Wang, Shirui and Zhou, Wenan and Jiang, Chao},
  date = {2020-03-01},
  journaltitle = {Computing},
  shortjournal = {Computing},
  volume = {102},
  number = {3},
  pages = {717--740},
  issn = {1436-5057},
  doi = {10.1007/s00607-019-00768-7},
  url = {https://doi.org/10.1007/s00607-019-00768-7},
  urldate = {2023-07-26},
  abstract = {The representational basis for downstream natural language processing tasks is word embeddings, which capture lexical semantics in numerical form to handle the abstract semantic concept of words. Recently, the word embeddings approaches, represented by deep learning, has attracted extensive attention and widely used in many tasks, such as text classification, knowledge mining, question-answering, smart Internet of Things systems and so on. These neural networks-based models are based on the distributed hypothesis while the semantic association between words can be efficiently calculated in low-dimensional space. However, the expressed semantics of most models are constrained by the context distribution of each word in the corpus while the logic and common knowledge are not better utilized. Therefore, how to use the massive multi-source data to better represent natural language and world knowledge still need to be explored. In this paper, we introduce the recent advances of neural networks-based word embeddings with their technical features, summarizing the key challenges and existing solutions, and further give a future outlook on the research and application.},
  langid = {english},
  keywords = {68T50,Distributed hypothesis,Multi-source data,Neural networks,Word embeddings},
  file = {/Users/max18768/Zotero/storage/7UDS6I3N/Wang et al. - 2020 - A survey of word embeddings based on deep learning.pdf}
}

@online{wangTextEmbeddingsWeaklySupervised2022,
  title = {Text {{Embeddings}} by {{Weakly-Supervised Contrastive Pre-training}}},
  author = {Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},
  date = {2022-12-07},
  eprint = {2212.03533},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2212.03533},
  url = {http://arxiv.org/abs/2212.03533},
  urldate = {2023-06-08},
  abstract = {This paper presents E5, a family of state-of-the-art text embeddings that transfer well to a wide range of tasks. The model is trained in a contrastive manner with weak supervision signals from our curated large-scale text pair dataset (called CCPairs). E5 can be readily used as a general-purpose embedding model for any tasks requiring a single-vector representation of texts such as retrieval, clustering, and classification, achieving strong performance in both zero-shot and fine-tuned settings. We conduct extensive evaluations on 56 datasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the first model that outperforms the strong BM25 baseline on the BEIR retrieval benchmark without using any labeled data. When fine-tuned, E5 obtains the best results on the MTEB benchmark, beating existing embedding models with 40x more parameters.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/Users/max18768/Zotero/storage/ACSNGMWD/Wang et al. - 2022 - Text Embeddings by Weakly-Supervised Contrastive P.pdf;/Users/max18768/Zotero/storage/7V9TV47B/2212.html}
}

@online{wangTSDAEUsingTransformerbased2021,
  title = {{{TSDAE}}: {{Using Transformer-based Sequential Denoising Auto-Encoder}} for {{Unsupervised Sentence Embedding Learning}}},
  shorttitle = {{{TSDAE}}},
  author = {Wang, Kexin and Reimers, Nils and Gurevych, Iryna},
  date = {2021-09-10},
  eprint = {2104.06979},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2104.06979},
  url = {http://arxiv.org/abs/2104.06979},
  urldate = {2022-12-13},
  abstract = {Learning sentence embeddings often requires a large amount of labeled data. However, for most tasks and domains, labeled data is seldom available and creating it is expensive. In this work, we present a new state-of-the-art unsupervised method based on pre-trained Transformers and Sequential Denoising Auto-Encoder (TSDAE) which outperforms previous approaches by up to 6.4 points. It can achieve up to 93.1\% of the performance of in-domain supervised approaches. Further, we show that TSDAE is a strong domain adaptation and pre-training method for sentence embeddings, significantly outperforming other approaches like Masked Language Model. A crucial shortcoming of previous studies is the narrow evaluation: Most work mainly evaluates on the single task of Semantic Textual Similarity (STS), which does not require any domain knowledge. It is unclear if these proposed methods generalize to other domains and tasks. We fill this gap and evaluate TSDAE and other recent approaches on four different datasets from heterogeneous domains.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/max18768/Zotero/storage/JEW32I97/Wang et al. - 2021 - TSDAE Using Transformer-based Sequential Denoisin.pdf;/Users/max18768/Zotero/storage/DVMG5EK5/2104.html}
}

@article{wietingParaphraseDatabaseCompositional2015a,
  title = {From {{Paraphrase Database}} to {{Compositional Paraphrase Model}} and {{Back}}},
  author = {Wieting, John and Bansal, Mohit and Gimpel, Kevin and Livescu, Karen},
  date = {2015-06-01},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  shortjournal = {Transactions of the Association for Computational Linguistics},
  volume = {3},
  pages = {345--358},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00143},
  url = {https://doi.org/10.1162/tacl_a_00143},
  urldate = {2024-04-04},
  abstract = {The Paraphrase Database (PPDB; Ganitkevitch et al., 2013) is an extensive semantic resource, consisting of a list of phrase pairs with (heuristic) confidence estimates. However, it is still unclear how it can best be used, due to the heuristic nature of the confidences and its necessarily incomplete coverage. We propose models to leverage the phrase pairs from the PPDB to build parametric paraphrase models that score paraphrase pairs more accurately than the PPDB’s internal scores while simultaneously improving its coverage. They allow for learning phrase embeddings as well as improved word embeddings. Moreover, we introduce two new, manually annotated datasets to evaluate short-phrase paraphrasing models. Using our paraphrase model trained using PPDB, we achieve state-of-the-art results on standard word and bigram similarity tasks and beat strong baselines on our new short phrase paraphrase tasks.},
  file = {/Users/max18768/Zotero/storage/TRM4WMC7/Wieting et al. - 2015 - From Paraphrase Database to Compositional Paraphra.pdf;/Users/max18768/Zotero/storage/Y78Q4YN5/From-Paraphrase-Database-to-Compositional.html}
}

@online{wilkersonLargeScaleComputerizedText2017,
  type = {SSRN Scholarly Paper},
  title = {Large-{{Scale Computerized Text Analysis}} in {{Political Science}}: {{Opportunities}} and {{Challenges}}},
  shorttitle = {Large-{{Scale Computerized Text Analysis}} in {{Political Science}}},
  author = {Wilkerson, John and Casas, Andreu},
  date = {2017-05-01},
  number = {2968080},
  location = {Rochester, NY},
  doi = {10.1146/annurev-polisci-052615-025542},
  url = {https://papers.ssrn.com/abstract=2968080},
  urldate = {2023-06-15},
  abstract = {Text has always been an important data source in political science. What has changed in recent years is the feasibility of investigating large amounts of text quantitatively. The internet provides political scientists with more data than their mentors could have imagined, and the research community is providing accessible text analysis software packages, along with training and support. As a result, text-as-data research is becoming mainstream in political science. Scholars are tapping new data sources, they are employing more diverse methods, and they are becoming critical consumers of findings based on those methods. In this article, we first describe the four stages of a typical text-as-data project. We then review recent political science applications and explore one important methodological challenge—topic model instability—in greater detail.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Andreu Casas,John Wilkerson,Large-Scale Computerized Text Analysis in Political Science: Opportunities and Challenges,SSRN},
  file = {/Users/max18768/Zotero/storage/Y2WDBP3Q/wilkerson2017.pdf.pdf}
}

@inproceedings{wolfTransformersStateoftheArtNatural2020,
  title = {Transformers: {{State-of-the-Art Natural Language Processing}}},
  shorttitle = {Transformers},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{System Demonstrations}}},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and family=Platen, given=Patrick, prefix=von, useprefix=true and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
  editor = {Liu, Qun and Schlangen, David},
  date = {2020-10},
  pages = {38--45},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2020.emnlp-demos.6},
  url = {https://aclanthology.org/2020.emnlp-demos.6},
  urldate = {2024-04-16},
  abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.},
  file = {/Users/max18768/Zotero/storage/PBW86K9M/Wolf et al. - 2020 - Transformers State-of-the-Art Natural Language Pr.pdf}
}

@article{yangSurveyExtractionCausal2022,
  title = {A Survey on Extraction of Causal Relations from Natural Language Text},
  author = {Yang, Jie and Han, Soyeon Caren and Poon, Josiah},
  date = {2022-05-01},
  journaltitle = {Knowledge and Information Systems},
  shortjournal = {Knowl Inf Syst},
  volume = {64},
  number = {5},
  pages = {1161--1186},
  issn = {0219-3116},
  doi = {10.1007/s10115-022-01665-w},
  url = {https://doi.org/10.1007/s10115-022-01665-w},
  urldate = {2022-12-12},
  abstract = {As an essential component of human cognition, cause–effect relations appear frequently in text, and curating cause–effect relations from text helps in building causal networks for predictive tasks. Existing causality extraction techniques include knowledge-based, statistical machine learning (ML)-based, and deep learning-based approaches. Each method has its advantages and weaknesses. For example, knowledge-based methods are understandable but require extensive manual domain knowledge and have poor cross-domain applicability. Statistical machine learning methods are more automated because of natural language processing (NLP) toolkits. However, feature engineering is labor-intensive, and toolkits may lead to error propagation. In the past few years, deep learning techniques attract substantial attention from NLP researchers because of its powerful representation learning ability and the rapid increase in computational resources. Their limitations include high computational costs and a lack of adequate annotated training data. In this paper, we conduct a comprehensive survey of causality extraction. We initially introduce primary forms existing in the causality extraction: explicit intra-sentential causality, implicit causality, and inter-sentential causality. Next, we list benchmark datasets and modeling assessment methods for causal relation extraction. Then, we present a structured overview of the three techniques with their representative systems. Lastly, we highlight existing open challenges with their potential directions.},
  langid = {english},
  keywords = {Causality extraction,Deep learning,Explicit intra-sentential causality,Implicit causality,Inter-sentential causality},
  file = {/Users/max18768/Zotero/storage/J9FTAW9R/Yang et al. - 2022 - A survey on extraction of causal relations from na.pdf}
}

@online{youngRecentTrendsDeep2018,
  title = {Recent {{Trends}} in {{Deep Learning Based Natural Language Processing}}},
  author = {Young, Tom and Hazarika, Devamanyu and Poria, Soujanya and Cambria, Erik},
  date = {2018-11-24},
  eprint = {1708.02709},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1708.02709},
  url = {http://arxiv.org/abs/1708.02709},
  urldate = {2023-06-15},
  abstract = {Deep learning methods employ multiple processing layers to learn hierarchical representations of data and have produced state-of-the-art results in many domains. Recently, a variety of model designs and methods have blossomed in the context of natural language processing (NLP). In this paper, we review significant deep learning related models and methods that have been employed for numerous NLP tasks and provide a walk-through of their evolution. We also summarize, compare and contrast the various models and put forward a detailed understanding of the past, present and future of deep learning in NLP.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/max18768/Zotero/storage/NWL9T54I/Young et al. - 2018 - Recent Trends in Deep Learning Based Natural Langu.pdf;/Users/max18768/Zotero/storage/WXUJTMXJ/1708.html}
}

@article{yuReviewRecurrentNeural2019,
  title = {A {{Review}} of {{Recurrent Neural Networks}}: {{LSTM Cells}} and {{Network Architectures}}},
  shorttitle = {A {{Review}} of {{Recurrent Neural Networks}}},
  author = {Yu, Yong and Si, Xiaosheng and Hu, Changhua and Zhang, Jianxun},
  date = {2019-07-01},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {31},
  number = {7},
  pages = {1235--1270},
  issn = {0899-7667},
  doi = {10.1162/neco_a_01199},
  url = {https://doi.org/10.1162/neco_a_01199},
  urldate = {2024-04-06},
  abstract = {Recurrent neural networks (RNNs) have been widely adopted in research areas concerned with sequential data, such as text, audio, and video. However, RNNs consisting of sigma cells or tanh cells are unable to learn the relevant information of input data when the input gap is large. By introducing gate functions into the cell structure, the long short-term memory (LSTM) could handle the problem of long-term dependencies well. Since its introduction, almost all the exciting results based on RNNs have been achieved by the LSTM. The LSTM has become the focus of deep learning. We review the LSTM cell and its variants to explore the learning capacity of the LSTM cell. Furthermore, the LSTM networks are divided into two broad categories: LSTM-dominated networks and integrated LSTM networks. In addition, their various applications are discussed. Finally, future research directions are presented for LSTM networks.},
  file = {/Users/max18768/Zotero/storage/YBXV4M3E/A-Review-of-Recurrent-Neural-Networks-LSTM-Cells.html}
}

@inproceedings{zadSurveyDeepLearning2021,
  title = {A {{Survey}} of {{Deep Learning Methods}} on {{Semantic Similarity}} and {{Sentence Modeling}}},
  booktitle = {2021 {{IEEE}} 12th {{Annual Information Technology}}, {{Electronics}} and {{Mobile Communication Conference}} ({{IEMCON}})},
  author = {Zad, Samira and Heidari, Maryam and Hajibabaee, Parisa and Malekzadeh, Masoud},
  date = {2021-10},
  pages = {0466--0472},
  issn = {2644-3163},
  doi = {10.1109/IEMCON53756.2021.9623078},
  abstract = {Semantics is a research field that has gained an extensive interest recently. This survey describes recent works in the field of semantics, a part of the broader area of computational linguistics. One of the important aspects of computational linguistics is using proper methods to distribute semantics for obtaining representations of the meaning of words. This survey summarizes the latest state of the art approaches in semantics that use deep learning methods, datasets, and lexical databases, specifying semantics under two categories such as semantic similarity and sentence modeling.},
  eventtitle = {2021 {{IEEE}} 12th {{Annual Information Technology}}, {{Electronics}} and {{Mobile Communication Conference}} ({{IEMCON}})},
  keywords = {computational linguistics,Computational modeling,Conferences,Databases,deep learning,Deep learning,Mobile communication,natural language processing,Natural language processing,neural networks,Prio 1,Semantic similarity,semantic textual similarity,Semantics,sentence modeling,survey,word embeddings},
  file = {/Users/max18768/Zotero/storage/JI5VFQKF/Zad et al. - 2021 - A Survey of Deep Learning Methods on Semantic Simi.pdf;/Users/max18768/Zotero/storage/4B9ZG27E/9623078.html}
}

@online{zhangContrastiveLearningSentence2023,
  title = {Contrastive {{Learning}} of {{Sentence Embeddings}} from {{Scratch}}},
  author = {Zhang, Junlei and Lan, Zhenzhong and He, Junxian},
  date = {2023-05-24},
  eprint = {2305.15077},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.15077},
  url = {http://arxiv.org/abs/2305.15077},
  urldate = {2023-07-31},
  abstract = {Contrastive learning has been the dominant approach to train state-of-the-art sentence embeddings. Previous studies have typically learned sentence embeddings either through the use of human-annotated natural language inference (NLI) data or via large-scale unlabeled sentences in an unsupervised manner. However, even in the case of unlabeled data, their acquisition presents challenges in certain domains due to various reasons. To address these issues, we present SynCSE, a contrastive learning framework that trains sentence embeddings with synthesized data. Specifically, we explore utilizing large language models to synthesize the required data samples for contrastive learning, including (1) producing positive and negative annotations given unlabeled sentences (SynCSE-partial), and (2) generating sentences along with their corresponding annotations from scratch (SynCSE-scratch). Experimental results on sentence similarity and reranking tasks indicate that both SynCSE-partial and SynCSE-scratch greatly outperform unsupervised baselines, and SynCSE-partial even achieves comparable performance to the supervised models in most settings.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/max18768/Zotero/storage/H5Y3PGQN/Zhang et al. - 2023 - Contrastive Learning of Sentence Embeddings from S.pdf;/Users/max18768/Zotero/storage/VUEYN4BJ/2305.html}
}

@article{zhengDetectionMedicalText2019,
  title = {Detection of Medical Text Semantic Similarity Based on Convolutional Neural Network},
  author = {Zheng, Tao and Gao, Yimei and Wang, Fei and Fan, Chenhao and Fu, Xingzhi and Li, Mei and Zhang, Ya and Zhang, Shaodian and Ma, Handong},
  date = {2019-08-07},
  journaltitle = {BMC Medical Informatics and Decision Making},
  shortjournal = {BMC Med Inform Decis Mak},
  volume = {19},
  number = {1},
  pages = {156},
  issn = {1472-6947},
  doi = {10.1186/s12911-019-0880-2},
  url = {https://doi.org/10.1186/s12911-019-0880-2},
  urldate = {2024-04-01},
  abstract = {Imaging examinations, such as ultrasonography, magnetic resonance imaging and computed tomography scans, play key roles in healthcare settings. To assess and improve the quality of imaging diagnosis, we need to manually find and compare the pre-existing reports of imaging and pathology examinations which contain overlapping exam body sites from electrical medical records (EMRs). The process of retrieving those reports is time-consuming. In this paper, we propose a convolutional neural network (CNN) based method which can better utilize semantic information contained in report texts to accelerate the retrieving process.},
  langid = {english},
  keywords = {Convolutional neural network,LIME,Natural language processing,Text similarity},
  file = {/Users/max18768/Zotero/storage/SIIJYYGS/Zheng et al. - 2019 - Detection of medical text semantic similarity base.pdf}
}

@inproceedings{zhouProblemsCosineMeasure2022,
  title = {Problems with {{Cosine}} as a {{Measure}} of {{Embedding Similarity}} for {{High Frequency Words}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Zhou, Kaitlyn and Ethayarajh, Kawin and Card, Dallas and Jurafsky, Dan},
  editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
  date = {2022-05},
  pages = {401--423},
  publisher = {Association for Computational Linguistics},
  location = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-short.45},
  url = {https://aclanthology.org/2022.acl-short.45},
  urldate = {2024-03-16},
  abstract = {Cosine similarity of contextual embeddings is used in many NLP tasks (e.g., QA, IR, MT) and metrics (e.g., BERTScore). Here, we uncover systematic ways in which word similarities estimated by cosine over BERT embeddings are understated and trace this effect to training data frequency. We find that relative to human judgements, cosine similarity underestimates the similarity of frequent words with other instances of the same word or other words across contexts, even after controlling for polysemy and other factors. We conjecture that this underestimation of similarity for high frequency words is due to differences in the representational geometry of high and low frequency words and provide a formal argument for the two-dimensional case.},
  eventtitle = {{{ACL}} 2022},
  file = {/Users/max18768/Zotero/storage/YLH5DLEN/Zhou et al. - 2022 - Problems with Cosine as a Measure of Embedding Sim.pdf}
}

@article{zhuangComprehensiveSurveyTransfer2021,
  title = {A {{Comprehensive Survey}} on {{Transfer Learning}}},
  author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  date = {2021-01},
  journaltitle = {Proceedings of the IEEE},
  volume = {109},
  number = {1},
  pages = {43--76},
  issn = {1558-2256},
  doi = {10.1109/JPROC.2020.3004555},
  url = {https://ieeexplore.ieee.org/abstract/document/9134370?casa_token=gXwLWIz8oPMAAAAA:qW4tsLJKyOxJKu6A3p4bwAarjZ92DHjwDZYkDTKKCO4nemL_gJn-TIGdz2ivvCgX08SFu72vF0E},
  urldate = {2024-04-06},
  abstract = {Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target-domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning research studies, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey article reviews more than 40 representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over 20 representative transfer learning models are used for experiments. The models are performed on three different data sets, that is, Amazon Reviews, Reuters-21578, and Office-31, and the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.},
  eventtitle = {Proceedings of the {{IEEE}}},
  keywords = {Adaptation models,Covariance matrices,Data models,Domain adaptation,interpretation,machine learning,Machine learning,Semisupervised learning,transfer learning,Transfer learning},
  file = {/Users/max18768/Zotero/storage/H42SQ6YR/Zhuang et al. - 2021 - A Comprehensive Survey on Transfer Learning.pdf;/Users/max18768/Zotero/storage/UQBITCAR/9134370.html}
}

@article{zhuComputingSemanticSimilarity2017,
  title = {Computing {{Semantic Similarity}} of {{Concepts}} in {{Knowledge Graphs}}},
  author = {Zhu, Ganggao and Iglesias, Carlos A.},
  date = {2017-01},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {29},
  number = {1},
  pages = {72--85},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2016.2610428},
  url = {https://ieeexplore.ieee.org/abstract/document/7572993},
  urldate = {2024-03-27},
  abstract = {This paper presents a method for measuring the semantic similarity between concepts in Knowledge Graphs (KGs) such as WordNet and DBpedia. Previous work on semantic similarity methods have focused on either the structure of the semantic network between concepts (e.g., path length and depth), or only on the Information Content (IC) of concepts. We propose a semantic similarity method, namely wpath, to combine these two approaches, using IC to weight the shortest path length between concepts. Conventional corpus-based IC is computed from the distributions of concepts over textual corpus, which is required to prepare a domain corpus containing annotated concepts and has high computational cost. As instances are already extracted from textual corpus and annotated by concepts in KGs, graph-based IC is proposed to compute IC based on the distributions of concepts over instances. Through experiments performed on well known word similarity datasets, we show that the wpath semantic similarity method has produced a statistically significant improvement over other semantic similarity methods. Moreover, in a real category classification evaluation, the wpath method has shown the best performance in terms of accuracy and F score.},
  eventtitle = {{{IEEE Transactions}} on {{Knowledge}} and {{Data Engineering}}},
  keywords = {DBpedia,information content,Integrated circuits,Knowledge based systems,Knowledge engineering,knowledge graph,Measurement,Motion pictures,semantic relatedness,Semantic similarity,Semantics,Taxonomy,WordNet},
  file = {/Users/max18768/Zotero/storage/2P38LGGX/Zhu and Iglesias - 2017 - Computing Semantic Similarity of Concepts in Knowl.pdf;/Users/max18768/Zotero/storage/GG8KNF43/7572993.html}
}

@inproceedings{zucconIntegratingEvaluatingNeural2015,
  title = {Integrating and {{Evaluating Neural Word Embeddings}} in {{Information Retrieval}}},
  booktitle = {Proceedings of the 20th {{Australasian Document Computing Symposium}}},
  author = {Zuccon, Guido and Koopman, Bevan and Bruza, Peter and Azzopardi, Leif},
  date = {2015-12-08},
  series = {{{ADCS}} '15},
  pages = {1--8},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2838931.2838936},
  url = {https://dl.acm.org/doi/10.1145/2838931.2838936},
  urldate = {2024-04-04},
  abstract = {Recent advances in neural language models have contributed new methods for learning distributed vector representations of words (also called word embeddings). Two such methods are the continuous bag-of-words model and the skipgram model. These methods have been shown to produce embeddings that capture higher order relationships between words that are highly effective in natural language processing tasks involving the use of word similarity and word analogy. Despite these promising results, there has been little analysis of the use of these word embeddings for retrieval. Motivated by these observations, in this paper, we set out to determine how these word embeddings can be used within a retrieval model and what the benefit might be. To this aim, we use neural word embeddings within the well known translation language model for information retrieval. This language model captures implicit semantic relations between the words in queries and those in relevant documents, thus producing more accurate estimations of document relevance. The word embeddings used to estimate neural language models produce translations that differ from previous translation language model approaches; differences that deliver improvements in retrieval effectiveness. The models are robust to choices made in building word embeddings and, even more so, our results show that embeddings do not even need to be produced from the same corpus being used for retrieval.},
  isbn = {978-1-4503-4040-3},
  file = {/Users/max18768/Zotero/storage/ERV5MAC2/Zuccon et al. - 2015 - Integrating and Evaluating Neural Word Embeddings .pdf}
}
