%\documentclass[11pt]{scrartcl}
\documentclass[11pt]{scrreprt}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
% \usepackage[ngerman]{babel}

\usepackage{graphicx}
\graphicspath{ 
	{../resources/imgs/},
	{../../data/plots/},
	{../../data/plots/dataset_evaluation/}
}

%\usepackage[backend=biber, style=ieee]{biblatex}
\usepackage[backend=bibtex, style=authoryear-comp, url=false]{biblatex}
%\newcommand{\citep}{\parencite}  % adds \citep alias for citing with parenthesis
\let\citef\cite  % makes \citef an alias for \cite
\let\cite\parencite  % makes \cite an alias for \parencite
\addbibresource{../resources/MA.bib}
\AtEveryBibitem{
%\ifentrytype{article}{
%    \clearfield{urlyear}
%}{}
%\ifentrytype{book}{
%    \clearfield{urlyear}
%}{}
%\ifentrytype{collection}{
%    \clearfield{urlyear}
%}{}
%\ifentrytype{incollection}{
%    \clearfield{urlyear}
%}{}
%\ifentrytype{inproceedings}{
%    \clearfield{urlyear}
%}{}
\ifentrytype{online}{
    \clearfield{urlyear}
}{}
}

\KOMAoptions{parskip=half}

\usepackage[margin=3cm]{geometry} % Adjust margins

\usepackage{soul}
\usepackage{todonotes}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{rotating}
\usepackage{pdfpages}
\usepackage{csquotes}
\usepackage{algorithm}
\usepackage{newfloat}
\usepackage{enumitem}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[export]{adjustbox}

%\usepackage{array}  % needed for '\newcolumntype' command
%\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}  % define "L" column type
%% Define a new counter for tracking the list number

% Counter for observation lists
\newcounter{listcounter}
\setcounter{listcounter}{0}

\DeclareFloatingEnvironment[
  listname = {List of Patterns} ,
  name = Pattern,
  placement = h,
  within = none
]{pattern}

\DeclareFloatingEnvironment[
  listname = {List of Hyperedges} ,
  name = Hyperedge,
  placement = h,
  within = none
]{hedge}

\Crefname{pattern}{Pattern}{Patterns}
\crefname{pattern}{pattern}{patterns}
\Crefname{hedge}{Hyperedge}{Hyperedges}
\crefname{hedge}{hyperedge}{hyperedges}



\usepackage{lipsum}  % produces dummy text


\begin{document}


% ========== Title page

\titlehead
{
\begin{tabular}{ll}
\begin{minipage}{0.5\textwidth}
	\textbf{Technische Universität Berlin} \\
	Fakultät IV: Elektrotechnik und Informatik \\
	Institut für Telekommunikationssysteme \\
	Fachgebiet Verteilte offene Systeme	
\end{minipage}
&
\begin{minipage}{0.5\textwidth}
	\raggedleft
	\includegraphics[width=0.3\textwidth]{logos/tub_logo_bw.jpg}			
\end{minipage}

\end{tabular}
}

\subject{Masters Thesis in Computer Science}
\title{Extending Semantic Hypergraphs by Neural Embedding-based Semantic Similarity for Pattern Matching}
\author{Max Reinhard \\ \small Matrikelnummer: 359417}

\date{\today}
\publishers{Supervised by Prof. Dr. Manfred Hauswirth \\
	Additional guidance by Prof. Dr. Camille Roth\thanks{Centre Marc Bloch (An-Institut der Humboldt-Universität zu Berlin)} \\ 
	and Dipl.-Math. Thilo Ernst\thanks{Fraunhofer-Institut für offene Kommunikationssysteme}}
	
\maketitle

% ========== Abstract
\begin{abstract}
\textbf{Abstract}
\lipsum[1-2]
\end{abstract}

% ========== TOC
\tableofcontents
\newpage

% ========== Body
% ==============================

% ========== 
\chapter{Introduction}
\begin{itemize}
	\item Context: The big problem
	\item Problem statement: The small problem
	\item Methodology / Strategy
	\item Structure
\end{itemize}

\textbf{Notes:}
\begin{itemize}
	\item Huge amounts of text, which can provide insight about stuff
	\item Automatic tools can provide assistance for humans to process all the text
	\item This generally means filtering the original text corpus or otherwise reducing amount of information the information that has to be processed by humans
	\item Filtering introduces a bias
	\item Especially for scientific purposes it is relevant to mitigate bias or at least understand what bias has been introduced (to make it transparent)
	\item Semantic Hypergraphs can be a valuable tool for that because...
\end{itemize}


Human life in times of widespread use of the internet and smartphones is most certainly more than ever interspersed with text-based communication...

Great progress has been made in the made in NLP, IR and IE in the past decade. This advancement of the state-of-the-art can primarily be attributed \textit{Deep Learning} based methods, often also referred to as \textit{Neural Networks}. \cite{youngRecentTrendsDeep2018, minRecentAdvancesNatural2023, hirschbergAdvancesNaturalLanguage2015}

\section{References from the Future}
\textit{Natural Language Processing} (NLP)
\textit{Information Retrieval} (IR)
\textit{Information Extraction} (IE)

open-opaque / strict-adaptive categorisation dimensions



\section{Expose intro}

A significant part of the social world is nowadays being represented by digitally manifested text. Examples for this range from instant messaging, social media and any form of collective web activity to encyclopaedic websites, digitized libraries and government intelligence. The amount and richness of available social text makes it a valuable data source for social science research while simultaneously creating an interest in automatic systems to analyze these texts on a large scale \cite{evansMachineTranslationMining2016}. Such research can be understood as part of the domain of \textit{Computational Social Science} (CSS) \cite{lazerComputationalSocialScience2009}.

Systems based on techniques from the field of \textit{ Natural Language Processing} (NLP), as well well as the interlinked fields of \textit{Information Retrieval} (IR) and \textit{Information Extraction} (IE), have demonstrated great success in a variety of task related to text analysis. This success is largly attributed to the advancements of applying of \textit{Machine Learning} (ML) and especially \textit{Deep Learning} (DL) methods to text \cite{hirschbergAdvancesNaturalLanguage2015} \cite{qiuPretrainedModelsNatural2020}. While being very effective at predicting or decision making, ML- and specifically DL-based systems generally do not deliver an explanation for their judgement and can mostly be viewed as "black box models" that are not transparent in their prediction or decision making process \cite{rudinStopExplainingBlack2019}. Conversely this transparency and explainability is of high interest in CSS applications such as predicting political opinion based on social media activity \cite{wilkersonLargeScaleComputerizedText2017}.
%DL models 

The \textit{Semantic Hypergraph} (SH) \cite{menezesSemanticHypergraphs2021} is a framework for representing and analyzing \textit{Natural Language} (NL). NL sentences can be modelled as an ordered, recursive hypergraph which can be represented in a formal language. The framework allows to specify semantic patterns in this formal language which can be matched against an existing SH. It aims to provide an \textit{open} and \textit{adaptive} system to analyse text corpora, especially in the domain of CSS. The framework is \textit{open} in the sense that its representation formalism is inspectable and intelligible by humans and that the pattern matching follows explicit rules. The framework is adaptive in the sense that the parsing is built from adaptive, ML-based subsystems and therefore allows for an error-tolerant parsing from \textit{NL} to \textit{SH} in regards to grammatical and syntactical correctness.



% ========== 
\chapter{Background}
In this chapter the necessary background for this work is presented. This includes on the one hand the Semantic Hypergraph (SH) framework and its pattern matching capabilities in particular. On the other hand the concept of semantic relatedness and specifically semantic similarity is explained with a focus on its realisation in the form of neural embedding based semantic similarity (NESS).

\section{Semantic Hypergraph Framework}
\label{sec:semantic-hypergraph-framework}

The Semantic Hypergraph framework, introduced in \citef{menezesSemanticHypergraphs2021}, offers a novel approach for the representation and analysis of natural language (NL) text. At its core, the SH framework transforms text into a structured, symbolic format, thereby the analysis of text collections. A pattern language intrinsic to the SH framework enables the definition of patterns that encapsulate generalisations of text in the SH format.

Designed primarily for computational social science (CSS) researchers, the SH framework emphasises a high degree of openness, encompassing transparency and explainability. Additionally, the SH framework exhibits a level of adaptiveness, attributing to its reliance on Machine Learning (ML) components for the translation of NL text into the SH format. This adaptiveness reflects the framework's capability to evolve and accommodate the variable form of natural language.

One of the objectives of the SH framework is to bridge the existing gap in open-adaptive natural language processing (NLP) systems. By providing a mechanism for structured symbolic representation and pattern-based analysis of text, the SH framework proposes a solution that combines the benefits of machine learning adaptiveness with the requisites of open and explainable research in CSS and beyond.

For a more comprehensive description of the Semantic Hypergraph framework, the original publication \cite{menezesSemanticHypergraphs2021} should be consulted.


\subsection{Structure and Syntax}
In the SH framework, natural language is represented as a recursive, ordered \textit{hypergraph} (in the following also simply referred to as \textit{graph}). Such a hypergraph consists of \textit{hyperedges} (in the following also referred to as \textit{edges}). These hyperedges themselves can consist of a generally unrestricted number of hyperedges. To denote the hierarchy between hyperedges the terms \textit{parent edge} and \textit{child edge} or \textit{sub edge} are introduced. If an edge \(e_A\) contains edges \(e_B\) and \(e_C\), \(e_A\) is considered to be the parent edge of child or sub edges \(e_B\) and \(e_C\). Since hyperedges are ordered, the edge \(e_A = (e_B \ e_C)\) is different to an edge \((e_C \  e_B) \neq e_A\). If a hyperedge is not a composition of multiple hyperedges, it is called an \textit{atom}.


\subsubsection{Hyperedge Examples}
Some example phrases and their corresponding hyperedges are given to help illustrate the general concept of the hypergraph representation and the hyperedge notation:

\begin{itemize}
	\item "banana": \textsf{banana/C}, "eat": \textsf{eat/C}, "sweet": \textsf{sweet/C}
	\item "Mango is tasty.": \textsf{(is/P.sc mango/C tasty/C)}
	\item "Sophie throws the ball.": \textsf{(throws/P.so sophie/C (the/M ball/C))}
\end{itemize}


\subsubsection{Hyperedge Components and Notation}
\label{sec:hyperedge-notation}
An atomic hyperedge is noted as follows:

\begin{center}
	\texttt{<CONTENT>}\textsf{/}\texttt{<TYPE>} or \\
	\texttt{<CONTENT>}\textsf{/}\texttt{<TYPE>}\textsf{.}\texttt{<ARGUMENT ROLES>}
\end{center}

The different edge components \textit{content}, \textit{type} and potentially \textit{argument roles} are here represented by placeholders in brackets (\texttt{<>}). In an atom, the content is always noted in lowercase. Non-atomic hyperedges are noted as a composition of their sub edges, as illustrated above. They also always have content and type, which are not part of the notation, but can be inferred. Each of the hyperedge components is explained in the following.

\subsubsection{Hyperedge Types}
Every hyperedge in a semantic hypergraph has a specific type. All types that an edge can assume are listed in \cref{tab:hyperedge-types}, which is adapted from \citef[p. 7]{menezesSemanticHypergraphs2021}.

\begin{table}[h]
\centering
\begin{tabular}{clp{5cm}l}
\toprule
\textbf{Code} & \textbf{Type} & \textbf{Purpose} & \textbf{Example} \\
\midrule
\textsf{C} & Concept & Define atomic concepts & \textbf{\textsf{apple/C}} \\
\textsf{P} & Predicate & Build relations & \textsf{(\textbf{is/P} berlin/C nice/C)} \\
\textsf{M} & Modifier & Modify a concept, predicate, modifier, trigger & \textsf{(\textbf{red/M} shoes/C)} \\
\textsf{B} & Builder & Build concepts from concepts & \textsf{(\textbf{of/B} capital/C germany/C)} \\
\textsf{T} & Trigger & Build specifications & \textsf{(\textbf{in/T} 1994/C)} \\
\textsf{J} & Conjunction & Define sequences of concepts or relations & \textsf{(\textbf{and/J} meat/C potatoes/C)} \\
\textsf{R} & Relation & Express facts, statements, questions, orders, ... & \textsf{\textbf{(is/P berlin/C nice/C)}} \\
\textsf{S} & Specifier & Relation specification (e.g., condition, time, ...) & \textsf{\textbf{(in/T 1976/C)}} \\
\bottomrule
\end{tabular}
\caption{Hyperedge types with their purpose and examples. For the examples the sub edge(s) that correspond(s) to the respective type is noted in bold face.}
\label{tab:hyperedge-types}
\end{table}

%\begin{table}[ht]
%\centering
%\begin{tabular}{clccc}
%\toprule
%\textbf{Code} & \textbf{Type} & \textbf{Connector} & \multicolumn{2}{c}{\textbf{Atomicity}} \\
%              &               &               & \textbf{Atom} & \textbf{Non-atom} \\
%\midrule
%\textsf{C}    & Concept       &               & \checkmark & \checkmark \\
%\textsf{P}    & Predicate     & \checkmark    & \checkmark & \checkmark \\
%\textsf{M}    & Modifier      & \checkmark    & \checkmark & \checkmark \\
%\textsf{B}    & Builder       & \checkmark    & \checkmark &            \\
%\textsf{T}    & Trigger       & \checkmark    & \checkmark &            \\
%\textsf{J}    & Conjunction   & \checkmark    & \checkmark &            \\
%\textsf{R}    & Relation      &               &            & \checkmark \\
%\textsf{S}    & Specifier     &               &            & \checkmark \\
%\bottomrule
%\end{tabular}
%\caption{Hyperedge types with their connector status and atomicity.}
%\label{table:hyperedge_types}
%\end{table}

\paragraph{Type Inference Rules}
The type of a parent hyperedge is inferred based on the types and the order of its child hyperedges following specific inference rules. These rules can be inspected in \citef[p. 8]{menezesSemanticHypergraphs2021}, but are omitted here because they are deemed mostly irrelevant for this work.


\subsubsection{Hyperedge Content}
The textual representation of an edge is referred to as \textit{edge content} or \textit{edge text}. For atoms this is generally a word, except for special symbols (see below). Generally speaking, a hyperedges content is the reconstruction of the word, phrase or sentence that it represents, which can be produced by following the hyperedge structure.

\paragraph{Special Content Symbols}
\label{sec:special-type-symbols}
The content of edges of the builder and conjunction type can also take on the form of special symbols \textsf{+/B} and \textsf{:/J}. This is utilised to represent phrases where there is no (meaningful) word or character attributable to the edges content. Examples of this are (taken from \citef[p. 7]{menezesSemanticHypergraphs2021}):

\begin{itemize}
	\item "guitar player": \textsf{(+/B guitar/C player/C)}
	\item "Freud, the famous psychiatrist": \textsf{(:/J freud/C (the/M (famous/M psychiatrist/C)))}
\end{itemize}


\subsubsection{Argument Roles}
To make the meaning of the relation of sub edges in a parent edge more explicit, builder and predicate edges can have argument roles. In parent edges that start with an edge of one of these two types, the roles of the other edges can be specified by adding argument roles to the builder or predicate edges. These are denoted as shown in \cref{sec:hyperedge-notation}. We only list the possible argument roles here to avoid confusing the reader, since they are used in some of the edges and patterns in the following, but they are not in the focus of this work. An explanation of the functionality of all argument roles can be inspected in \citef[p. 8]{menezesSemanticHypergraphs2021}.


\subsection{Translation Process}
For natural language text to be represented in the Semantic Hypergraph framework, it needs to be translated (or parsed). This aspect of the framework is mostly irrelevant for this work in is therefore only described briefly here. The translation is realised by machine learning based software components. This is a high-level description of the process: 

\begin{enumerate}
	\item Segment text into sentences and tokenise sentences
	\item Annotate tokens with POS-tags, dependency labels and NER categories
	\item Use a trained classification model to predict an atom edge type for a token using its annotation labels as input features
	\item Recursively construct more complex hyperedges from the sequence of atomic edges by essentially applying the type inference rules
\end{enumerate}

The edges that correspond to the sentences into which the original input text has been segmented are referred to as \textit{root edges} in the following. 


\subsection{Pattern Matching}
\label{sec:sh-pattern-matching}
Text represented in the SH framework can be analysed via \textit{Semantic Hypergraph Pattern Matching} (SHPM). Information can be extracted from texts represented as hypergraph by constructing patterns in the frameworks pattern language and matching these patterns against the graph. The extracted information can take on the form of e.g. specific kinds of expressions and parts of these expression that fulfil a certain role.


\subsubsection{Pattern Language}
\label{sec:sh-pattern-language}
The \textit{Semantic Hypergraph Pattern Language} (SHPL) essentially allows for generalisations of hyperedge structure and content. All valid hyperedges are generally also valid patterns in the SHPL. \Cref{tab:pattern-language-elements} list all of the pattern languages elements (also referred to as operators), which are relevant to this work. Their functionality and therefore the form of generalisation that they enable is also described.

\begin{table}[h]
\centering
\begin{tabular}{lcp{9cm}}
\toprule
\textbf{Element} & \textbf{Notation} & \textbf{Function} \\
\midrule
Variable & \textsf{VAR} & Matches any edge (restrictable by type and argument roles). Returns the edge that it captures. \\
Wildcard & \textsf{*} & Matches any edge (restrictable by type and argument roles). \\
Options list & \textsf{[...]} & Matches one of the given options. Applicable to edge type and content. \\
Innermost atom & \textsf{>} & Matches against the innermost atom of the edge that it captures. Removes an arbitrary level of nested modifiers. \\
Lemma function & \textsf{lemma} & Matches if the lemma of the captured edge content equals the given word. Combinable with an options list. Applicable only to atoms. \\
Argument role set & \textsf{\{\(\alpha\beta\gamma, \epsilon\)\}} & Allows for argument roles in arbitrary order. Comma-separated argument roles are optional \\
\bottomrule
\end{tabular}
\caption{Pattern language elements with their notation and function}
\label{tab:pattern-language-elements}
\end{table}

\subsubsection{Matching Process}
\label{sec:pattern-matching-process}
At its core, the matching process involves comparing a predefined pattern against either an entire hypergraph, a set of edges, or a single edge. Independent of the matching target, this comparison is conducted one edge at a time, determining whether this edge aligns with the pattern. If pattern and edge do not match the result of this process is only this binary information of \textit{No Match}. If pattern and edge do match, the result of the matching process depends on whether the pattern contains variables. I this case, the result is not only the binary information of \textit{Match}, but also the assignment of the variables contained in the pattern. It is possible for a single edge to yield multiple variable assignments, however this aspect will not be discussed further it holds limited relevance for this work. 

The matching process works recursively, processing both the structure and content of the edge in relation to the pattern. A detailed discussion of this process is delegated to \cref{cha:implementation}\todo{maybe more specific ref}. If a generalising part of the pattern (sub pattern) structurally matches a sub edge of the candidate edge, this sub edge is considered to be \textit{captured} by this sub pattern. This is relevant for returning variable assignments and further processing as is the case with the innermost atom operator or any content generalisation functionality.

Assuming the matching the matching process is examining a specific sub pattern and sub edge, the content defined in the sub pattern is referred to as the \textit{(pattern) reference content}. It is worth noting that a pattern may exclusively consist of structural elements, without specifying content. The content of the sub edge under comparison is termed as the \textit{(edge) candidate content}. Apart of structural matching, the process also involves a comparison between the reference content and the candidate content, employing either a direct string-based comparison comparing the candidate words lemma against the reference word(s).

Edge argument roles are ignored when matching, if they are not specified in a pattern. The matching of argument roles is not elaborated further, as it is of little relevance here.


\paragraph{Example Matchings} To illustrate the expressiveness and of the SHPL and the
functionalities of the differtent language elements, some example matchings of patterns against edges are shown in \cref{tab:shpl-example-matchings}. The corresponding edges and patterns are listed below.

\paragraph{Example Edges}
\begin{enumerate}[label={$e_{\arabic*}$ = }]
	\item \textsf{( plays/P.so ann/C ( acoustic/M guitar/C ) )}
	\item \textsf{( plays/P.so ann/C ( acoustic/M piano/C ) )}
	\item \textsf{( plays/P.so ann/C ( aggressive/M rugby/C ) )}
	\item \textsf{( plays/P.sx ann/C ( if/T ( has/P.so she/C time/C ) ) )}
	\item \textsf{( play/P.so ( and/J ann/C bob/C ) ( a/M song/C ) )}
	\item \textsf{( ( again/M perform/P.so ) they/C ( the/M ( same/M song/C ) ) )}
\end{enumerate}


\paragraph{Example Patterns}
%\begin{multicols}{2}
\begin{enumerate}[label={$p_{\arabic*}$ = }]
	\item \textsf{( plays/P PERS/C THING/C )}
	\item \textsf{( plays/P ann/C * )} 
	\item \textsf{( plays/P ann/C VAR/[CS] )}
	\item \textsf{( plays/P ann/C >[guitar, piano]/C )}
	\item \textsf{( plays/P ann/C >THING/C )}
	\item \textsf{( plays/P ann/C ( >INST/C [guitar, piano] ) )}
	\item \textsf{( ( lemma play/P ) ACTOR/C THING/C )}
	\item \textsf{( ( lemma >[play, perform]/P ) ACTOR/C THING/C )}
\end{enumerate}
%\end{multicols}

%\paragraph{Example Patterns}
%\begin{enumerate}
%	\item \textsf{(plays/P PERS/C INST/C)} utilises variables
%	\item \textsf{(plays/P ann/C *)} utilises wildcard
%	\item \textsf{(plays/P ann/C VAR/[CS])}  utilises variable and type options list
%	\item \textsf{(plays/P ann/C >[guitar, piano]/[CS])} utilises innermost atom, type options list and content options list
%	\item \textsf{(plays/P ann/C >INST/[CS])} utilises innermost variables, innermost atom and type options list
%	\item \textsf{(plays/P ann/C (>INST/[CS] [guitar, piano]))} utilises variable, innermost atom, type options list and content options list
%\end{enumerate}

%\vspace{\baselineskip}
\begin{table}[ht]
\centering
\begin{tabular}{cccl}
\toprule
\textbf{Pattern} & \textbf{Edge} & \textbf{Match?} & \textbf{Variables} \\
\midrule
\(p_1\) & \(e_1\) & \textit{Match} & \textsf{PERS=ann/C, THING=(acoustic/M guitar/C)} \\
\(p_1\) & \(e_4\) & \textit{No Match} & \\
\(p_2\) & \(e_1\), \(e_2\), \(e_3\), \(e_4\) & \textit{Match} & \\
\(p_2\) & \(e_5\), \(e_6\) & \textit{No Match} & \\
\(p_3\) & \(e_1\) & \textit{Match} & \textsf{VAR=(acoustic/M guitar/C)} \\
\(p_3\) & \(e_4\) & \textit{Match} & \textsf{VAR=if/T (has/P.so she/C time/C)} \\
\(p_4\) & \(e_1\), \(e_2\) & \textit{Match} & \\
\(p_4\) & \(e_3\), \(e_4\)  & \textit{No Match} & \\
\(p_5\) & \(e_1\)  & \textit{Match} & \textsf{THING=guitar/C} \\
\(p_5\) & \(e_3\)  & \textit{Match} & \textsf{THING=rugby/C} \\
\(p_5\) & \(e_4\)  & \textit{No Match} & \\
\(p_6\) & \(e_2\)  & \textit{Match} & \textsf{INST=piano/C} \\
\(p_6\) & \(e_3, e_4\)  & \textit{No Match} & \\
\(p_7\) & \(e_1\)  & \textit{Match} & \textsf{ACTOR=ann/C, THING=(acoustic/C guitar/C)}\\
\(p_7\) & \(e_5\)  & \textit{Match} & \textsf{ACTOR=(and/J ann/C bob/C), THING=(the/M song/C)}\\
\(p_7\) & \(e_6\)  & \textit{No Match} & \\
\(p_8\) & \(e_6\)  & \textit{Match} & \textsf{ACTOR=they/C, THING=(the/M (same/M song/C)}\\
\bottomrule
\end{tabular}
\caption{Example matchings of patterns against edges}
\label{tab:shpl-example-matchings}
\vspace{2\baselineskip}
\end{table}



\section{Semantic Similarity Measures}
\label{sec:semantic-similarity}
Determining the \textit{Semantic Relatedness} or (SR) \textit{Semantic Similarity} (SS) of two text items is a central task in the interlinked fields of natural language processing, information retrieval and information extraction. A text item can take on the form of a word, phrase, sentence paragraph or entire document. While SR and SS are often used synonymously, semantic relatedness can also be understood to entail a more nuanced description of the semantic relations of text items \cite{chandrasekaranEvolutionSemanticSimilarity2021, harispeSemanticSimilarityNatural2015}.  In this work, the term \textit{semantic similarity} is therefore used to refer to the task of measuring the closeness, distance or similarity in regard to the meanings conveyed by two text items. This has to be differentiated from lexical (i.e. string-based) similarity measures, which in principle do not rely on the meaning of text items \cite{p.SurveySemanticSimilarity2019}. \todo{mention the STS task specifically (and refer to it later)?}

There exists a great variety of approaches to measuring semantic similarity and their categorisation also takes on different forms in the literature.
%A structure that is in line with most of the reviewed literature employs the following categories: \textit{Knowledge-based Semantic Similarity} (KBSS), \textit{Distributional} or \textit{Corpus-based Semantic Similarity} (CBSS), \textit{Deep Learning-} or \textit{Neural Network-based Semantic Similarity} (NNSS) and hybrid approaches which combine techniques from multiple categories 
A structure that is in line with most of the reviewed literature employs the following categories: \textit{Knowledge-based Semantic Similarity} (KBSS), \textit{Corpus-based Semantic Similarity} (CBSS) and hybrid approaches which combine techniques from the former two categories
\cite{chandrasekaranEvolutionSemanticSimilarity2021, harispeSemanticSimilarityNatural2015, hanSurveyTechniquesApplications2021, zadSurveyDeepLearning2021}.

The focus of this work in assessing the state-of-the-art of semantic similarity measures lies on CBSS -- specifically a subform of corpus-based measures that is denoted here as \textit{Neural Embedding-based Semantic Similarity} (NESS). These measures share some commonality with \textit{Deep Learning-based Semantic Similarity} (DLSS), which is sometimes differentiated as an independent category in the literature.


\subsection{Knowledge-based Semantic Similarity}
Knowledges-based semantic similarity measures rely on some form of underlying structured knowledge source to compute the similarity between two text items. These knowledge sources (referred to as knowledge systems in the following) may be ontologies, knowledge bases, knowledge graphs, semantic graphs, (lexical) databases and similar systems \cite{chandrasekaranEvolutionSemanticSimilarity2021, harispeSemanticSimilarityNatural2015}. Examples of such structured knowledge systems are \textit{ConceptNet} \cite{speerConceptNetOpenMultilingual2017}, \textit{WordNet} \cite{millerWordNetLexicalDatabase1995}, the \textit{Paraphrase Database} (PPDB) \cite{ganitkevitchPPDBParaphraseDatabase2013}, \textit{Wikidata} \cite{vrandecicWikidataFreeCollaborative2014}, \textit{DBpedia} \cite{auerDBpediaNucleusWeb2007} and \textit{Yago} \cite{suchanekYagoCoreSemantic2007}.


Depending on the specific structure of the knowledge system, different methods of computing the semantic similarity are possible. Given two text items, they first need to be mapped to knowledge items -- often called \textit{concepts} -- in the system. Then the similarity of these concepts can be calculated. There exists a variety of methods to calculate the similarity of two concepts, which can be categorised as follows: \textit{structural or path-based}, \textit{feature-based} and \textit{information content (IC)-based}. Hybrid methods which combine multiple approaches also exists \cite{chandrasekaranEvolutionSemanticSimilarity2021, harispeSemanticSimilarityNatural2015}. 


Since most of the aforementioned knowledge systems generally have a graph-like structure, one type of measures is based on the spatial structure of the graph. The calculation of the similarity of two items is therefore mostly based on the length(s) of the path(s) between them. Another type of measure  utilises the features that the two concepts exhibit, where the specific set of features is dependent on the employed knowledge system. A comparison of the two items feature values is the basis for calculating their similarity. IC-based methods rely on the "information provided by the concept when appearing in a context" \cite{sanchezSemanticSimilarityMethod2013}. This is closely related to the specificity of words measured by such metrics as \textit{term frequency - inverse document frequency} (TF-IDF) \cite{aizawaInformationtheoreticPerspectiveTf2003}. Some of these methods determine the information content of concepts by using only information intrinsic to the knowledge system (or multiple knowledge systems). Other methods (additionally) utilize external information such as text corpora, so that the similarity measure could also be categorised as a hybrid of knowledge and corpus-based (see \cref{sec:corpus-based-semantic-similarity}. For a more in-depth exploration of knowledge-based similarity measures confer e.g. \citef{chandrasekaranEvolutionSemanticSimilarity2021, harispeSemanticSimilarityNatural2015, zhuComputingSemanticSimilarity2017, mihalceaCorpusbasedKnowledgebasedMeasures2006}.



\subsection{Corpus-based Semantic Similarity}
\label{sec:corpus-based-semantic-similarity}
Corpus-based semantic similarity measures generally derive the semantics of text items and therefore their similarity not from a structured knowledge source, but from corpora of unstructured text. An assumption that is central to most of the approaches in this category is the \textit{Distributional Hypothesis}, which states that words occurring in the same contexts tend to have similar meanings \cite{harrisDistributionalStructure1954}. Consequently these approaches are also referred to as \textit{Distributional Semantic Similarity Measures} \cite{mohammadDistributionalMeasuresSemantic2012}. Although not all corpus-based semantic similarity measures are based on the distributional hypothesis, it is considered to be the dominant and most relevant type of approaches in this category \cite[Section 2.4]{harispeSemanticSimilarityNatural2015}.

\subsubsection{Vector Space Model}
The Vector Space Model (VSM) \cite{turneyFrequencyMeaningVector2010} is the underlying framework for the text representation used by distributional semantic similarity measures. In this model, vectors represent text items, typically words, and are designed to encapsulate the distribution of words within a corpus. This generally involves counting word occurrences within particular contexts and organising these counts into a matrix. If the context is defined as a document (assuming the corpus is structured into distinct documents), the matrix is known as a \textit{term-document} or \textit{word-document frequency matrix}. Here, rows represent words and columns signify documents, with each cell indicating the frequency of a word's occurrence in a document. The context may also be defined more broadly as e.g. paragraphs, sentences, phrases, or a text window of arbitrary size (around another word). The entries in the resulting \textit{word-context frequency matrix} can be weighted to reflect their specificity to the context, using measures such as TF-IDF or \textit{(Positive) Point-wise Mutual Information} \cite{churchWordAssociationNorms1989, niwaCoOccurrenceVectorsCorpora1994}. Due to the potentially large vocabulary size and high number of context items, the word-context frequency matrix -- and consequently the derived word vectors -- may be high dimensional and sparse. It has been demonstrated that semantic similarity measurements benefit from techniques that smooth the matrix and therefore reduce the dimensionality of the vectors \cite{deerwesterIndexingLatentSemantic1990}. A commonly employed method for this purpose is \textit{Singular Value Decomposition} (SVD), or specifically its variant \textit{truncated SVD}. Though other approaches, such as \textit{Nonnegative Matrix Factorisation} \cite{leeLearningPartsObjects1999} or filtering of low entropy dimensions \cite{lundProducingHighdimensionalSemantic1996} have also been explored. A principle limitation of these approaches is, that they treat the context as a \textit{bag of words} -- ignoring word order beyond word proximity.


\subsubsection{Vector Similarity Measures}
In assessing the similarity between two vectors within the VSM, a variety of measures can be utilised to quantify their relationship. Spatial similarity measures treat the VSM as a geometric construct and determine a spatial relation of the vectors. The most notable examples include the Manhattan distance (L1 norm), the Euclidean distance (L2 norm), and cosine similarity. Alternative measures are mostly motivated by an information theory or probabilistic perspective on the VSM. Relevant examples among these are the Kullback-Leibler divergence and the Jensen-Shannon divergence \cite{mohammadDistributionalMeasuresSemantic2012}. It shall be noted, that a probabilistic interpretation entails constraints for the characteristics of the VSM. The choice of a suitable vector similarity measure is generally dependent upon the specific construction of the vectors in use. Cosine similarity, in particular, has been highlighted for its effectiveness \cite{mohammadDistributionalMeasuresProxies2012, turneyFrequencyMeaningVector2010}. 

\subsubsection{Spatial VSM-based Semantic Similarity Measures}
This work focusses on spatially interpreted VSMs and their associated similarity measures, since they can be seen as a precursor to the neural embedding-based similarity measures, which are outlined and further elaborated on in \cref{sec:neural-embedding-semantic-similarity}.
In the following some distributional semantic similarity measures are presented, which were relevant for the development of the field. For each approach, we note the definition of the context for the word frequency matrix and the matrix' weighting and smoothing  (i.e. dimensionality reduction) procedure. 
%All of the presented methods utilise a spatial measure to determine vector similarity in the constructed VSM.

\textit{Latent Semantic Analysis} (LSA) \cite{deerwesterIndexingLatentSemantic1990, landauerSolutionPlatoProblem1997, landauerIntroductionLatentSemantic1998}: In LSA, the context for the frequency matrix is a paragraphs. The matrix undergoes dimensionality reduction through a variant of Singular Value Decomposition (SVD), which reduces columns but retains rows, aiming to preserve the similarity structure among words. The similarity between word vectors, represented by their row values, is calculated using cosine similarity.

\textit{Hyperspace Analogue to Language} (HAL) \cite{lundProducingHighdimensionalSemantic1996}: HAL employs a sliding window to define its context, creating a word co-occurrence matrix with association strength values that depend on word proximity. Dimensionality reduction is achieved by excluding columns with low entropy. The vector similarity measure used is either Euclidean or Manhattan distance.

\textit{Correlated Occurrence Analogue to Lexical Semantics} (COALS) \cite{rohde2006improved}: COALS is a improved version of HAL and also uses a sliding window for its context. The matrix undergoes correlation normalisation and possibly SVD for smoothing and dimensionality reduction. Cosine similarity is used as vector similarity measure.

\textit{Explicit Semantic Analysis} (ESA) \cite{gabrilovich2007computing}: ESA's frequency matrix context are Wikipedia articles, also called concepts. Weighting of the word-concept matrix is performed using TF-IDF. Semantic similarity is measured using cosine similarity between weighted concept vectors. The reliance on Wikipedia structure arguably renders ESA a hybrid combination of corpus- and knowledge-based approaches.


\section{Neural Embedding-based Semantic Similarity}
\label{sec:neural-embedding-semantic-similarity}
In natural language processing, the term \textit{embedding} was first used in the context of Latent Semantic Analysis \cite{deerwesterIndexingLatentSemantic1990} to describe a vector representation of text. Since then, \textit{text embedding} has become widely used in NLP to describe a dense, fixed-length vector representations of text items, which can be interpreted spatially to measure semantic relations between them \cite{almeidaWordEmbeddingsSurvey2023}.

Strictly speaking, \textit{neural embeddings} of text refer to representations that have been produced by a neural network model. The development of these approaches is deeply interlinked with the advent of \textit{Neural (Network) Language Modelling} (NNLM). The task of \textit{language modelling} traditionally is to predict the most probable next word given a sequence of previous words \cite{chenEmpiricalStudySmoothing1999}. While \citef{bengioNeuralProbabilisticLanguage2000} are commonly referred to as the first to use a neural network for language modelling, \citef{collobertUnifiedArchitectureNatural2008} are seen to be the first to build a NNLM with the focus on producing embeddings. 
%Apart from the neural architecture, a key development in the field of NLP, contributing to state-of-the-art performance achieved by neural embeddings, was the increased scale of the training process and especially the amount of training data \cite{minaeeDeepLearningBased2021}.

\textit{Neural Embedding-based Semantic Similarity Measures} (NESS) in this work refer to methods that compute the semantic similarity of text items based on a spatial measurement between their respective neural embeddings. NESS measures can be considered as a subform of CBSS, since they also fundamentally rely on the distributional hypothesis and employ the same general approach of deriving distributional semantics from a text corpus. Here, neural embedding-based approaches are further differentiated into \textit{Fixed Word Embedding-based Semantic Similarity} (FNESS) measures and \textit{Contextual Embedding-based Semantic Similarity} (CNESS) measures, which is elaborated in the following. \todo{mention training data sets (and sizes?)}


\subsubsection{Fixed Word Embeddings vs. Contextual Embeddings}
\label{sec:fixed-word-vs-contextual-embeddings}
In this work, \textit{(fixed) word embeddings} refer to neural text embeddings produced by models  that are limited to single words. For a or given word, such a model will always generate the same embedding, hence the specification \textit{fixed} (or \textit{static}) word embedding. A significant consequence of this limitation is, that these word embedding models are not able to differentiate \textit{homonyms}\footnote{\textit{homonym}: "A word that both sounds and is spelled the same as another word"\\(\url{https://en.wiktionary.org/wiki/homonym}} or more specifically \textit{homographs}\footnote{\textit{homograph}: "A word that is spelled the same as another word [...]"\\(\url{https://en.wiktionary.org/wiki/homograph})} \cite{liWordEmbeddingUnderstanding2018}. These differentiations are only possible by taking the context in which the word appears into consideration. The context may be a phrase, sentence, paragraph or any sequence of tokens. A token can be word, but also the result of another kind of segmentation of the input text. While a sequence of words is generally processable using a fixed word embedding model, this results in a sequence of independently generated word embeddings. In this work, \textit{contextual embedding} models are understood to incorporate the context. That means the generation of an embedding for a token contained in context is influenced by the other tokens in the sequence. This may result in different token embeddings for the same token given different contexts. Therefore contextual embeddings are principally able to represent different meanings of homographs and may also differentiate more subtle variations of word meaning, that can be derived from context \cite{liuSurveyContextualEmbeddings2020}.


\subsubsection{Deep Learning-based Semantic Similarity}
\label{sec:deep-learning-based-semantic-similarity}
\textit{Deep Learning-based Semantic Similarity Measures} (DLSS) are often treated as a distinct category in the literature \cite{harispeSemanticSimilarityNatural2015, chandrasekaranEvolutionSemanticSimilarity2021, hanSurveyTechniquesApplications2021, zadSurveyDeepLearning2021}. In this work, DLSS is not seen as a separate category, instead it is subsumed by the category of NESS. Deep learning-based approaches employ \textit{Deep Neural Network} (DNN) models, which have multiple hidden layers, allowing them in principle to learn hierarchical of representations of the input data \cite{goodfellowDeepLearning2016}. Given a text item as input, the state of a specific hidden layer in the trained network after an inference pass is generally considered to be the embedding of the text item. From this perspective, all DLSS measures can be seen as neural embedding-based. 

However, there also exists a variety of deep learning-based approaches, which use models that directly predict the semantic similarity between two input text items in the output layer. Here, the embeddings are only intermediate representations, but not explicitly used to measure spatial relations between them and they might not be suited for that. These methods are not in the focus of this work, for such approaches confer e.g. \citef{taiImprovedSemanticRepresentations2015, hePairwiseWordInteraction2016, shaoHCTISemEval2017Task2017, tianECNUSemEval2017Task2017, wangSentenceSimilarityLearning2017, lopez-gazpioWordNgramAttention2019, tienSentenceModelingMultiple2019, tienSentenceModelingMultiple2019a, zhengDetectionMedicalText2019}.

Following the fixed word embeddings vs. contextual embedding distinction outlined above, DLSS approaches correlate strongly with the contextual embedding-based approaches, which is elaborated on in \cref{sec:contextual-embedding-semantic-similarity}.

%The relevant distinction for this work is that DLSS generally allow for similarity measurements of text items larger than words, i.e. phrases, sentences, paragraphs or entire documents. The utilised neural networks are able to process and represent longer text items as sequences of tokens, which may be words, but can also be the result of a different form of text segmentation. Typically an embedding of each token exists at 
%
%While there exist non-deep semantic similarity methods which allow for that, e.g. by comparing document (column) vectors instead of word vectors when utilising LSA, these methods most often treat the text as a \textit{bag of words} -- ignoring word order. Resolving this limitation is a core feature of most deep learning-based techniques, since the utilised neural networks are able to process and represent longer text items as sequences of words (or tokens). Differentiating by this distinctive feature, the field of neural embedding-based semantic similarity is further elaborated on in the following.

\subsection{Fixed Word Embedding-based Semantic Similarity}
\label{sec:fixed-word-embedding-semantic-similairity}
\textit{Fixed Word Embedding-based Semantic Similarity} (FNESS) measures in this work denote semantic similarity measures that employ fixed word embedding models. Here, the concept of \textit{neural embeddings} extends to encompass not only those embeddings directly derived from neural models, but also includes embedding generation approaches that incorporate elements of neural model methodologies and thereby achieving performances that rival strictly neural embeddings. This broader interpretation aligns with perspectives found in existing literature \cite{zucconIntegratingEvaluatingNeural2015, sezererSurveyNeuralWord2021}. Fixed word embedding models are generally based on non-deep machine learning methods and can be further divided into \textit{prediction-based}, \textit{count-based} and \textit{hybrid} or \textit{meta} models. In the following, some of the most relevant word embeddings models are presented. 

\subsubsection{Prediction-based Models} 
These mdoels are based on the concept of language modelling and work with local context windows. They are trained on predicting a target word (or token) given a context window of words (or tokens) or the other way around.

\textit{Word2Vec} \cite{mikolovDistributedRepresentationsWords2013, mikolovLinguisticRegularitiesContinuous2013, mikolovEfficientEstimationWord2013}: Word2Vec has two forms; the \textit{Continuous Bag of Words} (CBOW) model and the skip \textit{Skip-Gram} (SG) model. CBOW predicts the target word given a context and SG predicts each context word given a target word. Both models are log-(bi)linear models, which basically learn a logistic regression with one hidden layer, whose weights are then used as embeddings. The best performing variant of Word2Vec is \textit{Skip-Gram with Negative Sampling} (SGNS), which reportedly produced state-of-the-art results.

\textit{fasttext} \cite{bojanowskiEnrichingWordVectors2017}: fasttext an extension of Word2Vec, which is reported to outperform SGNS. The central modification is, that the model works on subword tokens, which helps it to generalise to word which are unknown from the training data.

\subsubsection{Count-based Models}
These models are no neural embedding models in a strict sense, since they do not employ a neural network architecture. In terms of approach, the fall in line with spatially interpreted VSMs based on constructing a word-context frequency matrix (and are therefore fundamentally based on word counts) \cite{almeidaWordEmbeddingsSurvey2023, turneyFrequencyMeaningVector2010}, which are discussed in \cref{sec:corpus-based-semantic-similarity}. In distinction to these approaches, the methods presented here also incorporate local context window information inspired by NNLM approaches and perform competitively when compared with strictly neural embeddings.

\textit{GloVe} \cite{penningtonGloVeGlobalVectors2014} GloVe leverages corpus-wide, hence \textit{global}, word co-ocurrences represented as word-word frequency matrix populated with probability ratios. The word co-occurrence probability ratios are weighted based on the distance of the word pairs in the context window from which they are extracted. It reportedly outperforms SGNS, given the same training corpus, training time and context window size.

\textit{LexVec} \cite{salleMatrixFactorizationUsing2016, salleEnhancingLexVecDistributed2016, salleWhyRoleNegative2019}: LexVec is based on the low-rank, weighted factorisation of the positive point-wise mutual information matrix via stochastic gradient descent. It incorporates local context window information by applying a weighting scheme to the matrix that has been shown to be implicitly performed by SGNS \cite{levyNeuralWordEmbedding2014}. There also exists a subword based variant of LexVec, which is reported to perform competitively compared to fasttext \cite{salleIncorporatingSubwordInformation2018}.


\subsubsection{Hybrid and Meta Models}
\textit{Hybrid word embedding} models in this work refer to approaches which are not only trained on unstructured text corpora, but also incorporate some form of structured knowledge source. This is akin to the ESA method presented in \cref{sec:corpus-based-semantic-similarity}. \textit{Meta word embedding} models merge the information of multiple existing word embedding models into a new embedding model \cite{bollegalaSurveyWordMetaEmbedding2022}. Both approaches are applicable simultaneously, resulting in a \textit{hybrid meta word embedding} model.

%do not refer to combinations prediction-based and count-based approaches, considering that this property is arguably already fulfilled by the count-based methods presented above.

\textit{Conceptnet Numberbatch} \cite{speerConceptNetOpenMultilingual2018, speerConceptNetOpenMultilingual2017}: Conceptnet Numberbatch is hybrid meta model that integrates structured knowledge from ConceptNet with existing word embedding models. First, ConceptNet-specific word embeddings are constructed by representing the knowledge graph's structure as a matrix of term-term connections, with cell values reflecting the sum of connecting edge weights. Further processing (i.a. smoothing and dimensionality reduction) is applied to this matrix and the resulting word embeddings are merged with Word2Vec, GloVe and fasttext embeddings. The method produced state-of-the-art-result, reportedly outperforming LexVec and all subsumed embedding models. A third-party comparative analysis of word embedding-based semantic similarity involving Word2Vec, fasttext, GloVe, LexVec and Conceptnet Numberbatch found, that the latter achieved best performance, when evaluating word similarity correlation with a ground truth \cite{toshevskaComparativeAnalysisWord2020}.

\textit{MetaVec} \cite{garcia-ferreroBenchmarkingMetaembeddingsWhat2021}: MetaVec is in principle a pure meta word embedding model which integrates fasttext, ConceptNet Numberbatch, \textit{JOINTChyb} \cite{goikoetxeaBilingualEmbeddingsRandom2018a} and \textit{Paragram} \cite{wietingParaphraseDatabaseCompositional2015a}. However, JOINTChyb leverages data from WordNet, Paragram utilises the PPDB and Conceptnet Numberbatch relies on ConceptNet as described above. Consequently MetaVec is highly influenced by structured knowledge as well and can therefore also be considered to be a hybrid meta word embedding model. It is reported to outperform ConceptNet Numberbatch in multiple evaluations and in this work is seen to be the current state-of-the-art of fixed word embedding models for measuring semantic similarity.



%---
%all methods use 300-dimensional embeddings

\subsection{Contextual Embedding-based Semantic Similarity Measures}
\label{sec:contextual-embedding-semantic-similarity}
\textit{Contextual Embedding based Semantic Similarity} (CNSS) measures in this work denote semantic similarity measures, which employ contextual embeddings models. These models need to be able to jointly process sequences of tokens and produce the token embeddings interdependently (see \cref{sec:fixed-word-vs-contextual-embeddings}). Contextual embedding models are generally based on deep neural networks, which employ different architectural approaches to capture patterns in processing compositional data structures such as a token sequences \cite{goodfellowDeepLearning2016}. A wide array of DNN architectures has been applied to natural language processing and the generation of contextual embeddings specifically. The evolution of the current state-of-the-art of contextual embedding models is delineated here by focussing on three of the most relevant architecture types: \textit{Convolutional Neural Networks} (CNN), \textit{Recursive Neural Networks} (RNN) and \textit{Transformer}-based models. Additionally some pivotal architecture-agnostic methodologies are introduced. Not all approaches presented here aim at producing embeddings in the spatially interpretable sense outlined above, but are still mentioned if they were highly influential on the development of the field. For a more comprehensive overview of deep learning based approaches to NLP, confer e.g. \citef{youngRecentTrendsDeep2018, minaeeDeepLearningBased2021, minRecentAdvancesNatural2023}

\paragraph{Transfer Learning and Pre-training} \textit{Transfer learning} involves leveraging the representations learned from a different yet related task to train a model for a specific task \cite{zhuangComprehensiveSurveyTransfer2021}. When these representations are derived from language modelling, the process is generally termed \textit{pre-training} in NLP, notably advanced by \citef{daiSemisupervisedSequenceLearning2015} for the generation of contextual embeddings.

\paragraph{Encoder-Decoder Design}
The \textit{encoder-decoder} configuration is model design that can be applied to multiple DNN architecture types. It consists of two major model blocks: an \textit{encoder} and a \textit{decoder}. The encoder first processes the input data (e.g., token sequence in the source language) to generate a  intermediate representation of fixed size, which is then passed to the decoder to generates the output data (e.g., translation in the target language). This model design has been proven very successful for sequence-to-sequence tasks such as machine translation and speech recognition, especially when the length of the input sequence and output sequence may differ \cite{choPropertiesNeuralMachine2014}.

\paragraph{Attention mechanisms}
Attention mechanisms are another important innovation in the processing of sequences, which help a model to focus on more relevant parts of the input sequence when producing an output sequence \cite{galassiAttentionNaturalLanguage2021}. In successfully applying an attention mechanism to a machine translation task, the work of \citef{bahdanauNeuralMachineTranslation2016}  is a significant contribution in demonstrating the efficacy of this technique mechanism for NLP.


\subsubsection{CNN and RNN Models}
Convolutional neural networks extract patterns from compositional data structures through their convolutional layers or filters, adept at identifying spatial patterns, rendering them particularly effective for tasks such as image processing, but also natural language processing \cite{albawiUnderstandingConvolutionalNeural2017}. Recurrent Neural Networks are tailored for sequential data processing, with an architecture that allows preceding input elements to influence the processing of subsequent ones. This principally positions them well for handling token sequences -- crucial in generating contextual embeddings \cite{yuReviewRecurrentNeural2019}.

\citef{collobertUnifiedArchitectureNatural2008} are credited with being among the pioneers in employing CNNs for generating sentence embeddings. This approach has been further expanded by e.g.  \citef{kalchbrennerConvolutionalNeuralNetwork2014}. A significant contribution to the use of RNNs for language modelling was made by \citef{mikolovRecurrentNeuralNetwork2010}, demonstrating promising results a in speech recognition task.

%\cite{collobertNaturalLanguageProcessing2011} (unified architecture?)
%--> self supervised language modelling training objective

\paragraph{LSTM Models}
Within the spectrum of RNNs, \textit{Long-Short Term Memory} (LSTM) models are probably the most relevant variant. These models are engineered to overcome the limitations RNNs face with long-term dependencies in input sequences \cite{hochreiterLongShortTermMemory1997, yuReviewRecurrentNeural2019}. A notable development in the use of deep LSTM models for NLP was recorded in \citef{gravesSpeechRecognitionDeep2013}, where they reported state-of-the-art results in a speech recognition task.


%\vspace{\baselineskip} 
In the following, some of the most relevant methods of producing contextual embedding by employing
%encoder-decoder
LSTMs in combination with other aforementioned techniques are presented.

\textit{context2vec} \cite{melamudContext2vecLearningGeneric2016}: This method employs a bidirectional LSTM model to generate contextual embeddings, outperforming methods that rely on average pooling of word embeddings.

\textit{Context Vectors} (CoVe) \cite{mccannLearnedTranslationContextualized2018}: CoVe leverages a bidirectional LSTM in combination with a form of attention mechanism to generate context embeddings by training on a machine translations task, thereby applying transfer learning. A concatenation of the CoVe model output and GloVe word embeddings is used as input for evaluation tasks, reportedly producing state-of-the-art-results.

\textit{Embeddings from Language Models} (ELMo) \cite{petersDeepContextualizedWord2018}: ELMo methods produces context-dependent embeddings based on pre-training a bidirectional language model implemented in form of a bidirectional LSTM. Similar to the CoVe, the ELMo embeddings are concatenated with word embeddings to form the input for downstream tasks, for which task-specific networks are augmented by convolutional or attention layers. This approach is reported to improve the state-of-the-art in six of the evaluation tasks.

%\textit{ULMFiT} \cite{howardUniversalLanguageModel2018}


\subsubsection{Transformer-based Models}
The \textit{Transformer} architecture was first presented by \cite{vaswaniAttentionAllYou2017}, which was reported to significantly improve the state-of-the-art for a  machine translation task. A key component is a specific form of the attention mechanism -- called \textit{self-attention} -- that is used to weigh the importance of elements for each other when processing a sequence. Based in its application for machine translation, the original Transformer model follows the encoder-decoder design. Unlike recurrent networks, which process sequences sequentially, transformer-based models handle entire sequences in parallel. This parallel processing capability facilitates pre-training these models on larger datasets and therefore effectively scaling them to larger model sizes, which is central to the performance gains on a multitude of NLP task, that are associated with their use \cite{minRecentAdvancesNatural2023}. In the following, some of the approaches most relevant for the evolution of Transformer-based contextual embeddings models are presented.

\todo{add USE?}
%\textit{Universal Sentence Encoder} (USE) \cite{cerUniversalSentenceEncoder2018}
%transformer and deep averaging network (FFNN)

\textit{Bidirectional Encoder Representations from Text} (BERT) \cite{devlinBERTPretrainingDeep2019}: BERT introduces a novel training objective in the form of masked language modelling. In this approach, certain tokens within an input sequence are obscured at random, with the goal of predicting these hidden tokens. Additionally, it is trained on a next-sentence prediction, which involves taking two separate sentences and determining whether the second sentence follows the first. Utilizing a Transformer encoder, BERT processes the input in a bi-directional manner during its pre-training phase. The method obtains new state-of-the-art results on eleven NLP tasks. Many variants of the approach have emerged, such as \textit{RoBERTa} \cite{liuRoBERTaRobustlyOptimized2019}, which offers substantial performance enhancements over BERT through modification of the pre-training regime.


%\textit{Text-to-Text Transfer Transformer} (T5) \cite{raffelExploringLimitsTransfer2020}
%(GPT)

%\paragraph{Semantic Textual Similarity Task}
%formalisation of determining the semantic similarity of texts
%not transformer specific but hey

%contrastive loss and NLI efficient for sentence embeddings: InferSent
%\cite{conneauSupervisedLearningUniversal2017}


\paragraph{Embedding-specific Models} While BERT and RoBERTa achieved new state-of-the-art performances on semantic similarity tasks, these models require that the input contains both sequences whose similarity should be determined. A similarity measurement is then produced by the output layer of the model, but the immediately generated embeddings are not well suited as input for spatial measures such as cosine similarity \cite{reimersSentenceBERTSentenceEmbeddings2019}. This is hypothesised to be a consequence of anisotropic embedding spaces resulting from the masked language modelling objective \cite{liSentenceEmbeddingsPretrained2020}. Some of the most relevant approaches in resolving this limitation are presented in the following.


\textit{Sentence-BERT} (SBERT) \cite{reimersSentenceBERTSentenceEmbeddings2019}: SBERT utilises supervised fine-tuning on the foundational models BERT and RoBERTa through the use of siamese and triplet architectures. It leverages \textit{Natural Language Inference} (NLI) datasets, which consist of sentence pairs categorised into entailment, contradiction, or neutrality towards one another, as a basis for training with classification, regression, and triplet loss objectives. This approach has demonstrated superior performance over existing methods in generating sentence embeddings, regarding all evaluated benchmarks for semantic similarity. \todo{refer to USE, if added}

\textit{Simple Contrastive Sentence Embedding} (SimCSE) \cite{gaoSimCSESimpleContrastive2021}:
 SimCSE builds on BERT and RoBERTa models, employing supervised and unsupervised techniques for sentence embedding. Utilizing an unsupervised strategy, SimCSE trains on sentence duplication using dropout to introduce noise. Its supervised method incorporates NLI data alongside a contrastive loss function, thereby producing state-of-the art results, surpassing SBERT in all evaluated semantic similarity benchmarks. Contrastive learning aims to learn representations by contrasting positive examples (similar or related data points) against negative examples (dissimilar or unrelated data points). The contrastive loss approach is hypothesised to create a more isotropic space for embeddings.

\paragraph{Massive Text Embedding Benchmark}
Since the field of contextual embeddings is rapidly evolving, it is not trivial to asses the current state-of-the-art and pinpoint the best performing model available for semantic similarity measurements.
The \textit{Massive Text Embedding Benchmark} (MTEB) \cite{muennighoffMTEBMassiveText2023} offers support for this task. It is comprised of eight evaluation task, involving 58 datasets and 112 languages, including ten datasets for the semantic similarity task. An online leaderboard showcases the performance of all models that have been benchmarked by \citef{muennighoffMTEBMassiveText2023} and of those that have been submitted by third-parties.\footnote{MTEB Leaderboard: \url{https://huggingface.co/spaces/mteb/leaderboard} (Accessed on Oct 9, 2023)}. In the following, two of the best performing models according to the online MTEB leaderboard at the time of access are presented.

\textit{EmbEddings from bidirEctional Encoder rEpresentations} (E5) \cite{wangTextEmbeddingsWeaklySupervised2022}:
E5 uses the same architecture as BERT and also initialises the model with the weights obtained from BERT's pre-training. This is followed by training using a contrastive learning approach with weak supervision signals derived from a large curated dataset of text pairs. The E5 model was developed in three variants: \textit{small}, \textit{base} and \textit{large}, whose reported performance increases with size. There also exists a second version of the model (E5-v2), wich was trained on a larger quantity and more diverse text pair datasets. \footnote{See \url{https://huggingface.co/intfloat/e5-base-v2/discussions/1}} E5-v2 models outperform the corresponding original models on most benchmarks, while performing on par in the semantic similarity task.

\textit{General Text Embeddings} (GTE) \cite{liGeneralTextEmbeddings2023}: Falling in line with with SimCSE and E5 in terms of approach, GTE also uses the BERT architecture and initialisation followed by contrastive learning process. Here it consists of a first step of contrastive pre-training with unlabelled data and a second step of contrastive fine-tuning with labelled task-specific data. GTE is also developed in three different model sizes -- named \textit{small}, \textit{base} and \textit{large} -- which reportedly correlate with performance increases. Both, GTE and E5, construct the final sentence embedding by average pooling the produced token embeddings. Their publicly available implementation also share the same software interface. At the time of access, \textit{GTE-large} is the top performing model regarding the average of all benchmarks in the MTEB and the average of the semantic similarity benchmarks specifically. It is therefore seen in this work to be the current state-of-the-art regarding contextual embeddings models for measuring semantic similarity.

%\subsubsection{Tokenization}
%Input text must be first converted to tokens
%Some models tokenise as words and use word embeddings as input features
%different tokenizers -> wordpiece, sentencepiece, tiktoken
%--> are these all BPE in the end? eegaaaal
%

%\section{Hybrid Measures of Semantic Similarity}


%\subsubsection{Distance Measures}
%Mean reference vector vs. pairwise distance
%
%similarity threshold (ST)

% ========== 
\chapter{Problem Statement}
\label{cha:problem-statement}
Computational social science (CSS) researches may typically be interested in extracting statements of a specific kind from a text corpus, such as expressions of sentiment of an actor towards some entity or expressions of conflicts between different actors. Two sensible ways to frame this task are as \textit{text classification} \cite{kowsariTextClassificationAlgorithms2019} or \textit{text retrieval} \cite{manningIntroductionInformationRetrieval2008}. It can be addressed with a wide range of system, which will be generally referred to as  \textit{automatic text analysis} systems in the following. These systems are mostly based on techniques from the field of natural language processing, as well as the interlinked fields of information retrieval and information extraction \cite{chowdharyNaturalLanguageProcessing2020}.

A relevant perspective of categorising text analysis systems, especially from the point of view of CSS researchers, are the dimensions \textit{open}-\textit{opaque} and \textit{adaptive}-\textit{strict} \cite{menezesSemanticHypergraphs2021}. Here openness refers to the systems users ability to inspect and understand the processing, which we can also bee describes as transparency and explainability. These properties are of high interest in CSS applications such as predicting political opinion based on social media activity \cite{wilkersonLargeScaleComputerizedText2017}. An adaptive text analysis system does not (only) operate on strict rules, but is able to learn and modify its behaviour in some way. It is therefore in principle able to handle unforeseen variations in the text it processes. While both of these two properties are desirable for users are often found to be a trade-off. Current successful adaptive systems are most often bases on neural networks \cite{hirschbergAdvancesNaturalLanguage2015}, which are opaque in the way how they represent and process text \cite{rudinStopExplainingBlack2019}.

The Semantic Hypergraph (SH) framework (see \cref{sec:semantic-hypergraph-framework}) aims to fulfil both the open as well as the adaptive property of a text analysis system. It offers an inspectable and understandable representation of text that is constructed by a parser based on machine learning components. The SH representation and its construction can be therefore considered to fulfil the open-adaptive properties. The SH pattern matching language can be used to define patterns that match a specific subset of hyperedges in a given hypergraph. The matching process (described in \cref{sec:pattern-matching-process}) is purely symbolic and follows a set of fixed rules. It can therefore be considered to be open-strict. In the context of the SH framework the CSS research task described above is better framed as text retrieval. The SH pattern acts  as a \textit{query} for which the most relevant items are retrieved. While the SH frameworks capabilities are not restricted to text retrieval, the work is focused on this application.

The SH pattern defined by a user may among other things specify the structure of the edges that should match it as well as their type (and the types of possible sub-edges). The SH pattern language allows it to describe different levels of generalisations for the structural matching. Additionally the actual words that should match need to be specified i.e. the reference content against which the edge content is matched, if the edge matches the pattern structurally. In the original form of the SH framework, the only way of generalising content matching is via the lemma functional pattern, but there is no possibility to define semantically more sophisticated generalisations.

This limitation in content matching generalisation capability means that all acceptable variants of edge content -- i.e. specific words -- must be explicitly provided by the SH framework's user. This also entails that a bias is introduced into the matching process due to the manual selection of words by the user defining a pattern. To better illustrate the problem, we consider the following sentences represented as hyperedges:

\begin{hedge}[h!]
  \normalfont\sffamily
  \centering
  ( plays/P ann/C piano/C ) 
  \caption{Represents the sentence "Ann plays piano"}
  \label{hed:ann-plays-piano}
\end{hedge}

\begin{hedge}[h!]
  \normalfont\sffamily
  \centering
  ( plays/P ann/C theatre/C ) 
  \caption{Represents the sentence "Ann plays theatre"}
  \label{hed:ann-plays-theatre}
\end{hedge}
  


\Cref{hed:ann-plays-piano} and \cref{hed:ann-plays-theatre} both follow the same structure, but differ in the content of the last sub-edge. Both edges are hence matched by \cref{pat:ann-plays-something}, which does not restrict content for this sub-edge. The SH pattern language also allows to define a pattern that matches both edges via a list of words as in \cref{pat:ann-plays-piano-theatre}. 

\begin{pattern}[h!]
  \normalfont\sffamily
  \centering
  ( play/P ann/C */C )
  \caption{"Ann plays something" pattern}
  \label{pat:ann-plays-something}
\end{pattern}

\begin{pattern}[h!]
  \normalfont\sffamily
  \centering
  ( likes/P ann/C [piano, theatre]/C )
  \caption{"Ann plays piano or theatre" pattern}
  \label{pat:ann-plays-piano-theatre}
\end{pattern}

Assuming we want to retrieve hyperedges that represent sentences which express something along the lines of "Ann engages in an artistic activity", we would probably also want to match \cref{hed:ann-plays-theatre}. However, to do so we need to extend the list of words in \cref{pat:ann-plays-piano-theatre} by "ballad" or resort to the more general \cref{pat:ann-plays-something}. Although this pattern also matches \cref{hed:ann-plays-dead}, which is probably not what we aim for in meaning. 

\begin{hedge}[h!]
  \normalfont\sffamily
  \centering
  ( plays/P ann/C ( a/M ballad/C ) ) 
  \caption{Represents the sentence "Ann plays a ballad"}
  \label{hed:ann-plays-ballad}
\end{hedge}

\begin{hedge}[h!]
  \normalfont\sffamily
  \centering
   ( plays/P ann/C dead/C ) )
  \caption{Represents the sentence "Ann plays dead"}
  \label{hed:ann-plays-dead}
\end{hedge}

%
%However is not possible define a pattern that matches based on some form of semantic similarity regarding content.
%Referring to the example above this means using the SH framework it is not directly possible to to retrieve every sentences that declares that "Ann likes \textit{some kind of fruit}" or that "Ann likes \textit{fruits similar to apples}". This former would require to provide a comprehensive list of every fruit while the latter would require the user to specify all fruits he deems similar to apples.

Utilizing some form of semantic similarity in regard to edge content in the SH matching process would allow the SH framework's users to define patterns, which describe generalisations of edge content. In the example outlined above, an option to include words that are semantically similar to "piano" and "theatre", would remove the need for the user to include all acceptable words that describe artistic activities in the pattern. 

There exists a great variety of approaches for determining the semantic similarity of text as outlined in \cref{sec:semantic-similarity}. Fixed neural embedding-based semantic similarity measures -- specifically hybrid models, which integrate structural knowledge in their principally corpus-based approach -- have been identified in this work as one of the best performing methods to determine the similarity of single words (see \cref{sec:fixed-word-embedding-semantic-similairity}).
\todo{should i "prove" this more specifically?}
An in integration of FNESS into the SH pattern matching could alleviate the problem of lacking content generalisation as we have characterised it so far.

%which can generally be divided into \textit{Corpus-based Measures} and \textit{Knowledge-based measures} \cite[Section~1.3.2]{harispeSemanticSimilarityNatural2015}. The latter approaches may generally provide the openness in the measurement determination that is desired by CSS researchers. However among the former recent ML-based and especially DL-based approaches have been outperforming most other approaches \cite{chandrasekaranEvolutionSemanticSimilarity2021}. They generally rely on computing a vector space representation (or embedding) of texts which can then be used to calculate their similarity and will therefore be referred to as neural embedding-based semantic similarity (NESS) measures.


To retrieve all sentences that convey the meaning we are interested in, we probably also want to the query pattern to match hyperedges that are more structurally diverse than those, which we have inspected above. \Cref{hed:ann-plays-drama} is such an example, which would be matched by the structurally more complex \cref{pat:ann-plays-something-complex}. However, this pattern would also match \cref{hed:ann-plays-company}, which again is probably not what we want to express. 

\begin{hedge}[h!]
  \normalfont\sffamily
  \centering
 ( plays/P ann/C ( the/M ( main/M character/C ) ) ( in/T ( a/M drama/C ) ) ) )
  \caption{Represents the sentence "Ann plays the main character in a drama"}
  \label{hed:ann-plays-drama}
\end{hedge}

\begin{pattern}[h!]
  \normalfont\sffamily
  \centering
  ( plays/P ann/C */C */S)
  \caption{Structurally more complex "Ann plays something" pattern}
  \label{pat:ann-plays-something-complex}
\end{pattern}
  
\begin{hedge}[h!]
  \normalfont\sffamily
  \centering
( plays/P ann/C ( an/M ( important/M role/C ) ( for/T ( her/M company/C ) ) )
  \caption{Represents the sentence "Ann plays an important role for her company"}
  \label{hed:ann-plays-company}
\end{hedge}

Given more complex hyperedge structures, it becomes increasingly difficult to construct patterns that may match all of the desired edges, especially if this set of edges is strongly defined by edge content. In this example we are interested in an interpretation of "play" that somehow relates to artistic activity. Since word semantics generally depend on textual context, so does the semantic similarity between them \cite[Section 2.2.3]{harispeSemanticSimilarityNatural2015}. An option to leverage some form of context sensitive semantic similarity for edge content in the SH pattern matching, would allow the user to express this form of context-specific content generalisations. From a user perspective this would involve providing content references in the form of larger text items such as sentences. Considering our example, this could be realised by defining the sentences represented by \cref{hed:ann-plays-piano}, \cref{hed:ann-plays-theatre}, \cref{hed:ann-plays-ballad} and \cref{hed:ann-plays-drama} as reference content.

%Incorporating contextuality when extending the SH pattern matching process by SS therefore poses a central challenge. Context-dependent SS would allow to specify matching edge content beyond isolated word semantics, although this may not always be desirable or necessary as in the example above. 

Contextual semantic similarity measures generally aim to determine the SS between larger text items such as phrases, sentences or paragraphs \cite{vermaSemanticSimilarityShort2020, zadSurveyDeepLearning2021}. However, the task of integrating contextual semantic similarity into the SH pattern matching is not solved by assessing the SS of entire sentences. Instead the content of specific sub-edges (the content of the predicate in our example) shall be semantically compared to the corresponding parts of the reference contents, thereby taking the context of of the sub-edge, i.e. the content of their root edge into account. Contextual neural embedding-based approaches allow this form of semantic similarity measurement since embeddings for parts of a larger text item can be constructed by combining the corresponding token embeddings. These sub-edge- or part-specific embeddings can then be used for SS measurement. Although deep learning-based approaches, which directly operate on sentence level, may show even better performance \cite{chandrasekaranEvolutionSemanticSimilarity2021}, CNESS methods have been identified in this work to be very performant as well (see \cref{sec:contextual-embedding-semantic-similarity}). \todo{is this too weak of a statement?} 

Additionally, neural embedding-based models could also be integrated more efficiently into the SH framework's matching process than DLSS measures, which require the pairwise -- computationally expensive --  processing of text items to determine their similarity \cite{reimersSentenceBERTSentenceEmbeddings2019}. In contrast, the embeddings of text items principally only need to be computed once and can then be used to perform the computationally inexpensive spatial measurements to determine their corresponding semantic similarities.

%
%\todo[inline]{
%arguments for NESS (instead of DLSS):
%- create embeddings for sub-edges based on the embedding of the entire edge \\
%- embeddings for atoms based on root edge embeddings may be saved and added to the hypergraph structure, making later use much more efficient \\
%- (because it is in line for FNESS, wich shall also be tested) \\
%}

%\todo[inline]{
%This has to be adapted based on chapter 2 \\
%-> add reference to FNESS/CNESS and derive relevancy of both for this work -> modify RQs \\
%-> refer to the similarity threshold specifically when talking about controlling?
%}

Integrating neural embedding-based semantic similarity measures into the pattern matching process would allow for edge content generalisation and therefore would make the process more adaptive. Since NESS measures are generally based on machine learning methods and CNESS measures specifically are mostly based on deep neural network, they principally do not provide the openness that is inherent to the purely symbolic pattern matching process of the SH framework. In the sense of the open-opaque / strict-adaptive classification described above, this integration would mean a shift from openness to opaqueness and from strictness to adaptivity. To counteract the opaqueness introduced by an NESS integration into the SH pattern matching, allowing user control over the generalisation level can maintain some openness while still benefiting from increased adaptivity.


%One approach for performing the retrieval would be to use a system which allows to specify some form of pattern which abstractly represents the statements they are trying to capture. This requires the definition of some form of formal pattern language\footnote{The \textit{Google Search} query language can be seen as a simple example of such a pattern language, albeit with a different use case focus: \url{https://support.google.com/websearch/answer/2466433?hl=en}} and possibly the prior transformation of the text corpus into some form of structured format to match against. Another approach is to use a system, which accepts example statements concretely representing the statements that are desired to be retrieved. Those systems may require a large number of positive and negative examples to be able to perform the retrieval. The two types of retrieval systems described here are in tendency situated in the realms of symbolic IR/IE and probabilistic ML/DL respectively.

%The SH framework is more situated in the former symbolic realm. In SH text is represented in the form of \textit{hyperedges} (in the following also referred to as \textit{edges} only). These edges are either atomic or they consist of edges themselves, which essentially accounts for the recursive character of the SH. Each edge has a specific \textit{type} from a set of eight different types of which the most trivial two types are probably \textit{concept} (\textsf{C}) and \textit{predicate} (\textsf{P}). 

%There are additional operators in the pattern language such as the wildcard operator \textsf{*}, which can be used e.g. to match every atomic edge of a specific type and therefore discard content.

% ========== 
\section{Research Questions}
\label{sec:research-questions}
Based on the problem statement outlined above, we pose the following research questions:

\subsection{Primary Question}
\textbf{R} Can neural embedding-based semantic similarity measures be be integrated into the pattern matching of the Semantic Hypergraph framework to allow for more semantically generalising matching regarding edge content while providing some control over the level of generalisation and therefore maintaining some openness of the pattern matching process?

\subsection{Secondary Questions}
\textbf{R.1} Which specific neural embedding models would be the most suitable for assessing semantic similarity within the Semantic Hypergraph pattern matching process while addressing the challenges posed by contextuality?

\todo[inline]{answered by background chapter and implementation specific considerations}

\textbf{R.2} How can neural embedding based semantic similarity effectively and efficiently be integrated into the Semantic Hypergraph pattern matching?

\todo[inline]{answered by the solution approach design and its working implementation}

\textbf{R.3} Does integrating neural embedding-based semantic similarity measurements into the SH pattern matching improve the systems retrieval performance and how does it impact recall and precision? 

\todo[inline]{answered by results of \cref{sec:result-retrieval-performance-improvement} and \cref{sec:result-retrieval-precision-behaviour}}

\textbf{R.4} Does the utilisation of contextual NESS enable the SH pattern matching to differentiate between desired and undesired edges in cases, which cannot be differentiated when utilising fixed word NESS?

\todo[inline]{answered by \cref{sec:result-contextual-differentiation-ability}}

\textbf{R.5} How can the level of edge content related generalisation in the pattern matching process be effectively and transparently controlled and how does this impact precision, recall and general retrieval performance?

\todo[inline]{answered by the implementation of the similarity threshold and by the results of \cref{sec:result-retrieval-performance-improvement} and \cref{sec:result-retrieval-precision-behaviour}}

\textbf{R.6} How does measuring contextual semantic similarity corresponding to sub-edge content influence retrieval performance in comparison to measuring the semantic similarity between the entire candidate and reference content context items?

\todo[inline]{answered by results of \cref{sec:result-retrieval-performance-improvement} --> sub tokens is better but more variation regarding the selection of reference edges, more ref. edges means less variation and better results in general}

\textbf{R.7} What impact does the the selection of a specific NESS model -- both fixed word and contextual -- have on SH pattern matching retrieval performance? 

\todo[inline]{answered by results of \cref{sec:result-retrieval-performance-improvement} --> better model does not clearly mean better result, but only two models for each NESS type were compared}




% ========== 
\chapter{Solution Approach}
\label{cha:solution-approach}
The proposed solution to the research questions posed in \cref{sec:research-questions} is to conceptualise and implement the integration of neural embedding based Semantic Similarity into the pattern matching of the Semantic Hypergraph framework, followed by a suitable evaluation of this integration. This chapter outlines the concept of the approach, while the implementation and evaluation are detailed in \cref{cha:implementation} and \cref{cha:evaluation} respectively.

The integration strategy involves pinpointing the most opportune point within the SH framework's pattern matching for the inclusion of NESS. This  allows to identify which parts of the SH framework require modifications to accommodate NESS integration, as well as recognising any components that are currently missing. To facilitate this, the following section elaborates on the approach's general concept, addressing its core challenges and design decisions. 

The system that is conceived by integrating NESS into the Semantic Hypergraph pattern matching will be  referred to as \textit{Neural Embedding-based Semantic Similarity extended Semantic Hypergraph Pattern Matching} (NESS-SHPM)

\section{Neural Embedding-based Semantic Similarity extended Semantic Hypergraph Pattern Matching}
The pattern matching process is described in \cref{sec:pattern-matching-process}, where the concepts of edge candidate content and pattern reference content are introduced. At a certain point in the process, the candidate content is matched against the reference content. In the original form of the SH pattern matching, this is limited to exact string-based or lemma-based matching. The SH framework's functionality will be extended by neural embedding based semantic similarity measurement based matching regarding edge content.

The NESS measurement involves generatin embeddings for the candidate and reference content, which in the following are referred to as \textit{candidate embedding} and \textit{reference embeddings}. The distance between these embeddings is calculated using a suitable metric and then compared to a given semantic \textit{similarity threshold} \(s_t\). If the measured semantic similarity exceeds this threshold, the contents of the pattern and the edge are considered to match. This process requires that the candidate content, reference content, similarity threshold (plus possibly additional other relevant parameters) are specified and passed to the component responsible for this matching step.

Moreover, the integration of NESS-based content matching into the SH pattern matching process will take two forms: \textit{Fixed Neural Embedding-based Semantic Similarity extended Semantic Hypergraph Pattern Matching} (FNESS-SHMP) and  \textit{Contextual Embedding-based Semantic Similarity extended Semantic Hypergraph Pattern Matching} (CNESS-SHMP). The particularities of these two system variants are elaborated in the following.

%\subsection{FNESS-SHPM}
In the case of FNESS, embeddings are generated based on single words. Therefore the candidate content takes on the form of a \textit{candidate word} while the reference content may be one or multiple \textit{reference words}. It is considered potentially useful to leverage the generalisation capabilities that already exist in the SH pattern matching in the form of the lemma function. Hence a lemma based variant of FNESS extended SH pattern matching (LFNESS-SHMP) is also conceptualised. In the case of LFNESS, the candidate content takes on the form of the candidate words lemma.

%\subsection{CNESS-SHPM}
In the case of CNESS, embeddings are generated based on context which can be phrases or entire sentences. In the SH framework those are represented in the form of hyperedges and therefore the candidate and reference content take on the form of a \textit{candidate edge} and one or multiple \textit{reference edges}. 

\section{Integration into the Semantic Hypergraph Framework}

The Semantic Hypergraph framework offers two primary avenues for enhancement: through the expansion of its pattern language or through the adaptation or refinement of the software interface utilised in the matching process. These approaches are not mutually exclusive and can be effectively combined.

Leveraging the core principles of the SH framework, the most logical and straightforward method to enrich its pattern matching capabilities is by extending its pattern language. This strategy also facilitates the concurrent application of NESS-based content matching alongside string- or lemma-based content matching within the same matching process.

Therefore, the incorporation of NESS-SHMP into the existing SH framework encompasses three principal components: the extension of the pattern language, the requisite modifications to the pattern language processing and adding the components necessary for the actual NESS measurement calculations.

\todo[inline]{Here is probably the most suitable point to add a system/approach schema}
\newpage

\subsection{Extentions of the Pattern Language}
For the inclusion of NESS-based content matching within the SH pattern language, the \textit{SemSim} functional pattern will be introduced. This addition is targeted at the functional pattern segment of the language, which is already predisposed to support flexible enhancements for SH pattern matching. This approach is consistent with the existing mechanism of realising lemma-based content matching capabilities in the framework.


\subsubsection{SemSim Functional Pattern}
When constructing a SH pattern that should be matched against a hypergraph, the \textit{SemSim} functional pattern can be utilised at those points in the pattern, where content should be matched by NESS. The added SemSim functional pattern (also referred to as SemSim pattern) has one of the following syntactical structures:

\begin{center}
%	\texttt{<semsim function> <reference content>/<hyperedge type>.<argument roles> <similarity threshold>}
	\texttt{<SF> <IA><RC>/<HT>.<AR> <ST>}\ or \\
	\texttt{<SF> <IA><VA>/<HT>.<AR> <RC> <ST>}\
\end{center}

It consists of different syntactical components, here represented by placeholders that are enclosed in brackets (\texttt{<>}). These components are explained in the following:

%\subsubsection{SemSim Function (\texttt{SF})}
\paragraph{SemSim Function (\texttt{SF})}
 The semsim function can be one of the following: \texttt{semsim-fix}, \texttt{semsim-fix-lemma} and \texttt{semsim-ctx}. These functions correspond to fixed word neural embedding-based, lemma-based fixed word neural embedding-based and contextual neural embedding-based semantic similarity measurement.

%\subsubsection{Reference Content (\texttt{RC})} 
\paragraph{Reference Content (\texttt{RC})} 
The reference content is used to specify the reference word(s) for the FNESS variant. The square bracket notation which is already part of the SH pattern language is leveraged to pass multiple words as a list. The reference edge(s) that is needed for the contextual NESS variant is not given via the semsim functional pattern itself, but as a software interface argument to the matching process due to practical considerations (see \cref{sec:pl-extension-limitations}).


\paragraph{Variable Declaration (\texttt{VA})} 
If the content of the sub-edge captured by the SemSim sub-pattern should be captured in a variable, the second variant of the syntax applies. This notation is in line with the usage of the lemma functional pattern illustrated in \cref{sec:sh-pattern-matching}.


%\subsubsection{Hyperedge Type (\texttt{HT}) and Argument Roles (\texttt{AR})}
\paragraph{Hyperedge Type (\texttt{HT}),  Argument Roles (\texttt{AR}) and Innermost Atom Operator (\texttt{IA})} These pattern components can be utilised in the same way as illustrated above in \cref{sec:sh-pattern-matching} for the lemma functional pattern.
Hyperedge type and argument roles are matched following the symbolic rules also described above. The innermost atom operator (\texttt{>}) can be placed in front of the reference content or the variable declaration if applicable.


%\subsubsection{Similarity Threshold (\texttt{ST})}
\paragraph{Similarity Threshold (\texttt{ST})}
%The similarity threshold used to decide whether candidate and reference content matched, given that the NESS measurement has already been computed. This applies to the fixed as well as to the contextual variant of NESS. 
The similarity threshold \(s_t\) can optionally be specified for a specific occurrence of the SemSim functional pattern, but it can also be given as a global parameter of the matching process (see \cref{sec:ness-config}). 

\subsubsection{Example Usages}

To illustrate the usage of the SemSim functional pattern, some examples are given:

\begin{pattern}[h!]
  \normalfont\sffamily
  \centering
  ( likes/P Ann/C (semsim-fix apples/C) )
  \caption{"Ann likes something similar to apples" pattern}
  \label{pat:ann-likes-semsim-aplles}
\end{pattern}

\begin{pattern}[h!]
  \normalfont\sffamily
  \centering
  ( likes/P ann/C (semsim-fix [apples, bananas]/C) )
  \caption{"Ann likes similar to apples or bananas" pattern}
  \label{pat:ann-likes-apples-and-bananas}
\end{pattern}


\begin{pattern}[h!]
  \normalfont\sffamily
  \centering
  ( likes/P Ann/C (semsim-fix mangos/C 0.5) )
  \caption{"Ann likes something similar to mangos" pattern with \(s_t = 0.5\)}
  \label{pat:ann-likes-semsim-aplles}
\end{pattern}


\subsubsection{Limitations of the Pattern Language Extension}
\label{sec:pl-extension-limitations}
The utilisation of the fixed variant of NESS-SHMP is possible solely via the SemSim functional pattern. This means that all information that is required to apply FNESS-based content matching, specifically the reference content and the similarity threshold, can be included in a SH pattern. In contrast, it was found impractical to provide the reference edges needed for the contextual NESS variant via a SH pattern. While the reference content argument could generally be used for that, this would result in very exhaustive patterns and impair their readability for humans. Therefore the conceptual design choice was made to pass the reference edges via the software interface of the SH pattern matching.


\subsection{Modification of the Pattern Language Processing}
This section outlines the integration of SemSim functional pattern processing with the current pattern language processing. The process is triggered when a pattern, incorporating a SemSim pattern, is matched against a specific hyperedge and the SemSim pattern is reached without any prior mismatches. At this point, the SemSim pattern captures a sub-edge for processing, which includes the symbolic structural matching. 
%of the hyperedge type and possibly argument, as well as the application of the innermost atom operator, if specified. 
Additionally the necessary information for NESS measurement computations is extracted.

The operation of SemSim pattern processing changes based on the specific SemSim function applied. With FNESS-SHMP or LFNESS-SHMP, the process extracts the candidate word(s) or their lemma(s) from the identified sub-edge and the reference word(s) from the pattern. This extraction process requires the sub-edge to be atomic, which implies combination with the innermost atom operator to ensure functionality in all cases. For CNESS, it conducts a standard string-based matching of the reference content. Therefore  utilizing the wildcard operator as a reference content argument is sensible in most practical use cases. For any NESS-SHMP variant, the similarity threshold is extracted from the SemSim pattern, if given

Existing components manage the symbolic matching of hyperedge type and argument roles, as well as the application of the innermost atom operator, consistent with the standard pattern matching. The SemSim function, along with the similarity threshold (when provided), candidate words and reference words for (L)FNESS-SHMP, are passed to the newly designed NESS computation components.


%\subsection{NESS Measurement Computations}
%When the NESS-SHPM process arrives at this point, the candidate and reference contents have either been extracted by the pattern language processing (in case of FNESS-SHMPM) or passed down as software interface arguments (in the case of CNESS-SHPM). The similarity threshold has either also been extracted from the SemSim pattern or was given to the NESS-SHPM process as a global default parameter via the software interface. So now the candidate and reference content can be used go to obtain the respective embeddings used to perform the similarity measurement between them.
%
%\todo{How it the NESS model specified? talk about ness config?}
%
%\subsubsection{Fixed Neural Embeddings}
%To generate the fixed neural embeddings, only the candidate word(s) and the reference word(s) are needed. Since every word corresponds to a fixed embedding, a simple lookup is performed using the specified fixed neural embedding model to generate the candidate and reference embeddings.
%
%\subsubsection{Contextual Neural Embeddings}
%The construction of the contextual candidate and reference embeddings involves multiple steps. It is assumed that the embeddings should correspond to the sub-edge captured by the SemSim pattern and their counterparts in the reference edges. Thereby the reference edges need to structurally match (with no regard to edge content) the pattern which contains the SemSim functional pattern, so that these reference sub-edges can be identified. To enable verification of this assumption another version of the contextual neural embedding construction is conceptualised, which omits the sub-edge correspondence of the embeddings. For this version only the first two steps of the following construction process are relevant.
%
%\begin{enumerate}
%	\item Reconstruction of the phrases or sentences (context items) from the candidate and reference edges.
%	\item Generating the embeddings for the candidate and reference context items using the specified contextual neural embedding model.
%	\item Identification of the sub-edge in the reference edges that corresponds to the sub-edge in the candidate edge, which was captured by the SemSim functional pattern.
%	\item Mapping of the candidate and reference sub-edge to the relevant sub-embeddings of the candidate and reference context item embeddings.
%	\item Generation of the actual candidate and reference embeddings based on these candidate and reference sub-embeddings.
%\end{enumerate} 
%
%
%\subsubsection{Embedding Similarity Measurement}
%The cosine similarity between the candidate and reference embeddings is computed and compared to the given similarity threshold. If the computed similarity is greater than the threshold, the candidate and reference content are considered to match. 


\subsection{Neural Embedding-based Semantic Similarity Measurement}
Upon reaching this phase, the NESS-SHPM process has already either extracted candidate and reference contents via pattern language processing (in case of FNESS-SHPM) or received them through software interface arguments (in case ofCNESS-SHPM). Similarly, the similarity threshold has been either derived from the SemSim pattern or assigned as a default parameter through the software interface. Consequently, the process is set to obtain the respective embeddings for the candidate and reference content, which are crucial for conducting the similarity assessment.

\todo{How it the NESS model specified? talk about ness config?}


\subsubsection{Fixed Neural Embeddings}
The creation of fixed neural embeddings necessitates only the candidate and reference words. Given that each word maps directly to a specific embedding, a straightforward lookup in the chosen fixed neural embedding model suffices to produce the necessary embeddings for both candidate and reference.

\subsubsection{Contextual Neural Embeddings}
The development of contextual embeddings for the candidate and reference involves a sequence of steps, premised on the assumption that these embeddings should mirror the specific sub-edge of the candidate edge captured by the SemSim pattern and its analogous sub-edge(s) in the reference edge(s). Thereby the reference edges need to structurally match (with disregard to edge content) the pattern which contains the SemSim functional pattern, so that these reference sub-edges can be identified. An alternative approach to constructing contextual neural embeddings does not account for sub-edge matching and only consists of the initial two stages of the construction process outlined below:

\begin{enumerate}
	\item Reconstructing phrases or sentences from the candidate and reference edges (deemed context items).
    \item Producing embeddings for these context items using the designated contextual neural embedding model.
    \item Identifying the sub-edge within the reference that aligns with the candidate's sub-edge captured by the SemSim pattern.
    \item Associating the identified candidate and reference sub-edges with their respective sub-embeddings derived from the context item embeddings.
    \item Generating the final embeddings for both candidate and reference based on these sub-embeddings.
\end{enumerate}
    
\subsubsection{Embedding Similarity Measurement}
The procedure calculates the cosine similarity between the embeddings of candidate and reference content, comparing it to the pre-established similarity threshold. If the resultant similarity score surpasses this threshold, the candidate and reference contents are considered to match. Given multiple reference content items (and therefore multiple reference embeddings), the pairwise similarities between the candidate and the reference embedding is computed. The maximum of these similarities is then compared to the similarity threshold to asses whether the candidate and reference contents match.


\todo{why cosine similarity? why max similarity?}



% ========== 
\chapter{Implementation}
\label{cha:implementation}

\section{Relevant external Software Libraries used}
Here list libs and models to be referenced later.

Word2Vec via Gensim
SentenceTransformers
Transformers
SpaCy

\section{Modules newly added to the SH Framework}
Semsim instances

reference edge sample modification parameter


\section{Modifications of the SH Pattern Matching}
skip semsim

root edges

\subsection{Token position passing}
The matching process of the SH framework is extended by passing the tok pos down (this teferences implicitly the recursive implementagion of the pattern matching which might be too technical)


\subsection{SH PL implementation differences}
Differences between the formal notation and the notation used in the implementation
--> innermost atoms operator, variable functional pattern etc.
does this need to be included?

\section{Modifications to the Hypergraph database}
is this really necessary? tok pos etc, but not actually specific to semsim




\paragraph{NESS configuration}
\label{sec:ness-config}
Additional parameters need to be given to the NESS-SHMP process, which includes the specific neural embedding model to use 
 
\begin{itemize}
	\item Default similarity threshold
	\item Specific NESS model
	\item CNESS embedding prefix
	\item CNESS all tokens option
\end{itemize}




%\section{Similarity Threshold}
%\label{sec:similarity-threshold}


%\section{Tokenization}
%
%\subsection{SpaCy}
%SpaCy linguistic tokenization (https://spacy.io/usage/linguistic-features how-tokenizer-works)
%spacy (without transformers) uses an purely rule based (but language depended) tokenizer as far as I understand: https://spacy.io/usage/linguistic-features how-tokenizer-works (the call it linguistic tokenizer)
%
%side note about using different transformer models than the provided one (because i was always confused about this):
%it it possible to exchange the underlying transformer component for basically every transformer model (as long as it follows the conventions that spacy expects), but you would have to retrain the spacy model to be able to use the task specific heads (like e.g. NER)
%footnote: https://github.com/explosion/spaCy/discussions/10327
%
%an alignment is provided between the transformer-tokenizer and the spacy-tokenizer
%lib: https://github.com/explosion/spacy-alignments
%
%footnote: https://explosion.ai/blog/spacy-transformers
%
%
%\subsection{WordPiece and SentencePiece}
%
%SentencePiece: https://github.com/google/sentencepiece


%\section{Matching candidate edge and reference edge tokens}
%both edges should match the pattern and should act as a valid ref edge for each other
%but it is obvious that the tok_idx_trail that leads to the predicate in the one edge wont lead to the predicate in the other edge
%that was the premise on which i built the matching (and which we discussed i think)
%
%so there are 2 cases:
%[candidate edge is more specific than reference edge] the location trail of the token in the candidate edge is longer than in the reference edge. this case should be trivial. we can just cut off the location trail when we reach an atom in the reference tok_pos.
%[reference edge is more specific than candidate edge] the location trail of the token in the candidate edge does not lead to an atom in the reference edge. in this case there is not enough information to match the tokens. i see two possible solutions:
%use the whole sub-edge to compute the reference embedding (maybe i misunderstood and that was your conception all along)
%try to get the information which token to use in some other way. possibly by matching via the atom types… this should work for cases like predicates but would not work if we are looking for a modifier or something else which can appear multiple times. i have the feeling that this should be recoverable through the graphbrain matching process somehow, i just don’t know how….
%(edited)
%
%for now i have the tendency to implement 2a as it is much easier not sure about the semantic implications though



%-----------

%For fixed NESS-based matching, the processing occurs in-line with symbolic pattern matching, seamlessly integrating as a component of the overall pattern matching process. However, due to the intricacies involved with contextual NESS-based matching, this processing is performed as a post-process.


%
%\subsubsection{In-Line- vs. Post-Processing}
%While the processing of the SemSim functional pattern in the sense of pattern language processing always needs to happen in-line with the regular pattern matching, the actual NESS computations may happen as a post-processing step. When the pattern matching process arrives at a SemSim functional pattern, the symbolic part of the SemSim pattern -- i.e. the hyperedge type and the argument roles -- are always processed imeediatly, hence in-line. The result of this structural matching is then passed up to the calling procedure of the matching process. Whether its content is matched in-line, i.e. the NESS mesaurment compuations take place immediatly depends on the NESS type and the configuration of the pattern matching process. 
%
%In the case of the contextual variant, the CNESS compuations always occur as a post-process. This means that for given edge and a given pattern, which contains at least on ocurrence of a semsim-ctx sub-pattern, the entire edge and pattern are first matched in regards to everything that is not a semsim-ctx sub-pattern. This means that the information necessary for the CNESS computation gets only recorded at this point. The actual computations only take place afterward, if everything else has matched. This design choice is made, because it facilitated the realisation of the contextual NESS based matching. Firstly, the context information does not have to be passed down to the SemSim functional pattern matching step and secondly, the CNESS computations are computationally relatively expensive and should therefore be avoided if unnecessary
%
%
%
%\subsubsection{SemSim Skipping and SemSim Instances}
%
%%The FNESS computations may take place in-line, as the information required for them is already contained in the argument of the SemSim functional pattern, nonetheless the may also
%
%It is possible to configure the pattern matching process via the \textit{SemSim skipping} option of its software interface, so that the NESS computations are skipped entirely. In this case only the symbolical matching takes place when matching against a pattern that contains occurrences of the SemSim functional pattern. For each of these occcurence, a \textit{SemSim instance} will be recorded, that is added to the results of the symbolic matching. These instances can then be subjected to processing completely independent from the pattern language processing. This is especially useful if the NESS measurements shall be computed with a set of different NESS configuration parameters (e.g. different similarity thresholds), as is to be expected in an experimental evaluation. 
%



% ========== 
\chapter{Evaluation}
\label{cha:evaluation}
In this chapter the conceived concept (see \cref{cha:solution-approach}) and specific implementation (see \cref{cha:implementation}) of the NESS-SHPM system is being evaluated to answer the research question(s) posed in \cref{sec:research-questions}. Therefore a a case study is conducted to evaluate the system for a specific use case. 
\todo{term differentiation: NESS-SHMP and (F/C)NESS}
\todo[inline]{refer to the RQs more specifically? how are they going to be answered?}


\section{Case Study: Conflicts}
The conflicts case study follows the approach presented in \citef{menezesSemanticHypergraphs2021}, where expressions of conflict are extracted from a given SH using a single SH pattern. In their work they build upon the information extracted by the pattern to conduct further analyses, which are not in the scope of this work. Here the evaluation is limited to the task of classifying whether the content of a given edge in the SH is an expression of conflict or not. Or framed differently, the task is to retrieve exactly all those edges whose content is an expression of conflict. The evaluation will compare the retrieval performance of a suitable set of different SH patterns and corresponding configuration of the NESS-SHMP system by matching them against a labelled dataset of hyperedges.

\todo[inline]{should I explain why specifically the conflicts and not some other case study (i.e. dataset) --> because there was none... but then I need to show why there was none and what are the criteria for a case study to be suitable to evaluate the system}

\subsection{Expressions of Conflict}
\label{sec:conflict-definition}
An expression of conflict in the context of this case study is defined as a sentence which fulfils the following properties:

\begin{displayquote}
There is a conflict between two explicitly named actors, wherever these actors are mentioned in the sentence; whereby a conflict is defined as antagonizing desired outcomes.
\end{displayquote}

\subsection{Reddit Worldnews Corpus}
The corpus from which those expressions of conflict are retrieved consists of news titles that were shared on the social media platform \textit{Reddit}. Specifically all titles shared between January 1st, 2013 and August 1st, 2017 on \textit{r/worldnews}, which is described as: “A place for major news from around the world, excluding US-internal news.”\footnote{\url{http://reddit.com/r/worldnews}} This corpus contains 479,384 news headers and is in the following referred to as the \textit{Worldnews-Corpus}.

Each of these headers is comprised of a single sentence and is forms a root edge in the SH constructed from it. In the following this SH is referred to as the \textit{Worldnews-SH}. Parsing errors that may potentially occur during this constructed and can obstruct a correct retrieval of a wrongly parsed edge i.e. wrongly represented sentence. These errors are out of scope of this work. All edges in the Worldnews-SH are assumed to be correctly parsed.

%These are some randomly selected examples from the Worldnews-Corpus:
%\begin{itemize}
%	\item Example
%	\item Example
%	\item Example
%\end{itemize}
%
%\todo{add examples}


\subsection{Semantic Hypergraph Patterns}
\label{sec:sh-patterns}
The SH patterns that are used in this evaluation all have the same general form to isolate the effect of replacing a purely symbolic matching against a specific word or list of words with NESS-SHMP. In this section the general form of these pattern will be described, which entails consequences for the creation of the labelled dataset described in \cref{sec:conflict-dataset}. 

%The retrieval performance of the original purely symbolic pattern defined by \citef{menezesSemanticHypergraphs2021} is compared against the retrieval performance of patterns containing some form of the semsim functional pattern. \todo{remove?}


\subsubsection{Original Conflict Pattern}
\Cref{pat:original-conflict} is originally defined in \citef[p.~22]{menezesSemanticHypergraphs2021} and is therefore referred to as the \textit{original conflict pattern}. It is used to extract conflicts between two parties \textsf{SOURCE} and \textsf{TARGET}, potentially regarding some \textsf{TOPIC}. As mentioned before, the assignment of these variables is irrelevant for this case study.

The original conflict patterns contains two sub-patterns which utilize word lists. These sub-patterns match the trigger sub-edge and predicate sub-edge of a candidate edge respectively and are in following referred to as \textit{trigger sub-pattern} and \textit{predicate sub-pattern}. If not stated otherwise these terms will refer to \cref{pat:original-conflict}. 

\begin{itemize}
	\item \textbf{\textsf{Trigger sub-pattern:}} \textsf{[against,for,of,over]/T}
	\item \textbf{\textsf{Predicate sub-pattern:}}
		\textsf{( PRED/P.{so,x} ) \(\wedge\) \\ ( lemma/J >PRED/P [accuse,arrest,clash,condemn,kill,slam,warn]/P )}
\end{itemize}

In the trigger sub-pattern the content of the candidate trigger sub-edge is directly matched against a list of prepositions, which are in the following referred to as the \textit{conflict prepositions}. In case of the predicate sub-pattern, the word list is matched against the lemma of the innermost atom of the candidate predicate sub-edge, which is always a verb. The list of verbs used here will in the following be referred to as the \textit{conflict verbs}.

\begin{itemize}
	\item \textbf{\textsf{Conflict prepositions:}} against, for, of, over
	\item \textbf{\textsf{Conflict verbs:}} accuse, arrest, clash, condemn, kill, slam, warn
\end{itemize}


\begin{pattern}
  \normalfont\sffamily
  \centering
  ( PRED/P.{so,x} SOURCE/C TARGET/C [against,for,of,over]/T TOPIC/[RS] ) \(\wedge\) \\
  ( lemma/J >PRED/P [accuse,arrest,clash,condemn,kill,slam,warn]/P )
  \caption{Original conflict pattern}
  \label{pat:original-conflict}
\end{pattern}

\begin{pattern}
  \normalfont\sffamily
  \centering
  ( (lemma PRED >[accuse,arrest,clash,condemn,kill,slam,warn]/P.\{so,x\}) \\
  SOURCE/C TARGET/C [against,for,of,over]/T TOPIC/[RS] ) 

  \caption{Original conflict pattern (rewritten)}
  \label{pat:original-conflict-rewritten}
\end{pattern}

%\begin{pattern}
%  \normalfont\sffamily
%  \centering
%  ( (lemma >PRED/P.\{so,x\} [accuse,arrest,clash,condemn,kill,slam,warn]) \\
%  SOURCE/C TARGET/C [against,for,of,over]/T TOPIC/[RS] ) 
%  \caption{Original conflict pattern (rewritten)}
%  \label{pat:original-conflict-rewritten}
%\end{pattern}


%\begin{pattern}
%  \normalfont\sffamily
%  \centering
%  ( >(lemma PRED [accuse,arrest,clash,condemn,kill,slam,warn])/P.\{so,x\} \\
%  SOURCE/C TARGET/C [against,for,of,over]/T TOPIC/[RS] ) 
%  \caption{Original conflict pattern (rewritten)}
%  \label{pat:original-conflict-rewritten}
%\end{pattern}





\subsubsection{Wildcard Conflict Patterns}
\label{sec:wildcard-conflict-patterns}
Replacing either the trigger sub-pattern, the predicate sub-pattern or both of them with a semsim function are the options for utilizing NESS-SHPM in a modified version of \cref{pat:original-conflict} without modifying the general structure of the pattern. To evaluate which of these options are best suited to evaluate the retrieval performance of NESS-SHPM, three \textit{wildcard conflict patterns} are constructed. In these patterns the predicate sub-pattern (\cref{pat:wildcard-pred}) or the trigger sub-pattern (\cref{pat:wildcard-prep}) are replaced by the wildcard operator. 
%or both (\cref{pat:wildcard-pred-prep})

\begin{pattern}
  \normalfont\sffamily
  \centering
  ( PRED/P.{so,x} SOURCE/C TARGET/C [against,for,of,over]/T TOPIC/[RS] ) \(\wedge\) \\ ( PRED/P */P )
  \caption{Predicate wildcard pattern}
  \label{pat:wildcard-pred}
\end{pattern}

\begin{pattern}
  \normalfont\sffamily
  \centering
  ( PRED/P.{so,x} SOURCE/C TARGET/C */T TOPIC/[RS] ) \(\wedge\) \\  ( lemma/J >PRED/P [accuse,arrest,clash,condemn,kill,slam,warn]/P )
  \caption{Trigger wildcard pattern}
  \label{pat:wildcard-prep}
\end{pattern}

%\begin{pattern}
%  \normalfont\sffamily
%  \centering
%  ( */P.{so,x} SOURCE/C TARGET/C */T TOPIC/[RS] )
%  \caption{Predicate and trigger wildcard pattern}
%  \label{pat:wildcard-pred-prep}
%\end{pattern}

\paragraph{Preliminary Evaluation}
The three wildcard conflict patterns are matched against the Worldnews-SH and the number of matches is recorded. Comparing the number of matches of these patterns shows which of the sub-patterns is most influential for the retrieval performance of \cref{pat:original-conflict}. \Cref{tab:wildcard-pattern-evaluation} shows the results of these preliminary evaluations as well as the number of matches that result from matching \cref{pat:original-conflict} against the Worldnes-SH. It can be seen that the choice of conflict verbs is much more influential on the number of matches than the choice of conflict prepositions when compared to the number of matches resulting from the original conflict pattern. While replacing the predicate sub-pattern with a wildcard operator yields an increase with a factor of \(12,45\), replacing the trigger sub-pattern with a wildcard operator only yields an increase with a factor of \(1,07\). 


\begin{table}[h]
\centering
\begin{tabular}{lr}
\toprule
\multicolumn{1}{l}{Pattern name}				& \multicolumn{1}{l}{Number of matches} \\
\midrule
Original conflict pattern					& 5766	\\
Predicate wildcard pattern					& 71804 \\
Trigger wildcard pattern						& 6154	\\
%Predicate and trigger wildcard pattern		& 79431	\\
\bottomrule
\end{tabular}
\caption{Results of matching the wildcard patterns against the Worldnews-SH}
\label{tab:wildcard-pattern-evaluation}
\end{table}



\subsubsection{SemSim Conflict Patterns}
Based on the result of the preliminary evaluation in \cref{sec:wildcard-conflict-patterns}, the predicate sub-pattern of \cref{pat:original-conflict} is replaced by different forms of semsim functional patterns to construct different \textit{semsim conflict patterns}. These patterns are then used to evaluate the effects of utilizing NESS-SHPM. The trigger sub-pattern is not modified to better isolate these effects in comparison to purely symbolic SHPM.

\Cref{pat:semsim-conflict} describes the general form of a semsim conflict pattern. The \texttt{<SEMSIM-FUNCTION>} placeholder is replaced with one of the three implemented semsim functions to construct the \textit{semsim-fix conflict pattern} (\cref{pat:semsim-fix-conflict}), \textit{semsim-fix-lemma conflict pattern} (\cref{pat:semsim-fix-lemma-conflict}) and the \textit{semsim-ctx conflict pattern} (\cref{pat:semsim-ctx-conflict}). As \texttt{<SEMSIM-ARGUMENT>} the conflict verb list is used as similarity reference words in \cref{pat:semsim-fix-conflict} and \cref{pat:semsim-fix-lemma-conflict}, which utilize FNESS. In the semsim-ctx conflict pattern, the wildcard operator is used as \texttt{<SEMSIM-ARGUMENT>} since the necessary reference edges can only be provided via an external parameter and not inside the pattern.


\begin{pattern}[H]
  \normalfont\sffamily
  \centering
  ( PRED/P.{so,x} SOURCE/C TARGET/C [against,for,of,over]/T TOPIC/[RS] ) \(\wedge\)\\ 
  ( \texttt{<SEMSIM-FUNCTION>}/J PRED/P \texttt{<SEMSIM-ARGUMENT>}/P )
  \caption{General SemSim conflict pattern}
  \label{pat:semsim-conflict}
\end{pattern}

\begin{pattern}[H]
  \normalfont\sffamily
  \centering
  ( PRED/P.{so,x} SOURCE/C TARGET/C [against,for,of,over]/T TOPIC/[RS] ) \(\wedge\)\\ 
  ( semsim/J PRED/P [accuse,arrest,clash,condemn,kill,slam,warn]//P )
  \caption{semsim-fix conflict pattern}
  \label{pat:semsim-fix-conflict}
\end{pattern}

\begin{pattern}[H]
  \normalfont\sffamily
  \centering
  ( PRED/P.{so,x} SOURCE/C TARGET/C [against,for,of,over]/T TOPIC/[RS] ) \(\wedge\)\\ 
  ( semsim-fix-lemma/J PRED/P [accuse,arrest,clash,condemn,kill,slam,warn]//P )
  \caption{semsim-fix-lemma conflict pattern}
  \label{pat:semsim-fix-lemma-conflict}
\end{pattern}

\begin{pattern}[H]
  \normalfont\sffamily
  \centering
  ( PRED/P.{so,x} SOURCE/C TARGET/C [against,for,of,over]/T TOPIC/[RS] ) \(\wedge\)\\ 
  ( semsim-ctx/J PRED/P */P )
  \caption{semsim-ctx conflict pattern}
  \label{pat:semsim-ctx-conflict}
\end{pattern}



\section{Conflict Dataset}
\label{sec:conflict-dataset}
To conduct an evaluation which assesses the retrieval performance of the NESS-SHPM system it is necessary to have a dataset of edges with labels that state whether an edge is an expression of conflict or not. Since such a dataset does not exists it needs to be constructed. In the following the construction process of this \textit{conflict dataset} (CD), which is used for the evaluation in this case study, and the datasets characteristics are discussed.


\subsection{Base Edge Set}
\label{sec:base-edge-set}
The set of edges that can be retrieved by a conflict pattern, i.e. the original conflict pattern or a semsim conflict pattern is restricted the general form of these patterns. This entails that, given the same SH, every set of matching edges of a pattern of this form will be a subset of the matching edges of the predicate wildcard pattern (\cref{pat:wildcard-pred}). The set of edges resulting form matching this pattern against the Worldnews-SH are therefore used as the \textit{base edge set} (BES) from which the conflict dataset is constructed, instead of the entirety of all the hypergraphs root edges.

\paragraph{Predicate Lemma} Every edge in the BES has a predicate sub-edge that has an innermost atom, which is a verb that has a lemma. In the following this is called the \textit{predicate lemma} of an edge. Each of the edges matching \cref{pat:original-conflict} or a pattern in the form of of \cref{pat:semsim-conflict} therefore corresponds to a predicate lemma.

%Matching \cref{pat:wildcard-pred} against the Worldnews-SH results in \(n_{f} = 69 380\) matching edges. In the following the set of those edges will be referred to as the \textit{BSE} (FD).\todo{add examples}

\subsection{Desired Characteristics}
\label{sec:dataset-characteristics}
To effectively evaluate the effectiveness of the application of NESS by matching a pattern in the form of \cref{pat:semsim-conflict}, the dataset used for this should have the following characteristics:

\begin{itemize}
	\item Contain the largest possible number of unique predicate lemmas
	\item Contain the largest possible number of edges per unique predicate lemma
\end{itemize}

On the one hand it is desired to have as many different unique predicate lemmas as possible in the dataset to be able to evaluate whether NESS can differentiate if a predicate lemma indicates an expression of conflict or not. On the other hand it is desired to have as many different edges per unique lemma as possible in the dataset to be able to evaluate whether CNESS is able to differentiate if edges represent an expression of conflict or not, given that they correspond to the same predicate lemma.

\subsection{Construction Process}
To create the labelled CD, the edges of the dataset need to be manually labelled by human annotators, which is labor-intensive. The BES contains \(n_b=71804\) edges. Due to the time constrains of this work and the limited availability of three annotators, the BES needs to be subsampled to create the CD. 

\subsubsection{Filtering}
Since the desired characteristics described above relate the the distribution of predicate lemmas, it is relevant to verify that is possible to determine the predicate lemma for all edges in the edge set from which the CD is sampled. In some cases it is not possible to determine the predicate lemma of a given edge due to to implementation issues, which out of scope of this work. In these cases an edge is filtered from the BES, which results in the \textit{filtered base edge set} (FBES). The FBES contains \(n_{f} = 69 380\) edges. 


\subsubsection{Sampling} 
The edges in the FBES correspond to \(n_{l} = 2195\) unique predicate lemmas. Attaining to the desired dataset characteristics, the number of samples \(n_{s}\) in the subsampled dataset should ideally be a multiple \(m_{l} >= 2\) of \(n_{l}\), so that \(n_{s} = m_{l} \cdot  n_{l}\). This would mean that every predicate lemma contained in the FBES is statistically represented multiple times in the subsampled dataset.

A dataset size of \(n_{s}=2000\) was chosen, wich means \(m_{l} < 2\) and  \(n_s << n_f\). This entails that a trade-off between the desired dataset characteristics has to be made. To account for this, a sampling method is applied that offers more control over the distribution of predicate lemmas in the subsampled dataset than uniform random sampling does. This sampling method is based on the idea of \textit{Stratified Sampling} \cite{parsonsStratifiedSampling2017} \todo{is this correct?} and is described in detail in \cref{algo:dataset-sampling}. 

The procedure splits the FBES into multiple bins after the edges are sorted by number of occurrence of their predicate lemma and then uniformly randomly samples from each bin. This method guarantees that predicate lemmas which correspond to a relatively small number of edges in the FBES will be represented in the subsampled dataset. The distribution of uniques lemmas in the FBES and the CD is compared visually in \cref{fig:dataset-lemma-distribution}.

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]
{dataset_lemma_distribution/lemma_distribution_dataset_conflicts_1-2_pred_wildcard_full_dataset_conflicts_1-2_pred_wildcard_subsample-2000.png}
\caption{Distribution of unique lemmas in the FBES and CD}
\label{fig:dataset-lemma-distribution}
\end{figure}


\begin{algorithm}
\begin{enumerate}
	\item Create a list of tuples \(t\) of edges and their corresponding predicate lemma: \\ \(L = [(l_k, e_i), ...]\) with \(k \in \{0,...,m\}\) and \(i \in \{0,...,n\}\)
	\item Sort this list by the number of tuples containing a predicate lemma to create the list: \\ \(L_{sort} = [(l_0, e_0), ... (l_m, e_n)]\), so that:
	\begin{itemize}
		\item \(n_k\) is the number of tuples containing a lemma \(l_k\)
		\item \(t_j\) with \(j > i\) is a tuple wich sorted after tuple \(t_i\)
		\item \(n_o >= n_p\) if \(t_i = (l_o, e_i)\) and \(t_j = (l_p, e_j)\)
	\end{itemize}			
	\item Split the list \(L_{sort}\) into \(n_{b}\) bins.
	\item Uniformly sample \(n_{sb}\) tuples from each bin.
	\item Build a set of all edges \(e\) contained in the sampled tuples.
\end{enumerate}
\caption{Dataset sampling algorithm}
\label{algo:dataset-sampling}
\end{algorithm}

The subsampled dataset size resulting from this sampling method is \(n_{s} = n_{b} * n_{sb}\). Given \(n_{s} = 2000\), the values \(n_b = 10\) and \(n_{sb} = 200\) were chosen for sampling the CD.


\subsubsection{Labelling}
The labelling task is shared between the three annotators. A given edge will be either labeled as \textit{conflict} or \textit{no conflict} by an annotator following the definition given in \cref{sec:conflict-definition}. Because of the aforementioned time constraints, every edge is only labeled by one annotator. To nonetheless ensure a consistent labelling among all annotators, a set of 50 edge is labelled by all three annotators. Every edge for which a disagreement in labelling occurs between at least two of the annotators, is inspected to reach an agreement on the label. Utilizing this process, the annotators understanding of what constitutes an expression of conflict is refined. Following this preliminary step, the \(n_s\) edges of the dataset are equally distributed among the three annotators and individually labelled by them.

% It was confirmed that the percentage of edges that represent an expression of conflict in relation to all edges in the subset given to an annotator did not vary more than ... among all three annotators.

\subsection{Edge Set Comparison}
The CD is the result of the filtering, sampling and labelling described above. The size of the Worldnews-SH, BES, FBES and CD are listed in \cref{tab:dataset-descriptions} for comparison. If applicable the the number of unique lemmas as well as the number and percentage of edges which are labelled as an expression of conflict and of those which are not are also noted.\todo{add examples? (in appendix?)}


\begin{table}
\centering
\begin{tabular}{lrrrr}
\toprule
\multicolumn{1}{l}{Edge set name}	& \multicolumn{1}{c}{Number of} & \multicolumn{1}{c}{Number of}		& \multicolumn{1}{c}{Number of} 		& \multicolumn{1}{c}{Number of} \\
\multicolumn{1}{l}{} 				& \multicolumn{1}{c}{all edges} & \multicolumn{1}{c}{un. lemmas}			& \multicolumn{1}{c}{conflict edges} 	& \multicolumn{1}{c}{no conflict edges} \\
\multicolumn{1}{l}{} 				& \multicolumn{1}{c}{} 			& \multicolumn{1}{c}{}				& \multicolumn{1}{c}{(\% of all edges)} & \multicolumn{1}{c}{(\% of all edges)} \\
\midrule
Worldnews-SH						& 479384		& -			& -					& - \\
Base Edge Set (BES)					& 71804		& -			& -					& - \\
Filtered BES (FBES)					& 69380 		& 2195 		& - 					& - \\
Conflict Dataset (CD)				& 2000 		& 539 		& 599 (29.95 \%) 	& 1401 (70.05 \%) \\
\bottomrule
\end{tabular}
\caption{Number of edges, number of unique lemmas and proportion of labels for the different edge sets}
\label{tab:dataset-descriptions}
\end{table}


\section{Evaluation Process}
In this evaluation multiple \textit{evaluation runs} are conducted. Each evaluation run correspond to a SHPM process in which a pattern is matched against the CD. In the case of patterns utilizing NESS this requires that additional parameters in form of an \textit{NESS configuration} are given to the matching process. An evaluation run is described by an \textit{evaluation (run) configuration}. For each evaluation run the \textit{evaluation metrics} are computed. 

\subsection{Evaluation Run Configurations}
An evaluation configuration consists of the following parameters: \todo{add references to chapter 4}
\begin{itemize}
	\item Conflict Pattern
	\item NESS Configuration (in case NESS-SHMP): 
	\begin{itemize}
		\item NESS model
		\item Similarity Threshold 
		\item Use  all tokens (in the case of CNESS) 
		\item Reference Edge Set (in the case of CNESS) 
	\end{itemize}
\end{itemize}

\paragraph{Conflict Patterns}
The four conflict patterns used in this evaluation are described in detail in \cref{sec:sh-patterns}. An overview of the properties of these patterns can be seen in \cref{tab:evaluation-patterns}.


\paragraph{Similarity Thresholds}
In this evaluation the similarity threshold \(t_s\) is always selected from a range of thresholds \(r_t = \{0, 0.01, ...,  0.99, 1.00\}\), i.e. \(t_s \in r_t\). This results in 101 different values of \(t_s\).


\paragraph{Reference Edge Sets}
Multiple \textit{reference edge sets} (RES)  are randomly sampled from the set of edges in the CD, which are labelled as "conflict". These edges are then excluded from the dataset, to avoid introducing data from the test dataset to the system that is being evaluated. To compare the effect of different sample sizes, differently sized sets are drawn. To compare the effect of different samples, different samples are drawn. A RES with ID \textit{N-X} has \(N \in \{1, 3, 10\}\) samples and is from sample draw \(X \in \{1, 2, 3, 4, 5\}\). This results in 15 different RES in total. The specific sets that have been sampled can be seen in \cref{app-sec:ref-edge-sets}.


\subsubsection{Evaluation Run Names}
\label{sec:eval-run-names}
An evaluation run name has the form: \texttt{CP NM-AT r-N-X t-TS}

%\begin{center}
%\texttt{<CP> <NM>-<AT> <r-N-X> <t-TS>}
%\end{center}
In a specific evaluation run name, the placeholders (capitalised letters) are replaced with actual values. Such a name always begins with the conflict pattern (CP) name in its shortened form: \textit{original}, \textit{semsim-fix}, \textit{semsim-fix-lemma} or \textit{semsim-ctx}. In case of a NESS utilizing conflict pattern, the NESS model (NM) name is added in its shortened form: word2vec as \textit{w2v} and conceptnet-numberbatch as \textit{cn}. If the NESS type is CNESS, the usage of the all-tokens (AT) option is indicated by adding \texttt{-at} to the model name. If the option is not used, it is not added. Also in case of CNESS, the reference edge set ID N-X is indicated by appending \texttt{r-N-X}. For all NESS utilizing evaluation runs, the similarity threshold \(t_s\) is indicated by appending \texttt{t-TS}, where \texttt{TS} is the value of \(t_s\).



\subsubsection{Specification of Evaluation Run Configurations}
\label{sec:evaluation-configurations}
The configurations for all evaluation runs that are conducted in this case study are specified in \cref{tab:evaluation-run-configs}. All possible parameter combinations of an evaluation configuration, i.e. the conflict pattern and the NESS parameters, are evaluated. The total number of conducted evaluation runs therefore amounts to 6465.\footnote{1 (original) + 2 (semsim-fix and semxim-fix-lemma) * 2 (FNESS models) * 101 (STs) \\ + 1 (semsim-ctx) * 2 (CNESS models) * 2 (all tokens) * 15 (ref. edge sets) * 101 (STs)} In \cref{tab:evaluation-run-configs} the different values for the reference edge set ID and the ST are omitted. The \textit{random} evaluation run configuration relates to a hypothetical evaluation run in which edges are uniformly randomly matched. 


\subsection{Evaluation Metrics}
\label{sec:evaluation-metrics}
Using the information provided by the dataset labels it is determined whether a match is correct or not. If an edge is matches in a given evaluation run and is labeled as "conflict" in the dataset, it is considered a \textit{true positive} (TP). If an edge matches but is labeled "no conflict", it is considered a \textit{false positive} (FP). The \textit{true negatives} (TN) and \textit{false negatives} (FN) are determined analogously  by examining the non-matching edges. Based on the TP, FP, TN and FN the metrics \textit{precision}, \textit{recall} and \textit{F1-score} are computed. \todo{show how this metrics are computed?}

\paragraph{Relationship of Similarity Threshold and Recall}
It can be generally stated that the recall (\(r\)) of NESS-SHMP in relation to the similarity threshold (\(r(t_s)\)) is strictly monotonically decreasing, since the set of points in embedding space that is inside of the similarity boundary consistently gets smaller with increasing threshold. \todo{maybe add ref. to earlier section}

\todo[inline]{derive why these metrics were chosen: \\
accuracy is not interesting since the dataset is unbalanced, precision and recall both of interest, but are expected to be a trade-off (where recall should decline with rising ST). F1-score is an established metric that closely relates to precision and recall and represents this trade-off and therefore retrieval performance as a whole. MCC is arguably a better metric because it is symmetrical and incorporates true negatives. also the F1-score of the original pattern is worse than random, which indicates a metric mismatch. then again the close relation and equal value range of precision, recall and F1-score are a plus (for plotting especially).
}


\begin{table}
\centering
\begin{tabular}{lcccc}
\toprule
\multicolumn{1}{l}{Pattern name}		& \multicolumn{1}{c}{Lemma}		& \multicolumn{1}{c}{NESS}	& \multicolumn{1}{c}{Includes}		& \multicolumn{1}{c}{Requires} \\
\multicolumn{1}{l}{} 				& \multicolumn{1}{c}{based} 		& \multicolumn{1}{c}{type} 		& \multicolumn{1}{c}{ref. words} 	& \multicolumn{1}{c}{ref. edges} \\
\midrule
Original conflict pattern (\ref{pat:original-conflict})					& Yes 		& - 		& -			& - \\
semsim-fix conflict pattern (\ref{pat:semsim-fix-conflict})				& No		& Fixed		& Yes		& No \\
semsim-fix-lemma conflict (\ref{pat:semsim-fix-lemma-conflict}) 		& Yes 		& Fixed		& Yes		& No \\
semsim-ctx conflict patter (\ref{pat:semsim-ctx-conflict})				& No		& Contextual	& No		& Yes \\
\bottomrule
\end{tabular}
\caption{Properties of the conflict patterns used in the evaluation}
\label{tab:evaluation-patterns}
\end{table}


\begin{table}
\centering
\begin{tabular}{llcc}
\toprule
\multicolumn{1}{l}{Evaluation Run Name} & \multicolumn{1}{l}{Conflict Pattern} & \multicolumn{2}{c}{NESS Configuration} \\
\cmidrule{3-4}
\multicolumn{1}{l}{}   & \multicolumn{1}{l}{}  & \multicolumn{1}{c}{NESS Model}	& \multicolumn{1}{c}{all tokens} \\
\midrule
original                       & original                     & -                          & -                \\
semsim-fix w2v                 & semsim-fix                   & word2vec                   & -                \\
semsim-fix cn                 & semsim-fix                   & conceptnet-numbatch                      & -      \\
semsim-fix-lemma w2v           & semsim-fix-lemma             & word2vec                   & -                \\
semsim-fix-lemma cn           & semsim-fix-lemma             & conceptnet-numbatch                      & -      \\
semsim-ctx e5 r-N-X                & semsim-ctx                   & e5                         & No           \\
semsim-ctx gte r-N-X              & semsim-ctx                   & gte                        & No            \\
semsim-ctx e5-at r-N-X             & semsim-ctx                   & e5                         & Yes          \\
semsim-ctx gte-at r-N-X           & semsim-ctx                   & gte                        & Yes           \\
\end{tabular}
\caption{Evaluation Run Configurations}
\label{tab:evaluation-run-configs}
\end{table}


\section{Evaluation Results}
\label{sec:evaluation-results}
In this section the results of the evaluation runs which are defined by the evaluation configurations in \cref{sec:evaluation-configurations} are examined. Different perspectives on the result data are constructed in the form of tables and plots to enable answering the research questions. The following subsections each represent one perspective and conclude with significant observations that can be made based on it.

\subsubsection{Result Data Description Concepts}
To facilitate constructing insightful perspectives on the result data, some novel concepts for its description are introduced in the following.

\paragraph{Evaluation Run Sets}
Multiple evaluation runs can be grouped into an \textit{evaluation run set} (ERS) according to their shared configuration parameter values. The naming convention for an ERS follows the evaluation run naming convention described in \cref{sec:eval-run-names}. The parameters values that are not shared among the evaluation runs in the ERS are omitted from the name or replaced by the wildcard symbol \texttt{*}. The placeholders (capitalised letters) are used to refer to an ESR of a generic form with fixed parameter values without specifying these values.  By surrounding a part of the ERS name with parentheses \texttt{(*)}, it is indicated that this part is omitted if unsuitable. 

Examples are given to illustrate this:
\begin{itemize}
	\item An ERS of all evaluation runs utilizing NESS with \(t_s = 0.5\) is named: \\ \texttt{semsim-* t-0.5}
	\item An ERS of all evaluation runs utilizing FNESS with an unspecified but fixed NESS model has the form: \\ \texttt{semsim-fix(-*) NM} 
	\item An ERS of all evaluation runs utilizing NESS with all parameters (that are applicable) fixed but unspecified, except for the specific value \(t_s = 0.5\), has the form: \\ \texttt{semsim-* NM(-AT) (r-N-X) t-0.5}
\end{itemize}

\paragraph{Best F1-Score Evaluation run} The \textit{best F1-Score evaluation run} refers to the evaluation run with the highest F1-Score in an ERS corresponding to a NESS utilizing evaluation configuration where every parameter except for \(t_s\) is fixed. Such an ERS is generally named \texttt{semsim-* NM(-AT) (r-N-X)}. The corresponding F1-score is also simply referred to as \textit{best F1-score}.
%This means among the results for all similarity threshold in the given threshold range for a given evaluation run, the ST that results in the highest F1-score is selected. The scores of the other evaluation metrics also correspond to this ST.

\paragraph{Mean Reference Edge Set Evaluation Runs}
A \textit{mean reference edge set evaluation run} is constructed from the mean value of all evaluation scores for the evaluation runs in an ERS of the form \texttt{semsim-ctx NM-AT r-N-* t-TS}. This means for every \(t_s\) the mean of the corresponding evaluation scores of all reference edge sets of the same size is computed. In the following these synthetical evaluation runs are referred to in this form: \texttt{semsim-ctx NM-AT r-N-mean}
%These mean evaluation runs are primarily relevant for visualisation in the result plots (see \cref{sec:eval-metrics-vs-st}).

\paragraph{Mean Reference Edge Set Best F1-Score Evaluation Metric Scores}
The \textit{mean reference edge set (RES) best F1-Score evaluation metric scores} are the mean values of all evaluation scores corresponding to the best F1 score for all evaluation runs in an ERS of the form \texttt{semsim-ctx NM-AT r-N-*}. In the following these evaluation metric scores will be referred to in this form: \texttt{semsim-ctx NM-AT r-N-mean-best}
%For example \textit{semsim-ctx e5-at r-10-best-mean} refers to the mean of best F1 scores for all evaluation runs that use the semsim-ctx conflict pattern, the e5 model with all-tokens enabled and a reference edge set of size ten. 



\subsection{Best F1-Score based Evaluation Run Comparison}
\label{sec:best-f1-score-eval-run-comparison}
\Cref{tab:best-f1-score-mean-best} shows the evaluation scores for all evaluation metrics of the best F1-score evaluation runs for the original conflict pattern evaluation run and all evaluation runs utilizing FNESS. For the evaluation runs utilizing CNESS only the mean RES best F1-score evaluation metric scores and the standard deviation of the best F1-scores for the corresponding ERSs are shown. The \(t_s\) value listed for the mean RES best F1 score evaluation metrics is the mean of all \(t_s\) values for the best F1-scores in the corresponding ERSs.

In \cref{tab:best-f1-score-mean-best-semsim-ctx} and \cref{tab:best-f1-score-mean-best-semsim-ctx-at} of \cref{app-sec:result-tables} the best F1-score evaluation run results can be seen for evaluation runs utilizing CNESS wit all tokens disabled and enabled respectively. These tables also list the hypothetical best F1-score evaluation for run the mean reference edge set evaluation runs. Additionally the mean standard deviation for these ERSs is shown, i.e. the mean of the standard deviations of the F1-score for every ERS of the form \texttt{semsim-ctx NM-AT r-N-* t-TS}.


\subsubsection{Significant Observations}
\begin{enumerate}[label=\arabic{listcounter}.\arabic*]
\refstepcounter{listcounter}% Increment the list counter
%	\item The random evaluation run achieves a higher F1-score than the original conflict pattern evaluation run
	\item All evaluation runs utilizing NESS achieve a best F1-score that is higher than the F1-score of the random evaluation run and the original evaluation run \label{obs-itm:NESS-higher-best-f1}
	\item CNESS achieves achieves a higher F1-score than FNESS by 4.0\%, when comparing the highest F1-scores achieved among all FNESS utilizing evaluation runs and the highest mean RES best F1 score achieved among all CNESS utilizing evaluation runs (\texttt{semsim-fix-lemma cn t-0.30} and \texttt{semxim-ctx e5 r-10-mean-best} \label{obs-itm:CNESS-higher-best-f1-than-FNESS}
	\item Lemma based FNESS achieves a higher F1-score than non-lemma FNESS by 3.2\%, when comparing the highest F1-scores achieved by evaluation runs utilizing one of the two variants (\texttt{semsim-fix w2v t-0.27} and \texttt{semsim-fix-lemma cn t-0.30}) \label{obs-itm:lemma-based-FNESS-higher-best-f1}
	\item For lemma based FNESS, the conceptnet-numberbatch model achieves a higher best F1-score than the word2vec model by 4.2\% \\ (\texttt{semsim-fix-lemma cn t-0.30} vs \texttt{semsim-fix-lemma w2v t-0.33}) \label{obs-itm:lemma-FNESS-cn-better-than-w2v}
	\item For not lemma based FNESS, the word2vec model achieves a higher best F1-score than the conceptnet-numberbatch model by 1.4\% \\ (\texttt{semsim-fix w2v t-0.27} vs \texttt{semsim-fix cn t-0.25}) \label{obs-itm:word-FNESS-w2v-better-than-cn}
	\item CNESS with the AT option disabled achieves a higher or equal mean RES best F1-score than CNESS with AT option enabled in 6/6 (100\%) direct comparisons \\(\texttt{semsim-ctx NM r-N-mean-best} vs \texttt{semsim-ctx NM-at r-N-mean-best}) \label{obs-itm:CNESS-better-without-AT}
	\item CNESS with the e5 model achieves a higher or equal mean RES best F1-score than CNESS with the gte model in 6/6 (100\%) direct comparisons \\ (\texttt{semsim-ctx e5-* r-N-mean-best} vs \texttt{semsim-ctx gte-* r-N-mean-best}) \label{obs-itm:CNESS-e5-better-than-gte}
	\item CNESS with the AT option enabled has a lower standard deviation of best F1-score than CNESS with the AT option disabled in 5/6 (83\%) direct comparisons \\ (\texttt{semsim-ctx NM-at r-N-mean-best} vs \texttt{semsim-ctx NM r-N-mean-best}) \label{obs-itm:CNESS-lower-variation-with-AT}
%	\item CNESS with the gte model has a lower standard deviation of best F1-score than CNESS with the e5 model in 4/6 (67\%) direct comparisons \\ (\texttt{semsim-ctx gte-* r-N-mean-best} vs \texttt{semsim-ctx e-* r-N-mean-best})
%	\item For CNESS with the AT option disabled, the gte model has a lower standard deviation of best F1-score than the e5 model in 3/3 (100\%) direct comparisons \\ (\texttt{semsim-ctx gte r-N-mean-best} vs \texttt{semsim-ctx e5 r-N-mean-best})
\end{enumerate}


\begin{table}
\centering
\begin{tabular}{lllrrrrrr}
\toprule
\multicolumn{4}{l}{Evaluation Run Name} & \multicolumn{1}{c}{Prec.} & \multicolumn{1}{c}{Rec.} & \multicolumn{2}{c}{(Best) F1-Score}\\
\cmidrule{1-4}\cmidrule{7-8}
\multicolumn{1}{l}{CP} & \multicolumn{1}{l}{NM} & \multicolumn{1}{l}{RES} & \multicolumn{1}{c}{\(t_s\)} & \multicolumn{3}{l}{} & \multicolumn{1}{c}{Std. Dev.} \\
\midrule
random &  &  & - & 0.300 & 0.500 & \textbf{0.375} & - \\
\hline
original &  &  & - & 0.706 & 0.209 & \textbf{0.322} & - \\
\hline
semsim-fix & cn &  & 0.25 & 0.479 & 0.524 & 0.500 & - \\
semsim-fix & w2v &  & 0.27 & 0.483 & 0.533 & \textbf{0.507} & - \\
\hline
semsim-fix-l. & cn &  & 0.30 & 0.492 & 0.558 & \textbf{0.523} & - \\
semsim-fix-l. & w2v &  & 0.33 & 0.460 & 0.553 & 0.502 & - \\
\hline
semsim-ctx & e5 & r-1-mean-best & 0.65 & 0.392 & 0.772 & 0.518 & +/- 0.025 \\
semsim-ctx & gte & r-1-mean-best & 0.59 & 0.336 & 0.879 & 0.483 & +/- 0.025 \\
semsim-ctx & e5 & r-3-mean-best & 0.68 & 0.399 & 0.818 & 0.536 & +/- 0.021 \\
semsim-ctx & gte & r-3-mean-best & 0.65 & 0.365 & 0.799 & 0.499 & +/- 0.016 \\
semsim-ctx & e5 & r-10-mean-best & 0.72 & 0.416 & 0.790 & \textbf{0.544} & +/- 0.020 \\
semsim-ctx & gte & r-10-mean-best & 0.68 & 0.382 & 0.812 & 0.517 & +/- \textbf{0.010} \\

\hline
semsim-ctx & e5-at & r-1-mean-best & 0.69 & 0.369 & 0.841 & 0.509 & +/- 0.016 \\
semsim-ctx & gte-at & r-1-mean-best & 0.66 & 0.335 & 0.882 & 0.483 & +/- 0.021 \\
semsim-ctx & e5-at & r-3-mean-best & 0.72 & 0.378 & 0.821 & 0.516 & +/- 0.011 \\
semsim-ctx & gte-at & r-3-mean-best & 0.70 & 0.336 & 0.876 & 0.485 & +/- 0.017 \\
semsim-ctx & e5-at & r-10-mean-best & 0.74 & 0.382 & 0.843 & \textbf{0.525} & +/- 0.012 \\
semsim-ctx & gte-at & r-10-mean-best & 0.72 & 0.338 & 0.900 & 0.491 & +/- \textbf{0.008} \\
\bottomrule
\end{tabular}
\caption{Evaluation scores corresponding to best F1-scores for all evaluation runs}
\label{tab:best-f1-score-mean-best}
\end{table}

\subsection{Evaluation Metric vs. Similarity Threshold}
\label{sec:eval-metrics-vs-st}

These plots visualise the resulting evaluation scores for the different evaluation metrics in relation to different values for the similarity threshold. 

\Cref{fig:best-f1-per-pattern} shows the F1-score vs. the ST for the best performing evaluation runs for each conflict pattern. That means for every conflict pattern, this shows the evaluation run(s) with the configuration that resulted in the highest best F1-score. For the \texttt{random} and \texttt{original} evaluation runs, there is obviously no configuration to choose from. 

For the FNESS utilizing evaluation runs, the evaluation run with the highest F1-score in the ERSs of the form \texttt{semsim-fix(-lemma) NM} is selected. This means for the semsim-fix and semxim-fix-lemma conflict patterns the corresponding best F1-score evaluation runs of the best performing model are selected..

For the CNESS utilizing evaluation runs, the evaluation runs which correspond to highest mean RES best F1-score are selected. This means the ERS of the form \texttt{semsim-ctx NM-AT r-N-*} which resulted in the highest mean value of best F1-scores. This ERS consists of five evaluation runs, wich each have the form \texttt{semsim-ctx NM-AT r-N-X}. The F1-scores for these runs are plotted with a lighter curve. The additional synthetical \texttt{semsim-ctx NM-AT r-N-mean} score is plotted with a normally bold curve. 

% All evaluation runs  evaluation runs correspond to those highlighted by bold letters in \cref{tab:best-f1-score-mean-best}.

\Cref{fig:prec-rec-best-semsim} follows the same concept as \cref{fig:best-f1-per-pattern}, but instead of the F1-score this plot shows the scores of precision and recall vs. the ST. Also in the selection of evaluation runs, the semsim-fix (not lemma) conflict pattern based evaluation runs are excluded.

The plots in this section are selected because they are deemed to be most relevant for the following. A more comprehensive comparison of the different evaluation runs from this perspective can be found in \cref{app-sec:eval-metric-vs-st-plots}.

%This result in lines parallel to the x-axis for the original conflict pattern, since it does not depend on a ST. It can therefore be seen as a baseline.

\paragraph{Active Similarity Threshold Range}
To facilitate the description of observation for this perspective on the result data, the concept of the \textit{active similarity threshold range} (ASTR) is introduced. For a given ESR of the form \texttt{semsim-* NM(-AT) (r-N-X)}, the ASTR describes the range of \(t_s\) for which the recall (\(r\)) that is achieved by these evaluation runs is \(r \neq r_{max} = 1 \) and \(r \neq r_{min}\). Here \(r_{min}\) and \(r_{max}\) are the lowest and highest recall values, which correspond to \(t_{s1}\) <= \(t_{s2}\), since the function \(r(t_s)\) is monotonically decreasing.


\subsubsection{Significant Observations}
\begin{enumerate}[label=\arabic{listcounter}.\arabic*]
\refstepcounter{listcounter}% Increment the list counter
%	\item The best F1-score of the synthetic evaluation runs \texttt{semsim-ctx e5 r-10-mean} is higher than the highest F1 score in achieved by any of the evaluation runs in \texttt{semsim-fix-*}.
	\item The ASTR of \texttt{semsim-fix-lemma cn} is larger (\(0.0 < t_s < 1.0\)) than the ASTR of \texttt{semsim-ctx e5} (ca. \(0.625 < t_s < 0.875\)) \label{obs-itm:ASTR-lemma-FNESS-cn-higher-than-CNESS-e5}
	\item Generally the ASTR of ERSs of the form \texttt{semsim-fix-* NM} is larger than the the ASTR of ERSs of the form \texttt{semsim-ctx} (confer also \cref{fig:prec-rec-f1-semsim-fix-lemma}, \cref{fig:prec-rec-f1-semsim-fix-lemma-model} \cref{fig:prec-rec-f1-semsim-ctx-model} and \cref{fig:prec-rec-f1-semsim-ctx-at}) \label{obs-itm:ASTR-FNESS-higher-than-CNESS}
	\item The ASTRs are nearly equal for ERSs of the form \texttt{semsim-fix-* NM} \label{obs-itm:ASTR-equal-FNESS} (confer \cref{fig:prec-rec-f1-semsim-fix-lemma} and \cref{fig:prec-rec-f1-semsim-fix-lemma-model}) \label{obs-itm:ASTR-FNESS-equal}
	\item The ASTR of ERSs of the form \texttt{semsim-ctx gte-AT} begin at a lower value than for ERSs of the form \texttt{semsim-ctx e5-AT} (confer \cref{fig:prec-rec-f1-semsim-ctx-model} and \cref{fig:f1-semsim-ctx-model-at}) \label{obs-itm:ASTR-CNESS-gte-starts-earlier}
	\item The ASTR of ERSs of the form \texttt{semsim-ctx NM} begin at a lower value than for ERSs of the form \texttt{semsim-ctx NM-at} (confer \cref{fig:prec-rec-f1-semsim-ctx-at} and \cref{fig:f1-semsim-ctx-model-at}) \label{obs-itm:ASTR-CNESS-at-starts-earlier}
	\item The ASTRs end at nearly the same value for ERSs of the form \texttt{semsim-ctx NM-AT} \label{obs-itm:ASTR-equal-FNESS} (confer \cref{fig:prec-rec-f1-semsim-ctx-model}, \cref{fig:prec-rec-f1-semsim-ctx-at} and \cref{fig:f1-semsim-ctx-model-at}) \label{obs-itm:ASTR-CNESS-end-equal}

	\item The evaluation runs in the ERS \texttt{semsim-fix-lemma cn} achieve a higher F1 value than the evaluation run in the ERS \texttt{semsim-fix w2v} for nearly all values of \(t_s\) \label{obs-itm:lemma-FNESS-higher-f1-nearly-always}

	\item The precision of the evaluation run which achieves the highest precision among those in the synthetic ESR \texttt{semsim-ctx e5 r-10-mean} (\(p_\text{max}\)) and the precision of the original evaluation run (\(p_\text{og}\)) are nearly equal (\(p_\text{max} \approx p_\text{og}\)) \label{obs-itm:CNESS-highest-precision-equal-original} 	
	\item The precision of evaluation run \texttt{semsim-fix-lemma cn}  correlates with the ST until it reaches the value achieved by the \texttt{original} evaluation run, where it plateaus \label{obs-itm:FNESS-ST-precision-correlates}
	\item The precision of the evaluation runs in ERS \texttt{semsim-ctx e5} correlate with the ST until precision (\(p\)) and recall (\(r\)) reach approximately the same value (\(p \approx r \approx 0.5\)), after which it fluctuates (the specific fluctuation varies with the specific RES) \label{obs-itm:CNESS-ST-precision-correlates-until-crosspoint}

	\item The precision achieved by evaluation runs in ERSs of the form \texttt{semsim-ctx e5 r-10-* t-TS} has a higher variation for \(t_s \geq 0.75\) than for \(t_s < 0.75\) \label{obs-itm:CNESS-precision-variation-higher-ASTR}
	\item The precision of the evaluation run which achieves the highest precision among those in the synthetic ESR \texttt{semsim-ctx e5 r-10-mean} (\(p_\text{max}\)) and the precision of the original evaluation run (\(p_\text{og}\)) are nearly equal (\(p_\text{max} \approx p_\text{og}\)) \label{obs-itm:CNESS-highest-precision-equal-original} 	
	\item The evaluation metric scores of \texttt{semsim-fix-lemma cn t-1.00} are lower than those of the \texttt{original} evaluation run \label{obs-itm:FNESS-ST-limit}

%	\item Precision (\(p\)) and recall (\(r\)) have the same value (\(p \approx r \approx 0.5\)) at specific similarity thresholds \(t_1\) and \(t_2\) for the two ERSs \texttt{semsim-fix-lemma cn} and \texttt{semsim-ctx e5}

\end{enumerate}
 
 
\todo[inline]{should i quantify all these observations?}
 
\todo[inline]{would a more detailed analysis of precision and recall make sense? maybe a precision-recall curve or an roc? maybe recall at best precision and precision at random recall?}
 
\begin{figure}
 \centering
 \begin{subfigure}{\textwidth}
	\includegraphics[width=\textwidth]{dataset_conflicts_1-2_pred_wildcard_subsample-2000_evaluation_original_vs_semsim-fix_w2v_vs_semsim-fix-lemma_cn_vs_semsim-ctx_nref-10_e5_f1}
	\caption{F1-score vs. ST for the evaluation runs \texttt{random}, \texttt{original}, \texttt{semsim-fix w2v}, \texttt{semsim-fix-lemma cn} and \texttt{semsim-ctx e5 r-10-X}}
	\label{fig:best-f1-per-pattern}
 \end{subfigure}
 \newline
 \newline
 \begin{subfigure}{\textwidth}
	\includegraphics[width=\textwidth]{dataset_conflicts_1-2_pred_wildcard_subsample-2000_evaluation_original_vs_semsim-fix-lemma_cn_vs_semsim-ctx_nref-10_e5_precision-recall}
	\caption{Precision and  recall vs. ST for the evaluation runs \texttt{random}, \texttt{original}, \texttt{semsim-fix-lemma cn} and \texttt{semsim-ctx e5 r-10-X}}
	\label{fig:prec-rec-best-semsim}
 \end{subfigure}
 \caption{Evaluation metric scores vs. similarity threshold values}
\end{figure}
 
%\begin{figure}[hp]
%\centering
%\includegraphics[width=\textwidth]
%%\includegraphics[width=0.85\paperwidth, center]
%{dataset_conflicts_1-2_pred_wildcard_subsample-2000_evaluation_original_vs_semsim-fix_w2v_vs_semsim-fix-lemma_cn_vs_semsim-ctx_nref-10_e5_f1}
%\caption{F1-score vs. ST for the best performing evaluation run configuration for each conflict pattern (including the original pattern)}
%\label{fig:best-f1-per-pattern}
%\end{figure}
%
%\begin{figure}[hp]
%\centering
%%\includegraphics[width=\textwidth]
%\includegraphics[width=\textwidth]
%{dataset_conflicts_1-2_pred_wildcard_subsample-2000_evaluation_original_vs_semsim-fix-lemma_cn_vs_semsim-ctx_nref-10_e5_precision-recall}
%\caption{Precision and  recall vs. ST for the evaluation runs \texttt{original}, \texttt{semsim-fix-lemma cn} and \texttt{semsim-ctx e5 r-10-X}}
%\label{fig:prec-rec-best-semsim}
%\end{figure}


%\begin{figure}
%\centering
%\includegraphics[width=\textwidth]{dataset_conflicts_1-2_pred_wildcard_subsample-2000_evaluation_original_vs_semsim-ctx_nref-10_e5_vs_gte-at_precision-recall}
%\caption{Caption for this figure}
%\label{fig:unique-label-2}
%\end{figure}


\subsection{Best F1-Score vs. Number of Reference Edges}
\label{sec:best-f1-vs-nref}
\Cref{fig:best-f1-vs-nref} visualises the relation of the number of reference edges and the best-F1 score. For this purpose the number of reference edges \texttt{N} is plotted versus the mean RES best F1-score for all ERSs of the form \texttt{semsim-ctx NM-AT r-N-*}.  The standard deviation of the best F1 scores for these ERSs is visualised by the shaded areas around the curves of the mean RES best F1-scores.

\begin{figure}
\centering
\includegraphics[width=\textwidth]
{/dataset_n_ref_edges/dataset_conflicts_1-2_pred_wildcard_subsample-2000_best-f1_nref_vs_f1}
\caption{Mean reference edge set best F1-score for the ERSs \texttt{semsim-ctx e5 r-N-*}, \texttt{semsim-ctx e5-at r-N-*}, \texttt{semsim-ctx gte-at r-N-*} and \texttt{semsim-ctx gte-at r-N-*}}
\label{fig:best-f1-vs-nref}
\end{figure}

\subsubsection{Significant Observations}
\begin{enumerate}[label=\arabic{listcounter}.\arabic*]
\refstepcounter{listcounter}% Increment the list counter
	\item The mean RES best F1-score of the evaluation runs in an ERS of the form \texttt{semsim-ctx NM(-AT) r-\(N_1\)-*} is higher than the mean RES best F1-score of the evaluation runs in an ERS of the form \texttt{semsim-ctx NM(-AT) r-\(N_2\)-*}, if \(N_2 > N_1\) for \(N_1, N_2 \in \{1, 3, 10\}\) \label{obs-itm:CNESS-more-refs-higher-mean-best-f1}
	\item The standard deviation of the mean RES best F1-score of the evaluation runs in an ERS of the form \texttt{semsim-ctx NM(-AT) r-\(N_1\)-*} is lower than the standard deviation of the mean RES best F1-score of the evaluation runs in an ERS of the form \texttt{semsim-ctx NM(-AT) r-\(N_2\)-*}, if \(N_2 > N_1\) for \(N_1, N_2 \in \{1, 3, 10\}\), except for \texttt{semsim-ctx et-at r-3-*} (\(sd_1 = 0.11\)) and \texttt{semsim-ctx et-at r-10-*} (\(sd_2 = 0.12\)) \label{obs-itm:CNESS-more-refs-lower-stddev-best-f1}
%	\item , where \(sd_2 > sd_2\)
\end{enumerate}



\subsection{Predicate Lemma based Evaluation Run Comparison}
In this section it is explored how the different NESS systems differ in which edges they match. This is done by following up on the concept of the predicate lemma introduced in \cref{sec:base-edge-set}. In \cref{sec:dataset-characteristics} one of the two desired characteristics of the dataset states that it should contain the largest possible number of edges per unique predicate lemma. Specifically it is of interest, how the CNESS system performs in comparison to the FNESS system for subsets of edges which share the same predicate lemma. Such a subset of edges of the conflict dataset is in the following referred to as \textit{predicate lemma edge set} (PLES).


%\subsubsection{Evaluation Runs Representing the NESS System Types}
\paragraph{NESS Type Representatives}
Two evaluation runs are selected to represent the two versions of the NESS systems for the comparison. The lemma-based version of FNESS is chosen here, because of its superior performance regarding best F1-score that is observed in \cref{sec:best-f1-score-eval-run-comparison} and \cref{sec:eval-metrics-vs-st}. Specifically, the evaluation runs \texttt{semsim-fix-lemma cn t-0.30} (evaluation run \textit{A}) and \texttt{semsim-ctx e5 r-10-2 t-0.72} (evaluation run \textit{B}) are selected as representatives of the FNESS and CNESS system respectively. These are the best performing evaluation runs regarding F1-score for the respective semsim conflict patterns (i.e. NESS system), which can be seen in \cref{tab:best-f1-score-mean-best} and \cref{tab:best-f1-score-mean-best-semsim-ctx}.\footnote{
The latter table specifically shows that the evaluation runs \texttt{semsim-ctx e5 r-1-2 t-0.67}, \texttt{semsim-ctx e5 r-10-2 t-0.72} and \texttt{semsim-ctx e5 r-10-4 t-0.72} all correspond to an F1 score \(s_\text{F1} = 0.56\). The evaluation runs utilizing the RESs of size \(N = 10\) are chosen over the one utilizing an RES of size \(N = 1\), because of their generally superior performance regarding F1 score, which is observed in \cref{sec:best-f1-vs-nref}. Among the two remaining evaluation runs, \texttt{semsim-ctx e5 r-10-2 t-0.72} is selected randomly.


%While the lemma-based FNESS system is only able to match all or none of the edges in a PLES, the CNESS system is potentially able to differentiate between "conflict" and "no conflict" given a PLES.
%It shall be noted there that the FNESS system is able to differentiate between different verb forms forms corresponding to the same lemma, when using the semsim-fix and not the semsim-fix-lemma functional pattern. Nonetheless this is assumed to add relevant semantic differentiation capabilities to the FNESS system based on the observed 

}

\subsubsection{PLES Evaluation Score based Evaluation Run Comparison}

\paragraph{Label Balance Ratio}
The \textit{label balance ratio} (LBR) measures how balanced the labels in a given set of labelled edges are. It is calculated by \cref{eq:label-balance-ratio}. Here \(n_\text{pos}\) and \(n_\text{neg}\) are the number of positively ("conflict") and negatively ("no conflict") labeled edges in the edge set.
An edge set with fully balanced labels has \(LBR = 1\) and completely unbalanced labeled edge set has \(LBR = 0\).

\begin{equation}
LBR = 1 - \left(\frac{\left|n_{\text{pos}} - n_{\text{neg}}\right|}{n_{\text{pos}} + n_{\text{neg}}}\right)
\label{eq:label-balance-ratio}
\end{equation}

The evaluation metrics are computed for evaluation run A and B for each PLES, along with metrics that measure distributional properties of a PLES. Namely the number of edges \(n_e\), the number of positively labeled and negatively labeled edges (\(n_\text{pos}\) and \(n_\text{neg}\)), the LBR  and the entropy of a PLES.

\Cref{tab:predicate-lemma-highest-f1} lists the ten predicate lemmas for whose PLES the absolute difference in F1-score, which is achieved in the two evaluation runs, is the highest. Conversely \cref{tab:predicate-lemma-lowest-f1} lists the ten predicate lemma for whose PLES the difference in F1-score which is achieved in the tow evaluation runs is the lowest. In both tables the predicate lemmas have been filtered beforehand, so that only PLES with \(n_s >= 5\) samples are considered. Additionally the recalls (\(r_A, r_B\)) achieved by both evaluation runs regarding a PLES must fulfil the condition that \(r_A + r_B > 0\), i.e. at least the recall achieved by one of the evaluation runs must be non-zero. \Cref{tab:predicate-lemma-highest-f1-both-recall-non-zero} follows the same concept as \cref{tab:predicate-lemma-highest-f1}, except for the condition regarding the recalls being \(r_A * r_B > 0\), i.e. both recalls achieved by the tow evaluation runs must be non-zero. This second variant of the table is not shown for \cref{tab:predicate-lemma-lowest-f1}, because it only lists lemmas for whose PLES the F1-score achieved by both evaluation runs is zero.

\todo[inline]{should I add a table with the actual labels produced by the evaluation runs?}
%An comparison of the actual labels that were produced  by evaluation runs A and B for all PLES corresponding to the lemmas in \cref{tab:predicate-lemma-highest-precision} can be found in \cref{app-sec:ples-eval-labels}.
%specifically in \cref{tab:ples-labels-1}, \cref{tab:ples-labels-2}, \cref{tab:ples-labels-3}, \cref{tab:ples-labels-4}, \cref{tab:ples-labels-5}, \cref{tab:ples-labels-6} and \cref{tab:ples-labels-7} 

%\Cref{tab:predicate-lemma-f1} lists the ten predicate lemmas for whose PLES the best F1-scores are achieved in both of the two evaluation runs.

%\begin{table}[hp]
%\centering
%\begin{tabular}{lrr|lrr}
%\toprule
%\multicolumn{3}{c|}{\texttt{semsim-fix-lemma cn} at \(t_s = 0.30\)} & \multicolumn{3}{c}{\texttt{semsim-ctx e5 r-10-2} at \(t_s = 0.72\)} \\ 
%\midrule
%Lemma      & F1-score & Num. of Samples & Lemma      & F1-score & Num. of Samples \\
%\midrule
%arrest     & 1.00     & 11            & arrest     & 1.00     & 11            \\
%accuse     & 0.97     & 33            & file       & 1.00     & 5             \\
%warn       & 0.93     & 24            & attack     & 1.00     & 12            \\
%slam       & 0.92     & 7             & accuse     & 0.95     & 33            \\
%attack     & 0.91     & 12            & slam       & 0.92     & 7             \\
%criticize  & 0.91     & 6             & criticize  & 0.91     & 6             \\
%detain     & 0.89     & 5             & shoot      & 0.89     & 5             \\
%shoot      & 0.89     & 5             & order      & 0.88     & 14            \\
%condemn    & 0.84     & 25            & detain     & 0.86     & 5             \\
%dismiss    & 0.80     & 6             & launch     & 0.86     & 14            \\ 
%\bottomrule
%\end{tabular}
%\caption{Top ten lemmas regarding F1-score with number of samples per lemma \(n_s >= 5\) \\
%for the evaluation runs \texttt{semsim-fix-lemma cn t-0.30} and \texttt{semsim-ctx e5 r-10-2 t-0.72}}
%\label{tab:predicate-lemma-f1}
%\end{table}
%

\begin{table}[p]
\centering
\begin{tabular}{lccccccc}
\toprule
Lemma      & F1 Diff. & F1 A & F1 B & \(n_e\) & \(n_\text{pos}\)/\(n_\text{neg}\) & LBR & Entropy \\
\midrule
file       & 1.00      & 0.00           & 1.00           & 5               & 5/0     & 0.00 & 0.00 \\
order      & 0.88      & 0.00           & 0.88           & 14              & 7/7     & 1.00 & 1.00 \\
launch     & 0.86      & 0.00           & 0.86           & 14              & 8/6     & 0.86 & 0.99 \\
step       & 0.80      & 0.00           & 0.80           & 5               & 2/3     & 0.80 & 0.97 \\
target     & 0.77      & 0.00           & 0.77           & 8               & 6/2     & 0.50 & 0.81 \\
use        & 0.75      & 0.00           & 0.75           & 14              & 5/9     & 0.71 & 0.94 \\
block      & 0.73      & 0.00           & 0.73           & 8               & 4/4     & 1.00 & 1.00 \\
take       & 0.71      & 0.00           & 0.71           & 19              & 6/13    & 0.63 & 0.90 \\
open       & 0.67      & 0.00           & 0.67           & 8               & 1/7     & 0.25 & 0.54 \\
suspend    & 0.67      & 0.00           & 0.67           & 6               & 2/4     & 0.67 & 0.92 \\
build      & 0.67      & 0.00           & 0.67           & 6               & 1/5     & 0.33 & 0.65 \\
\cmidrule{7-8}
\multicolumn{6}{l}{} & \multicolumn{2}{c}{mean} \\
\multicolumn{6}{l}{} & 0.61 & 0.79 \\
\bottomrule
\end{tabular}
\caption{Top ten lemmas regarding the highest difference in F1-score between the evaluation runs with recalls \(r_A + r_B > 0\) and number of samples per lemma \(n_s >= 5\) for the evaluation runs \texttt{semsim-fix-lemma cn t-0.30} (A) and \texttt{semsim-ctx e5 r-10-2 t-0.72} (B)}
\label{tab:predicate-lemma-highest-f1}
\end{table}

\begin{table}[p]
\centering
\begin{tabular}{lccccccc}
\toprule
Lemma      & F1 Diff. & F1 A & F1 B & \(n_e\) & \(n_\text{pos}\)/\(n_\text{neg}\) & LBR & Entropy \\
\midrule
accept     & 0.42      & 0.25           & 0.67           & 7               & 1/6     & 0.29 & 0.59 \\
strike     & 0.30      & 0.80           & 0.50           & 9               & 6/3     & 0.67 & 0.92 \\
capture    & 0.22      & 0.44           & 0.67           & 7               & 2/5     & 0.57 & 0.86 \\
seize      & 0.17      & 0.67           & 0.50           & 12              & 6/6     & 1.00 & 1.00 \\
deny       & 0.17      & 0.50           & 0.67           & 6               & 2/4     & 0.67 & 0.92 \\
suggest    & 0.17      & 0.33           & 0.50           & 5               & 1/4     & 0.40 & 0.72 \\
warn       & 0.12      & 0.93           & 0.81           & 24              & 21/3    & 0.25 & 0.54 \\
claim      & 0.12      & 0.43           & 0.55           & 22              & 6/16    & 0.55 & 0.85 \\
attack     & 0.09      & 0.91           & 1.00           & 12              & 10/2    & 0.33 & 0.65 \\
threaten   & 0.09      & 0.52           & 0.61           & 20              & 7/13    & 0.70 & 0.93 \\
approve    & 0.08      & 0.12           & 0.20           & 16              & 1/15    & 0.12 & 0.34 \\
\cmidrule{7-8}
\multicolumn{6}{l}{} & \multicolumn{2}{c}{mean} \\
\multicolumn{6}{l}{} & 0.50 & 0.76 \\
\bottomrule
\end{tabular}
\caption{Top ten lemmas regarding the highest difference in F1-score between the evaluation runs with recalls \(r_A \cdot r_B > 0\) and number of samples per lemma \(n_s >= 5\) for the evaluation runs \texttt{semsim-fix-lemma cn t-0.30} (A) and \texttt{semsim-ctx e5 r-10-2 t-0.72} (B)}
\label{tab:predicate-lemma-highest-f1-both-recall-non-zero}
\end{table}


\begin{table}[htp]
\centering
\begin{tabular}{lccccccc}
\toprule
Lemma      & F1 Diff. & F1 A & F1 B & \(n_e\) & \(n_\text{pos}\)/\(n_\text{neg}\) & LBR & Entropy \\
\midrule
arrest     & 0.00      & 1.00           & 1.00           & 11              & 11/0    & 0.00 & 0.00 \\
slam       & 0.00      & 0.92           & 0.92           & 7               & 6/1     & 0.29 & 0.59 \\
criticize  & 0.00      & 0.91           & 0.91           & 6               & 5/1     & 0.33 & 0.65 \\
shoot      & 0.00      & 0.89           & 0.89           & 5               & 4/1     & 0.40 & 0.72 \\
condemn    & 0.00      & 0.84           & 0.84           & 25              & 18/7    & 0.56 & 0.86 \\
dismiss    & 0.00      & 0.80           & 0.80           & 6               & 4/2     & 0.67 & 0.92 \\
tell       & 0.01      & 0.49           & 0.50           & 28              & 9/19    & 0.64 & 0.91 \\
accuse     & 0.02      & 0.97           & 0.95           & 33              & 31/2    & 0.12 & 0.33 \\
kill       & 0.02      & 0.66           & 0.64           & 77              & 38/39   & 0.99 & 1.00 \\
say        & 0.03      & 0.45           & 0.48           & 31              & 9/22    & 0.58 & 0.87 \\
\cmidrule{7-8}
\multicolumn{6}{l}{} & \multicolumn{2}{c}{mean} \\
\multicolumn{6}{l}{} & 0.46 & 0.68 \\
\bottomrule
\end{tabular}
\caption{Top ten lemmas regarding the lowest absolute difference in F1-score between the evaluation runs with recalls \(r_A + r_B > 0\) and number of samples per lemma \(n_s >= 5\) for the evaluation runs \texttt{semsim-fix-lemma cn t-0.30} (A) and \texttt{semsim-ctx e5 r-10-2 t-0.72} (B)}
\label{tab:predicate-lemma-lowest-f1}
\end{table}

\subsubsection{Significant Observations}
\begin{enumerate}[label=\arabic{listcounter}.\arabic*]
\refstepcounter{listcounter}% Increment the list counter
	\item The CNESS utilizing evaluation run achieves a higher F1-score than the FNESS utilizing evaluation run for every PLES of the the top ten PLES regarding highest difference in F1-score between the two evaluation runs (independently of the recall condition) \label{obs-itm:CNESS-higher-f1-for-highest-f1-difference}
	\item The mean LBR and mean entropy of the top ten PLESs regarding the highest difference in F1-score between the two evaluation runs are higher than the mean LBR and mean entropy of the top ten PLESs regarding the lowest difference in F1-score between the two evaluation runs (independently of the recall condition) \label{obs-itm:PLES-more-diverse-for-highest-f1-difference}
	\item The differences in F1-score achieved by the two evaluation runs is higher for the recall condition \(r_A + r_B > 0\) than for \(r_A \cdot r_B > 0\), because for the first condition the F1-score of the FNESS evaluation run is zero for all PLESs \label{obs-itm:FNESS-zero-recall-highest-f1-difference}
	\item The lemmas corresponding to the the top ten PLES regarding highest difference in F1-score between the two evaluation runs are all not included in the conflict verbs: "accuse", "arrest", "clash", "condemn", "kill", "slam" \label{obs-itm:conflict-verbs-not-in-highest-f1-difference}
	\item Of the lemmas corresponding to  the top ten PLES regarding lowest difference in F1-score between the two evaluation runs, five of six are included in the conflict verbs: "arrest", "condemn", "kill", "slam" ("clash" is not included) \label{obs-itm:conflict-verbs-in-lowest-f1-difference}
%	\item Of the lemmas corresponding the to the the top ten PLES regarding highest difference in F1-score given the recall condition \(r_A + r_B > 0\), most are intuitively not considered to clearly relate to the definition of conflict
%	\item Of the lemmas corresponding the to  the top ten PLES regarding highest difference in F1-score given the recall condition \(r_A \cdot r_B > 0\) and of the lemmas corresponding the to the top ten PLES regarding lowest difference in F1-score, most are intuitively considered to clearly relate to the definition of conflict


\end{enumerate}


\section{Result Discussion}
\label{sec:result-discussion}
In this section the previously presented evaluation results are discussed. It synthesises the major insights derived from the observations of the different result data perspectives. The discussion is organised into categories and sub-categories, which relate to the research questions outlined in \cref{sec:research-questions}. Here retrieval performance generally refers to joined measure of precision and recall and therefore means F1-score, as stated above in \cref{sec:evaluation-metrics}. 

\todo[inline]{Add statement about limitations to specific dataset and unknown generalisability}
\todo[inline]{Missing observations and findings regarding "no breakpoints" (in recall or general?)}

\subsection{Retrieval Performance Improvement}
\label{sec:result-retrieval-performance-improvement}

\begin{itemize}

\item NESS-SHMP can achieve a better retrieval performance than the original conflict pattern, independent of NESS type and configuration (depending on the ST) \\
\textbf{Supporting Observations:} \Cref{obs-itm:NESS-higher-best-f1}

\item CNESS-SHPM using the sub tokens embedding can achieve the overall best retrieval performance (in comparison to FNESS-SHMP and the original conflict pattern) \\
\textbf{Supporting Observations:} \Cref{obs-itm:NESS-higher-best-f1}, \Cref{obs-itm:CNESS-higher-best-f1-than-FNESS}

\item Using lemma-based FNESS-SHMP instead of word-based FNESS-SHMP can achieve a better retrieval performance \\
\textbf{Supporting Observations:} \Cref{obs-itm:lemma-based-FNESS-higher-best-f1}, \Cref{obs-itm:lemma-FNESS-higher-f1-nearly-always}

\end{itemize}


\subsubsection{Similarity Threshold Impact}

\begin{itemize}

\item The relation of ST and NESS-SHMP retrieval performance and therefore the relevant ST range depends primarily on the NESS type \\
\textbf{Supporting Observations:} \Cref{obs-itm:ASTR-lemma-FNESS-cn-higher-than-CNESS-e5} \Cref{obs-itm:ASTR-FNESS-higher-than-CNESS} \Cref{obs-itm:ASTR-FNESS-equal}

\item The relation of ST and CNESS-SHMP retrieval performance depends secondarily on the NESS model and the usage of the all tokens option \\
\textbf{Supporting Observations:} \Cref{obs-itm:ASTR-CNESS-gte-starts-earlier} \Cref{obs-itm:ASTR-CNESS-at-starts-earlier} \Cref{obs-itm:ASTR-CNESS-end-equal}

\end{itemize}


\subsubsection{NESS Configuration Impact}

\begin{itemize}

\item Using a generally better performing NESS model (regarding established benchmarks) does not generally improve the NESS-SHMP retrieval performance \\
\textbf{Supporting Observations:} \Cref{obs-itm:lemma-FNESS-cn-better-than-w2v}, \Cref{obs-itm:word-FNESS-w2v-better-than-cn}, \Cref{obs-itm:CNESS-e5-better-than-gte}

\item Using the sub tokens embedding instead of the all tokens embedding improves CNESS-SHMPM retrieval performance, but using the all tokens embedding makes it less sensible to the selection of the reference edges
\\ \textbf{Supporting Observations:} \Cref{obs-itm:CNESS-better-without-AT}, \Cref{obs-itm:CNESS-lower-variation-with-AT}

\item CNESS-SHPM retrieval performance improves with a higher number of reference edges and is less sensible to the specific selection of reference edges
\\ \textbf{Supporting Observations:} \Cref{obs-itm:CNESS-more-refs-higher-mean-best-f1}, \Cref{obs-itm:CNESS-more-refs-lower-stddev-best-f1}

\end{itemize}


\subsection{Retrieval Precision Behaviour}
\label{sec:result-retrieval-precision-behaviour}

\begin{itemize}
	\item The precision of NESS-SHMPM correlates with the ST until a specific value of the ST, which itself is specific to the NESS type, NESS model (and other CNESS parameters, especially the selection of reference edges)
\\ \textbf{Supporting Observations:} \Cref{obs-itm:FNESS-ST-precision-correlates}, \Cref{obs-itm:CNESS-ST-precision-correlates-until-crosspoint}, \Cref{obs-itm:FNESS-ST-limit}

\item CNESS-SHMP achieves on average the same precision as the original conflict pattern and lemma-based FNESS, although CNESS-SHMP can achieve a higher precision dependent on the selection of the reference edges \\
\textbf{Supporting Observations:} \Cref{obs-itm:CNESS-precision-variation-higher-ASTR}, \Cref{obs-itm:CNESS-highest-precision-equal-original}


\end{itemize}

\subsection{Contextual Differentiation Ability}
\label{sec:result-contextual-differentiation-ability}

\begin{itemize}
	\item CNESS-SHPM is able differentiate when matching against a set of edges where purely symbolic SHPM and FNESS-SHMPM cannot, i.e. in cases where context is needed to determine the correct semantics of a word \\
	\textbf{Supporting Observations:} \Cref{obs-itm:CNESS-higher-f1-for-highest-f1-difference}, \Cref{obs-itm:PLES-more-diverse-for-highest-f1-difference}, \Cref{obs-itm:conflict-verbs-not-in-highest-f1-difference}, \Cref{obs-itm:conflict-verbs-in-lowest-f1-difference}
	
	\item While CNESS-SHPM achieves a highest difference in retrieval performance for sets of edges, where FNESS-SHMP does not match, it also achieves a better retrieval performance in cases where FNESS-SHPM does match \\
	\textbf{Supporting Observations:} \Cref{obs-itm:CNESS-higher-f1-for-highest-f1-difference} \Cref{obs-itm:FNESS-zero-recall-highest-f1-difference}
		
\end{itemize}

\todo[inline]{Add or integrate more direct answer to the research questions}



% ========== 
\chapter{Related Work}




% ========== 
\chapter{Future Work}
\section{Conceptual Improvements}
Somehow extend the token span used for CNESS beyond the word tokens but not to all tokens. Use the tokens of the next best sub-edge e.g. although in the case of a predicate this is probably the entire sentence most of the time.

try using different levels of contexutalisation --> build the embeddings from different layers of the model

different direction: employ knowledge based semantic similarity to maintain highest level of openness

\section{Implementation Improvements}
implemnt multiprocessing, i.e. server process for both hypergraph and semsim matchers. 

other option would be to leverage python shared memory capabilities but is likely to be less stable and has less scaling potential

for CNESS, pre-compute embeddings for all possible edges (is this feasible?) and add it to the hypergraph
--> instead of caching add edges to the hypergraph or build secondary structure that mirrors the hypergraph and hold different embeddings for different models

\section{Further Evaluations}
other datasets, other tasks
--> mention the task the telmo wanted to try


% ========== 
\chapter{Conclusion}




\printbibliography
\appendix
\chapter{Appendix}
\input{appendix}

\end{document}
 